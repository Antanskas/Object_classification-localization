{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "train.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "1sOUAjNxCaCC",
        "Zx0Q0RzVXLIs",
        "ZiQ44Bxqz1RQ",
        "NtolSUthCgAr",
        "pyy3qbq6Cz6u",
        "pkFeL0D-q9X2",
        "mFz3UuXM51EW",
        "hdgt8AVy53Bu",
        "QJavPGuaKxfo",
        "HdNiRwKJ_JLP",
        "-4TPfdTaWXI4",
        "rxm2_jJVrBZ0",
        "atR72cry9U9P",
        "gLKAH8San6wc",
        "xUs3VDQHzNPw",
        "cuNxtx2IzUiB",
        "Eda9WGuf7r8k",
        "8yXqx4ucrE07",
        "OP6nq44WIoys",
        "AATpl181ad-7",
        "CV7cscZbfIfB",
        "nExCgJ7RqReh",
        "1ocMnmNaqWZT",
        "9XTWHdVOqaYe",
        "TbsAnIKBr2qq",
        "moxFhmeMgkzw"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1sOUAjNxCaCC"
      },
      "source": [
        "# Imports\n",
        "Mount google drive drisk with notebook, import libraries we need"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JAIo3JCMHcmQ",
        "outputId": "ab4bdbd9-2bd9-4ab4-cc18-d243318c1f85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "u8GH7yUBH91u",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4907b6ae-550f-483b-ca22-f2a24e866d1c"
      },
      "source": [
        "from keras.applications.vgg16 import VGG16\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "import datetime as dt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "import random\n",
        "import warnings\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import keras\n",
        "\n",
        "from keras import backend as K\n",
        "from keras.optimizers import Adam\n",
        "from keras import regularizers\n",
        "from keras.models import Sequential\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, Dropout, Activation\n",
        "from keras.layers import Flatten, Conv2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import BatchNormalization, Input\n",
        "from keras.layers import Dropout, GlobalAveragePooling2D\n",
        "\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dropout, Dense, GlobalAveragePooling2D, Input, ZeroPadding2D, BatchNormalization, Activation, Add, AveragePooling2D\n",
        "from keras.initializers import glorot_normal\n",
        "\n",
        "from keras.callbacks import Callback, EarlyStopping\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import shutil\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "from keras.preprocessing import image\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "from keras.models import load_model\n",
        "\n",
        "from keras.applications.resnet50 import ResNet50\n",
        "\n",
        "from keras.models import model_from_json\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zx0Q0RzVXLIs",
        "colab_type": "text"
      },
      "source": [
        "# Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZiQ44Bxqz1RQ",
        "colab_type": "text"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NtolSUthCgAr"
      },
      "source": [
        "### Constants\n",
        "Some constants usefull for model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sjobbY-CKNTO",
        "colab": {}
      },
      "source": [
        "np.random.seed(7)\n",
        "\n",
        "start = dt.datetime.now()\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 20\n",
        "\n",
        "NUM_CLASSES = 7\n",
        "IMAGE_SIZE = 256\n",
        "\n",
        "IMAGE_SIZE_FULL_RESNET50 = 224\n",
        "\n",
        "# Increasing recursion limit to reach files deep in directories\n",
        "sys.setrecursionlimit(3000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Agk9XsdsH_9Q",
        "outputId": "10dd800d-2740-4c1f-956c-eced6540e6bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "working_path = \"/content/drive/My Drive/Dog_Breed/7_breeds/\"\n",
        "!ls \"/content/drive/My Drive/Dog_Breed/7_breeds/models/ResNet50_300/\"\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ResNet50_300+100+400_acc.png   ResNet50_300+100_acc.png   ResNet50_300_acc.png\n",
            "ResNet50_300+100+400.h5        ResNet50_300+100.h5\t  ResNet50_300.h5\n",
            "ResNet50_300+100+400_loss.png  ResNet50_300+100_loss.png  ResNet50_300_loss.png\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pyy3qbq6Cz6u"
      },
      "source": [
        "### Preprocess\n",
        "Change images size according to constants we've set, build and train ImageDataGenerators for data augmentations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3yd2G3B6hYb2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "d2444682-06a2-4962-d49c-06b4c614aa66"
      },
      "source": [
        "labels = pd.read_csv(working_path + 'data/256_labels.csv')\n",
        "breeds = np.unique(labels['class'])\n",
        "breed_dictionary = {} \n",
        "for i in range(len(breeds)):\n",
        "    breed_dictionary[i] = breeds[i]\n",
        "\n",
        "breed_dictionary"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'German_shepherd',\n",
              " 1: 'Labrador_retriever',\n",
              " 2: 'Rottweiler',\n",
              " 3: 'Yorkshire_terrier',\n",
              " 4: 'beagle',\n",
              " 5: 'pug',\n",
              " 6: 'standard_poodle'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4B0Qi0HhYb8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess(img):\n",
        "    img = cv2.resize(img, \n",
        "        (IMAGE_SIZE, IMAGE_SIZE), \n",
        "        interpolation = cv2.INTER_AREA)\n",
        "\n",
        "    img_1 = image.img_to_array(img)\n",
        "    img_1 = cv2.resize(img_1, (IMAGE_SIZE, IMAGE_SIZE), interpolation = cv2.INTER_AREA)\n",
        "    img_1 = np.expand_dims(img_1, axis=0) / 255.\n",
        "\n",
        "    return img_1[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwgtEHpAhYb_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "2011a769-2318-487c-88fc-f9768d0857e7"
      },
      "source": [
        "train_datagen = ImageDataGenerator(\n",
        "    preprocessing_function=preprocess,\n",
        "    rotation_range=30,\n",
        "    width_shift_range=0.3, \n",
        "    height_shift_range=0.3,\n",
        "    horizontal_flip=True,  \n",
        "    vertical_flip=False,\n",
        "    zoom_range=0.3)\n",
        "\n",
        "# we do not want to augment our validation dataset, want to keep it as real as possible\n",
        "val_datagen = ImageDataGenerator(\n",
        "    preprocessing_function=preprocess)\n",
        "\n",
        "train_gen = train_datagen.flow_from_directory(\n",
        "    working_path + \"data/train/\", \n",
        "    batch_size=BATCH_SIZE, \n",
        "    target_size=(IMAGE_SIZE, IMAGE_SIZE), \n",
        "    shuffle=True,\n",
        "    class_mode=\"categorical\")\n",
        "\n",
        "val_gen = val_datagen.flow_from_directory(\n",
        "    working_path + \"data/valid/\", \n",
        "    batch_size=BATCH_SIZE, \n",
        "    target_size=(IMAGE_SIZE, IMAGE_SIZE), \n",
        "    shuffle=True,\n",
        "    class_mode=\"categorical\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 846 images belonging to 7 classes.\n",
            "Found 369 images belonging to 7 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkFeL0D-q9X2",
        "colab_type": "text"
      },
      "source": [
        "### Model "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFz3UuXM51EW",
        "colab_type": "text"
      },
      "source": [
        "#### Vanilla\n",
        "Simple CNN architecture - easy to reach gradient vanishing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8DoYgglhYcF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Vanilla():\n",
        "    model = Sequential()\n",
        "\n",
        "  # Note the (7, 7) here. This is one of technics \n",
        "  # used to reduce memory use by the NN: we scan\n",
        "  # the image in a larger steps.\n",
        "  # Also note regularizers.l2: this technic is \n",
        "  # used to prevent overfitting. The \"0.001\" here\n",
        "  # is an empirical value and can be optimized.\n",
        "    model.add(Conv2D(16, (7, 7), padding='same', use_bias=False, \n",
        "                     input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3), \n",
        "                     kernel_regularizer=regularizers.l2(0.001)))\n",
        "\n",
        "  # Note the use of a standard CNN building blocks: \n",
        "  # Conv2D - BatchNormalization - Activation\n",
        "  # MaxPooling2D - Dropout\n",
        "  # The last two are used to avoid overfitting, also,\n",
        "  # MaxPooling2D reduces memory use.\n",
        "    model.add(BatchNormalization(axis=3, scale=False))\n",
        "    model.add(Activation(\"relu\"))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2), \n",
        "                           strides=(2, 2), padding='same'))\n",
        "    model.add(Dropout(0.5))\n",
        "\n",
        "    model.add(Conv2D(16, (3, 3), padding='same', \n",
        "                     use_bias=False, \n",
        "                     kernel_regularizer=regularizers.l2(0.01)))\n",
        "    model.add(BatchNormalization(axis=3, scale=False))\n",
        "    model.add(Activation(\"relu\"))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2), \n",
        "                           strides=(1, 1), padding='same'))\n",
        "    model.add(Dropout(0.5))\n",
        "  \n",
        "    model.add(Conv2D(32, (3, 3), padding='same', \n",
        "                     use_bias=False, \n",
        "                     kernel_regularizer=regularizers.l2(0.01)))\n",
        "    model.add(BatchNormalization(axis=3, scale=False))\n",
        "    model.add(Activation(\"relu\"))\n",
        "    model.add(Dropout(0.5))\n",
        "  \n",
        "    model.add(Conv2D(32, (3, 3), padding='same', \n",
        "                     use_bias=False, \n",
        "                     kernel_regularizer=regularizers.l2(0.01)))\n",
        "    model.add(BatchNormalization(axis=3, scale=False))\n",
        "    model.add(Activation(\"relu\"))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2), \n",
        "                           strides=(1, 1), padding='same'))\n",
        "    model.add(Dropout(0.5))\n",
        "  \n",
        "    model.add(Conv2D(64, (3, 3), padding='same', \n",
        "                     use_bias=False, \n",
        "                     kernel_regularizer=regularizers.l2(0.01)))\n",
        "    model.add(BatchNormalization(axis=3, scale=False))\n",
        "    model.add(Activation(\"relu\"))\n",
        "    model.add(Dropout(0.5))\n",
        "\n",
        "    model.add(Conv2D(64, (3, 3), padding='same', \n",
        "                     use_bias=False, \n",
        "                     kernel_regularizer=regularizers.l2(0.01)))\n",
        "    model.add(BatchNormalization(axis=3, scale=False))\n",
        "    model.add(Activation(\"relu\"))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2), \n",
        "                           strides=(1, 1), padding='same'))\n",
        "    model.add(Dropout(0.5))\n",
        "\n",
        "    model.add(Conv2D(128, (3, 3), padding='same', \n",
        "                     use_bias=False, \n",
        "                     kernel_regularizer=regularizers.l2(0.01)))\n",
        "    model.add(BatchNormalization(axis=3, scale=False))\n",
        "    model.add(Activation(\"relu\"))\n",
        "    model.add(Dropout(0.5))\n",
        "\n",
        "    model.add(Conv2D(128, (3, 3), padding='same', \n",
        "                     use_bias=False, \n",
        "                     kernel_regularizer=regularizers.l2(0.01)))\n",
        "    model.add(BatchNormalization(axis=3, scale=False))\n",
        "    model.add(Activation(\"relu\"))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2), \n",
        "                           strides=(1, 1), padding='same'))\n",
        "    model.add(Dropout(0.5))\n",
        "\n",
        "    model.add(Conv2D(256, (3, 3), padding='same', \n",
        "                     use_bias=False, \n",
        "                     kernel_regularizer=regularizers.l2(0.01)))\n",
        "    model.add(BatchNormalization(axis=3, scale=False))\n",
        "    model.add(Activation(\"relu\"))\n",
        "    model.add(Dropout(0.5))\n",
        "  \n",
        "    model.add(Conv2D(256, (3, 3), padding='same', \n",
        "                     use_bias=False, \n",
        "                     kernel_regularizer=regularizers.l2(0.01)))\n",
        "    model.add(BatchNormalization(axis=3, scale=False))\n",
        "    model.add(Activation(\"relu\"))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2), \n",
        "                           strides=(1, 1), padding='same'))\n",
        "    model.add(Dropout(0.5))\n",
        "  \n",
        "  # This is the end on \"convolutional\" part of CNN. \n",
        "  # Now we need to transform multidementional\n",
        "  # data into one-dim. array for a fully-connected\n",
        "  # classifier:\n",
        "    model.add(Flatten())\n",
        "  # And two layers of classifier itself (plus an \n",
        "  # Activation layer in between):\n",
        "    model.add(Dense(NUM_CLASSES, activation='softmax', \n",
        "                    kernel_regularizer=regularizers.l2(0.01))) \n",
        "    model.add(Activation(\"relu\"))\n",
        "    model.add(Dense(NUM_CLASSES, activation='softmax', \n",
        "                    kernel_regularizer=regularizers.l2(0.01)))\n",
        "\n",
        "  # We need to compile the resulting network. \n",
        "  # Note that there are few parameters we can\n",
        "  # try here: the best performing one is uncommented, \n",
        "  # the rest is commented out for your reference.\n",
        "  #model.compile(optimizer='rmsprop', \n",
        "  #    loss='categorical_crossentropy', \n",
        "  #    metrics=['accuracy'])\n",
        "  #model.compile(\n",
        "  #    optimizer=keras.optimizers.RMSprop(lr=0.0005), \n",
        "  #    loss='categorical_crossentropy', \n",
        "  #    metrics=['accuracy'])\n",
        "  \n",
        "    model.compile(optimizer='adam', \n",
        "                  loss='categorical_crossentropy', \n",
        "                  metrics=['accuracy'])\n",
        "  #model.compile(optimizer='adadelta', \n",
        "  #    loss='categorical_crossentropy', \n",
        "  #    metrics=['accuracy'])\n",
        "  \n",
        "  #opt = keras.optimizers.Adadelta(lr=1.0, \n",
        "  #    rho=0.95, epsilon=0.01, decay=0.01)\n",
        "  #model.compile(optimizer=opt, \n",
        "  #    loss='categorical_crossentropy', \n",
        "  #    metrics=['accuracy'])\n",
        "  \n",
        "  #opt = keras.optimizers.RMSprop(lr=0.0005, \n",
        "  #    rho=0.9, epsilon=None, decay=0.0001)\n",
        "  #model.compile(optimizer=opt, \n",
        "  #    loss='categorical_crossentropy', \n",
        "  #    metrics=['accuracy'])\n",
        "  \n",
        "  # model.summary()\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdgt8AVy53Bu",
        "colab_type": "text"
      },
      "source": [
        "#### ResNet50 pretrained \n",
        "Load pretrained ResNet50 model from library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2eAYCrec56Y4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 95 acc, 30epoch, retrain on top layers\n",
        "\n",
        "def ResNet50_train_top():\n",
        "  base_model = ResNet50(weights='imagenet', \n",
        "\tinclude_top=False, pooling='avg', \n",
        "\tinput_shape=(IMAGE_SIZE, IMAGE_SIZE, 3))\n",
        "\n",
        "  x = base_model.output\n",
        "  x = Dense(512)(x)\n",
        "  x = Activation('relu')(x)\n",
        "  x = Dropout(0.5)(x)\n",
        "\t\t  \n",
        "  predictions = Dense(NUM_CLASSES, \n",
        "\tactivation='softmax')(x)\n",
        "\n",
        "  model = Model(inputs=base_model.input, \n",
        "\toutputs=predictions)\n",
        "\n",
        "  #model.layers[0].trainable = False\n",
        "\t\t\t  \n",
        "#  model.compile(loss='categorical_crossentropy', \n",
        "#\toptimizer='adam', metrics=['accuracy'])\n",
        "  model.compile(optimizer='sgd', \n",
        "\tloss='categorical_crossentropy', \n",
        "\tmetrics=['accuracy']) \n",
        "\n",
        "  #model.summary()            \n",
        "\t\t  \n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJavPGuaKxfo",
        "colab_type": "text"
      },
      "source": [
        "#### ResNet50 \n",
        "Build ResNet architecture from scrach, compile and train it from beginning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5KPLTD1DKwTs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def identity_block(X, f, filters, stage, block):\n",
        "    # names\n",
        "    conv_name = 'res' + str(stage) + block + 'branch'\n",
        "    bn_name = 'bn' + str(stage) + block + 'branch'\n",
        "    \n",
        "    # filters\n",
        "    F1, F2, F3 = filters\n",
        "    \n",
        "    # input value to add in the end\n",
        "    X_skip = X\n",
        "    \n",
        "    # main path block 1\n",
        "    X = Conv2D(filters=F1, kernel_size=(1,1), strides=(1,1), padding='valid', name=conv_name + '2a', kernel_initializer=glorot_normal(seed=0))(X)\n",
        "    X = BatchNormalization(axis=3, name=bn_name + '2a')(X)\n",
        "    X = Activation('relu')(X)\n",
        "    \n",
        "    # main path block 2\n",
        "    X = Conv2D(filters=F2, kernel_size=(f,f), strides=(1,1), padding='same', name=conv_name + '2b',  kernel_initializer=glorot_normal(seed=0))(X)\n",
        "    X = BatchNormalization(axis=3, name=bn_name + '2b')(X)\n",
        "    X = Activation('relu')(X)    \n",
        "    \n",
        "    # main path block 3\n",
        "    X = Conv2D(filters=F3, kernel_size=(1,1), strides=(1,1), padding='valid', name=conv_name + '2c', kernel_initializer=glorot_normal(seed=0))(X)\n",
        "    X = BatchNormalization(axis=3, name=bn_name + '2c')(X)\n",
        "    \n",
        "    # add skip connection with main path\n",
        "    X = Add()([X, X_skip])\n",
        "    X = Activation('relu')(X)\n",
        "    \n",
        "    return X\n",
        "  \n",
        "def convolutional_block(X, f, filters, stage, block, s = 2):\n",
        "    # names\n",
        "    conv_name = 'res' + str(stage) + block + 'branch'\n",
        "    bn_name = 'bn' + str(stage) + block + 'branch'\n",
        "    \n",
        "    # filters\n",
        "    F1, F2, F3 = filters  \n",
        "  \n",
        "    # input value to add in the end\n",
        "    X_skip = X  \n",
        "  \n",
        "    # main path block 1\n",
        "    X = Conv2D(filters=F1, kernel_size=(1,1), strides=(s,s), name=conv_name + '2a', kernel_initializer=glorot_normal(seed=0))(X)\n",
        "    X = BatchNormalization(axis=3, name=bn_name + '2a')(X)\n",
        "    X = Activation('relu')(X)  \n",
        "\n",
        "    # main path block 2\n",
        "    X = Conv2D(filters=F2, kernel_size=(f,f), strides=(1,1), padding='same', name=conv_name + '2b',  kernel_initializer=glorot_normal(seed=0))(X)\n",
        "    X = BatchNormalization(axis=3, name=bn_name + '2b')(X)\n",
        "    X = Activation('relu')(X)      \n",
        "  \n",
        "    # main path block 3\n",
        "    X = Conv2D(filters=F3, kernel_size=(1,1), strides=(1,1), name=conv_name + '2c', kernel_initializer=glorot_normal(seed=0))(X)\n",
        "    X = BatchNormalization(axis=3, name=bn_name + '2c')(X) \n",
        "  \n",
        "    # skip path\n",
        "    X_skip = Conv2D(F3, kernel_size=(1,1), strides=(s,s), name=conv_name + '1',  kernel_initializer=glorot_normal(seed=0))(X_skip)\n",
        "    X_skip = BatchNormalization(axis=3, name=bn_name + '1')(X_skip)\n",
        "    \n",
        "    # add skip connection with main path\n",
        "    X = Add()([X, X_skip])\n",
        "    X = Activation('relu')(X)\n",
        "    \n",
        "    return X\n",
        "\n",
        "\n",
        "def ResNet50_scrach():\n",
        "    \n",
        "    input_shape = (IMAGE_SIZE, IMAGE_SIZE, 3)\n",
        "    classes = 7\n",
        "    \n",
        "    X_input = Input(input_shape)\n",
        "    \n",
        "    X = ZeroPadding2D((3, 3))(X_input)\n",
        "    \n",
        "    # Stage 1\n",
        "    X = Conv2D(64, (7, 7), strides = (2, 2), name = 'conv1')(X)\n",
        "    X = BatchNormalization(axis = 3, name = 'bn_conv1')(X)\n",
        "    X = Activation('relu')(X)\n",
        "    X = ZeroPadding2D((1, 1))(X)\n",
        "    X = MaxPooling2D((3, 3), strides=(2, 2))(X)\n",
        "  \n",
        "    # Stage 2\n",
        "    X = convolutional_block(X, f = 3, filters = [64, 64, 256], stage = 2, block='a', s = 1)\n",
        "    X = identity_block(X, 3, [64, 64, 256], stage=2, block='b')\n",
        "    X = identity_block(X, 3, [64, 64, 256], stage=2, block='c')\n",
        "    \n",
        "    # Stage 3\n",
        "    X = convolutional_block(X, f = 3, filters = [128, 128, 512], stage = 3, block='a', s = 2)\n",
        "    X = identity_block(X, 3, [128, 128, 512], stage=3, block='b')\n",
        "    X = identity_block(X, 3, [128, 128, 512], stage=3, block='c')\n",
        "    X = identity_block(X, 3, [128, 128, 512], stage=3, block='d')\n",
        "    \n",
        "    # Stage 4\n",
        "    X = convolutional_block(X, f = 3, filters = [256, 256, 1024], stage = 4, block='a', s = 2)\n",
        "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='b')\n",
        "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='c')\n",
        "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='d')\n",
        "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='e')\n",
        "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='f')\n",
        "    \n",
        "    # Stage 5\n",
        "    X = convolutional_block(X, f = 3, filters = [512, 512, 2048], stage = 5, block='a', s = 2)\n",
        "    X = identity_block(X, 3, [512, 512, 2048], stage=5, block='b')\n",
        "    X = identity_block(X, 3, [512, 512, 2048], stage=5, block='c')\n",
        "    \n",
        "    # AVGPOOL\n",
        "    X = GlobalAveragePooling2D('channels_last', name='global_avg_pool')(X)\n",
        "    \n",
        "    # output layers\n",
        "    #X = Flatten()(X)\n",
        "\n",
        "    X = Dense(512)(X)\n",
        "    X = Activation('relu')(X)\n",
        "    X = Dropout(0.5)(X)\n",
        "    \n",
        "    X = Dense(classes, activation='softmax', name='fc' + str(classes))(X)\n",
        "    \n",
        "    model = Model(inputs = X_input, outputs = X, name='ResNet50')\n",
        "    \n",
        "    \n",
        "    model.compile(optimizer='sgd', \n",
        "              loss='categorical_crossentropy', \n",
        "              metrics=['accuracy']) \n",
        "    \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdNiRwKJ_JLP",
        "colab_type": "text"
      },
      "source": [
        "#### Base model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4rLRLRn_K93",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def identity_block_base(X, f, filters, stage, block):\n",
        "    # names\n",
        "    conv_name = 'res' + str(stage) + block + 'branch'\n",
        "    bn_name = 'bn' + str(stage) + block + 'branch'\n",
        "    \n",
        "    # filters\n",
        "    F1, F2, F3 = filters\n",
        "    \n",
        "    # input value to add in the end\n",
        "    X_skip = X\n",
        "    \n",
        "    # main path block 1\n",
        "    X = Conv2D(filters=F1, kernel_size=(1,1), strides=(1,1), padding='valid', name=conv_name + '2a', kernel_initializer=glorot_normal(seed=0))(X)\n",
        "    X = BatchNormalization(axis=3, name=bn_name + '2a')(X)\n",
        "    X = Activation('relu')(X)\n",
        "    \n",
        "    # main path block 2\n",
        "    X = Conv2D(filters=F2, kernel_size=(f,f), strides=(1,1), padding='same', name=conv_name + '2b',  kernel_initializer=glorot_normal(seed=0))(X)\n",
        "    X = BatchNormalization(axis=3, name=bn_name + '2b')(X)\n",
        "    X = Activation('relu')(X)    \n",
        "    \n",
        "    # main path block 3\n",
        "    X = Conv2D(filters=F3, kernel_size=(1,1), strides=(1,1), padding='valid', name=conv_name + '2c', kernel_initializer=glorot_normal(seed=0))(X)\n",
        "    X = BatchNormalization(axis=3, name=bn_name + '2c')(X)\n",
        "    \n",
        "    # add skip connection with main path\n",
        "    X = Add()([X, X_skip])\n",
        "    X = Activation('relu')(X)\n",
        "    \n",
        "    return X\n",
        "  \n",
        "def convolutional_block_base(X, f, filters, stage, block, s = 2):\n",
        "    # names\n",
        "    conv_name = 'res' + str(stage) + block + 'branch'\n",
        "    bn_name = 'bn' + str(stage) + block + 'branch'\n",
        "    \n",
        "    # filters\n",
        "    F1, F2, F3 = filters  \n",
        "  \n",
        "    # input value to add in the end\n",
        "    X_skip = X  \n",
        "  \n",
        "    # main path block 1\n",
        "    X = Conv2D(filters=F1, kernel_size=(1,1), strides=(s,s), name=conv_name + '2a', kernel_initializer=glorot_normal(seed=0))(X)\n",
        "    X = BatchNormalization(axis=3, name=bn_name + '2a')(X)\n",
        "    X = Activation('relu')(X)  \n",
        "\n",
        "    # main path block 2\n",
        "    X = Conv2D(filters=F2, kernel_size=(f,f), strides=(1,1), padding='same', name=conv_name + '2b',  kernel_initializer=glorot_normal(seed=0))(X)\n",
        "    X = BatchNormalization(axis=3, name=bn_name + '2b')(X)\n",
        "    X = Activation('relu')(X)      \n",
        "  \n",
        "    # main path block 3\n",
        "    X = Conv2D(filters=F3, kernel_size=(1,1), strides=(1,1), name=conv_name + '2c', kernel_initializer=glorot_normal(seed=0))(X)\n",
        "    X = BatchNormalization(axis=3, name=bn_name + '2c')(X) \n",
        "  \n",
        "    # skip path\n",
        "    X_skip = Conv2D(F3, kernel_size=(1,1), strides=(s,s), name=conv_name + '1',  kernel_initializer=glorot_normal(seed=0))(X_skip)\n",
        "    X_skip = BatchNormalization(axis=3, name=bn_name + '1')(X_skip)\n",
        "    \n",
        "    # add skip connection with main path\n",
        "    X = Add()([X, X_skip])\n",
        "    X = Activation('relu')(X)\n",
        "    \n",
        "    return X\n",
        "\n",
        "\n",
        "def ResNet50_base(classes):\n",
        "    \n",
        "    input_shape = (IMAGE_SIZE, IMAGE_SIZE, 3)\n",
        "    \n",
        "    X_input = Input(input_shape)\n",
        "    \n",
        "    X = ZeroPadding2D((3, 3))(X_input)\n",
        "    \n",
        "    # Stage 1\n",
        "    X = Conv2D(64, (7, 7), strides = (2, 2), name = 'conv1')(X)\n",
        "    X = BatchNormalization(axis = 3, name = 'bn_conv1')(X)\n",
        "    X = Activation('relu')(X)\n",
        "    X = ZeroPadding2D((1, 1))(X)\n",
        "    X = MaxPooling2D((3, 3), strides=(2, 2))(X)\n",
        "  \n",
        "    # Stage 2\n",
        "    X = convolutional_block_base(X, f = 3, filters = [64, 64, 256], stage = 2, block='a', s = 1)\n",
        "    X = identity_block_base(X, 3, [64, 64, 256], stage=2, block='b')\n",
        "    X = identity_block_base(X, 3, [64, 64, 256], stage=2, block='c')\n",
        "    \n",
        "    # Stage 3\n",
        "    X = convolutional_block_base(X, f = 3, filters = [128, 128, 512], stage = 3, block='a', s = 2)\n",
        "    X = identity_block_base(X, 3, [128, 128, 512], stage=3, block='b')\n",
        "    X = identity_block_base(X, 3, [128, 128, 512], stage=3, block='c')\n",
        "    X = identity_block_base(X, 3, [128, 128, 512], stage=3, block='d')\n",
        "    \n",
        "    # Stage 4\n",
        "    X = convolutional_block_base(X, f = 3, filters = [256, 256, 1024], stage = 4, block='a', s = 2)\n",
        "    X = identity_block_base(X, 3, [256, 256, 1024], stage=4, block='b')\n",
        "    X = identity_block_base(X, 3, [256, 256, 1024], stage=4, block='c')\n",
        "    X = identity_block_base(X, 3, [256, 256, 1024], stage=4, block='d')\n",
        "    X = identity_block_base(X, 3, [256, 256, 1024], stage=4, block='e')\n",
        "    X = identity_block_base(X, 3, [256, 256, 1024], stage=4, block='f')\n",
        "    \n",
        "    # Stage 5\n",
        "    X = convolutional_block_base(X, f = 3, filters = [512, 512, 2048], stage = 5, block='a', s = 2)\n",
        "    X = identity_block_base(X, 3, [512, 512, 2048], stage=5, block='b')\n",
        "    X = identity_block_base(X, 3, [512, 512, 2048], stage=5, block='c')\n",
        "    \n",
        "    return X"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rXv45XRAoKJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def classifier_layer(X, num_classes):\n",
        "#   input_shape = (8, 8, 2048)\n",
        "  X_input = Input(256,256,3)\n",
        "  \n",
        "  # AVGPOOL\n",
        "  X = GlobalAveragePooling2D('channels_last', name='global_avg_pool')(X)\n",
        "\n",
        "  X = Dense(512)(X)\n",
        "  X = Activation('relu')(X)\n",
        "  X = Dropout(0.5)(X)\n",
        "    \n",
        "  X_class = Dense(num_classes, activation='softmax', name='fc' + str(classes), kernel_initializer='zero')(X)\n",
        "  \n",
        "  return X_class\n",
        "\n",
        "#   X_regr = Dense(4 * (num_classes-1), activation='linear', kernel_initializer='zero')\n",
        "#   return [X_class, X_regr]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4TPfdTaWXI4",
        "colab_type": "text"
      },
      "source": [
        "### Model info\n",
        "Some model architectures info"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-szB6fPWYt8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "583d457f-bfaa-4292-ceeb-3e11dbaf8ef9"
      },
      "source": [
        "model = ResNet50_scrach()\n",
        "print('Number of layers: ', len(model.layers))\n",
        "print('------------------------------------------------------------')\n",
        "model.summary()"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of layers:  180\n",
            "------------------------------------------------------------\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_10 (InputLayer)           (None, 256, 256, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d_11 (ZeroPadding2 (None, 262, 262, 3)  0           input_10[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv1 (Conv2D)                  (None, 128, 128, 64) 9472        zero_padding2d_11[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "bn_conv1 (BatchNormalization)   (None, 128, 128, 64) 256         conv1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_443 (Activation)     (None, 128, 128, 64) 0           bn_conv1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d_12 (ZeroPadding2 (None, 130, 130, 64) 0           activation_443[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_10 (MaxPooling2D) (None, 64, 64, 64)   0           zero_padding2d_12[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "res2abranch2a (Conv2D)          (None, 64, 64, 64)   4160        max_pooling2d_10[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn2abranch2a (BatchNormalizatio (None, 64, 64, 64)   256         res2abranch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_444 (Activation)     (None, 64, 64, 64)   0           bn2abranch2a[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "res2abranch2b (Conv2D)          (None, 64, 64, 64)   36928       activation_444[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn2abranch2b (BatchNormalizatio (None, 64, 64, 64)   256         res2abranch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_445 (Activation)     (None, 64, 64, 64)   0           bn2abranch2b[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "res2abranch2c (Conv2D)          (None, 64, 64, 256)  16640       activation_445[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res2abranch1 (Conv2D)           (None, 64, 64, 256)  16640       max_pooling2d_10[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn2abranch2c (BatchNormalizatio (None, 64, 64, 256)  1024        res2abranch2c[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn2abranch1 (BatchNormalization (None, 64, 64, 256)  1024        res2abranch1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add_145 (Add)                   (None, 64, 64, 256)  0           bn2abranch2c[0][0]               \n",
            "                                                                 bn2abranch1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_446 (Activation)     (None, 64, 64, 256)  0           add_145[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "res2bbranch2a (Conv2D)          (None, 64, 64, 64)   16448       activation_446[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn2bbranch2a (BatchNormalizatio (None, 64, 64, 64)   256         res2bbranch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_447 (Activation)     (None, 64, 64, 64)   0           bn2bbranch2a[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "res2bbranch2b (Conv2D)          (None, 64, 64, 64)   36928       activation_447[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn2bbranch2b (BatchNormalizatio (None, 64, 64, 64)   256         res2bbranch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_448 (Activation)     (None, 64, 64, 64)   0           bn2bbranch2b[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "res2bbranch2c (Conv2D)          (None, 64, 64, 256)  16640       activation_448[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn2bbranch2c (BatchNormalizatio (None, 64, 64, 256)  1024        res2bbranch2c[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_146 (Add)                   (None, 64, 64, 256)  0           bn2bbranch2c[0][0]               \n",
            "                                                                 activation_446[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_449 (Activation)     (None, 64, 64, 256)  0           add_146[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "res2cbranch2a (Conv2D)          (None, 64, 64, 64)   16448       activation_449[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn2cbranch2a (BatchNormalizatio (None, 64, 64, 64)   256         res2cbranch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_450 (Activation)     (None, 64, 64, 64)   0           bn2cbranch2a[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "res2cbranch2b (Conv2D)          (None, 64, 64, 64)   36928       activation_450[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn2cbranch2b (BatchNormalizatio (None, 64, 64, 64)   256         res2cbranch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_451 (Activation)     (None, 64, 64, 64)   0           bn2cbranch2b[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "res2cbranch2c (Conv2D)          (None, 64, 64, 256)  16640       activation_451[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn2cbranch2c (BatchNormalizatio (None, 64, 64, 256)  1024        res2cbranch2c[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_147 (Add)                   (None, 64, 64, 256)  0           bn2cbranch2c[0][0]               \n",
            "                                                                 activation_449[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_452 (Activation)     (None, 64, 64, 256)  0           add_147[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "res3abranch2a (Conv2D)          (None, 32, 32, 128)  32896       activation_452[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn3abranch2a (BatchNormalizatio (None, 32, 32, 128)  512         res3abranch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_453 (Activation)     (None, 32, 32, 128)  0           bn3abranch2a[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "res3abranch2b (Conv2D)          (None, 32, 32, 128)  147584      activation_453[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn3abranch2b (BatchNormalizatio (None, 32, 32, 128)  512         res3abranch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_454 (Activation)     (None, 32, 32, 128)  0           bn3abranch2b[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "res3abranch2c (Conv2D)          (None, 32, 32, 512)  66048       activation_454[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res3abranch1 (Conv2D)           (None, 32, 32, 512)  131584      activation_452[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn3abranch2c (BatchNormalizatio (None, 32, 32, 512)  2048        res3abranch2c[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn3abranch1 (BatchNormalization (None, 32, 32, 512)  2048        res3abranch1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add_148 (Add)                   (None, 32, 32, 512)  0           bn3abranch2c[0][0]               \n",
            "                                                                 bn3abranch1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_455 (Activation)     (None, 32, 32, 512)  0           add_148[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "res3bbranch2a (Conv2D)          (None, 32, 32, 128)  65664       activation_455[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn3bbranch2a (BatchNormalizatio (None, 32, 32, 128)  512         res3bbranch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_456 (Activation)     (None, 32, 32, 128)  0           bn3bbranch2a[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "res3bbranch2b (Conv2D)          (None, 32, 32, 128)  147584      activation_456[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn3bbranch2b (BatchNormalizatio (None, 32, 32, 128)  512         res3bbranch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_457 (Activation)     (None, 32, 32, 128)  0           bn3bbranch2b[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "res3bbranch2c (Conv2D)          (None, 32, 32, 512)  66048       activation_457[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn3bbranch2c (BatchNormalizatio (None, 32, 32, 512)  2048        res3bbranch2c[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_149 (Add)                   (None, 32, 32, 512)  0           bn3bbranch2c[0][0]               \n",
            "                                                                 activation_455[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_458 (Activation)     (None, 32, 32, 512)  0           add_149[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "res3cbranch2a (Conv2D)          (None, 32, 32, 128)  65664       activation_458[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn3cbranch2a (BatchNormalizatio (None, 32, 32, 128)  512         res3cbranch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_459 (Activation)     (None, 32, 32, 128)  0           bn3cbranch2a[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "res3cbranch2b (Conv2D)          (None, 32, 32, 128)  147584      activation_459[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn3cbranch2b (BatchNormalizatio (None, 32, 32, 128)  512         res3cbranch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_460 (Activation)     (None, 32, 32, 128)  0           bn3cbranch2b[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "res3cbranch2c (Conv2D)          (None, 32, 32, 512)  66048       activation_460[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn3cbranch2c (BatchNormalizatio (None, 32, 32, 512)  2048        res3cbranch2c[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_150 (Add)                   (None, 32, 32, 512)  0           bn3cbranch2c[0][0]               \n",
            "                                                                 activation_458[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_461 (Activation)     (None, 32, 32, 512)  0           add_150[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "res3dbranch2a (Conv2D)          (None, 32, 32, 128)  65664       activation_461[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn3dbranch2a (BatchNormalizatio (None, 32, 32, 128)  512         res3dbranch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_462 (Activation)     (None, 32, 32, 128)  0           bn3dbranch2a[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "res3dbranch2b (Conv2D)          (None, 32, 32, 128)  147584      activation_462[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn3dbranch2b (BatchNormalizatio (None, 32, 32, 128)  512         res3dbranch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_463 (Activation)     (None, 32, 32, 128)  0           bn3dbranch2b[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "res3dbranch2c (Conv2D)          (None, 32, 32, 512)  66048       activation_463[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn3dbranch2c (BatchNormalizatio (None, 32, 32, 512)  2048        res3dbranch2c[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_151 (Add)                   (None, 32, 32, 512)  0           bn3dbranch2c[0][0]               \n",
            "                                                                 activation_461[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_464 (Activation)     (None, 32, 32, 512)  0           add_151[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "res4abranch2a (Conv2D)          (None, 16, 16, 256)  131328      activation_464[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn4abranch2a (BatchNormalizatio (None, 16, 16, 256)  1024        res4abranch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_465 (Activation)     (None, 16, 16, 256)  0           bn4abranch2a[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "res4abranch2b (Conv2D)          (None, 16, 16, 256)  590080      activation_465[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn4abranch2b (BatchNormalizatio (None, 16, 16, 256)  1024        res4abranch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_466 (Activation)     (None, 16, 16, 256)  0           bn4abranch2b[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "res4abranch2c (Conv2D)          (None, 16, 16, 1024) 263168      activation_466[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res4abranch1 (Conv2D)           (None, 16, 16, 1024) 525312      activation_464[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn4abranch2c (BatchNormalizatio (None, 16, 16, 1024) 4096        res4abranch2c[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn4abranch1 (BatchNormalization (None, 16, 16, 1024) 4096        res4abranch1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add_152 (Add)                   (None, 16, 16, 1024) 0           bn4abranch2c[0][0]               \n",
            "                                                                 bn4abranch1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_467 (Activation)     (None, 16, 16, 1024) 0           add_152[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "res4bbranch2a (Conv2D)          (None, 16, 16, 256)  262400      activation_467[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn4bbranch2a (BatchNormalizatio (None, 16, 16, 256)  1024        res4bbranch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_468 (Activation)     (None, 16, 16, 256)  0           bn4bbranch2a[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "res4bbranch2b (Conv2D)          (None, 16, 16, 256)  590080      activation_468[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn4bbranch2b (BatchNormalizatio (None, 16, 16, 256)  1024        res4bbranch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_469 (Activation)     (None, 16, 16, 256)  0           bn4bbranch2b[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "res4bbranch2c (Conv2D)          (None, 16, 16, 1024) 263168      activation_469[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn4bbranch2c (BatchNormalizatio (None, 16, 16, 1024) 4096        res4bbranch2c[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_153 (Add)                   (None, 16, 16, 1024) 0           bn4bbranch2c[0][0]               \n",
            "                                                                 activation_467[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_470 (Activation)     (None, 16, 16, 1024) 0           add_153[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "res4cbranch2a (Conv2D)          (None, 16, 16, 256)  262400      activation_470[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn4cbranch2a (BatchNormalizatio (None, 16, 16, 256)  1024        res4cbranch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_471 (Activation)     (None, 16, 16, 256)  0           bn4cbranch2a[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "res4cbranch2b (Conv2D)          (None, 16, 16, 256)  590080      activation_471[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn4cbranch2b (BatchNormalizatio (None, 16, 16, 256)  1024        res4cbranch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_472 (Activation)     (None, 16, 16, 256)  0           bn4cbranch2b[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "res4cbranch2c (Conv2D)          (None, 16, 16, 1024) 263168      activation_472[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn4cbranch2c (BatchNormalizatio (None, 16, 16, 1024) 4096        res4cbranch2c[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_154 (Add)                   (None, 16, 16, 1024) 0           bn4cbranch2c[0][0]               \n",
            "                                                                 activation_470[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_473 (Activation)     (None, 16, 16, 1024) 0           add_154[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "res4dbranch2a (Conv2D)          (None, 16, 16, 256)  262400      activation_473[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn4dbranch2a (BatchNormalizatio (None, 16, 16, 256)  1024        res4dbranch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_474 (Activation)     (None, 16, 16, 256)  0           bn4dbranch2a[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "res4dbranch2b (Conv2D)          (None, 16, 16, 256)  590080      activation_474[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn4dbranch2b (BatchNormalizatio (None, 16, 16, 256)  1024        res4dbranch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_475 (Activation)     (None, 16, 16, 256)  0           bn4dbranch2b[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "res4dbranch2c (Conv2D)          (None, 16, 16, 1024) 263168      activation_475[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn4dbranch2c (BatchNormalizatio (None, 16, 16, 1024) 4096        res4dbranch2c[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_155 (Add)                   (None, 16, 16, 1024) 0           bn4dbranch2c[0][0]               \n",
            "                                                                 activation_473[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_476 (Activation)     (None, 16, 16, 1024) 0           add_155[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "res4ebranch2a (Conv2D)          (None, 16, 16, 256)  262400      activation_476[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn4ebranch2a (BatchNormalizatio (None, 16, 16, 256)  1024        res4ebranch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_477 (Activation)     (None, 16, 16, 256)  0           bn4ebranch2a[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "res4ebranch2b (Conv2D)          (None, 16, 16, 256)  590080      activation_477[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn4ebranch2b (BatchNormalizatio (None, 16, 16, 256)  1024        res4ebranch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_478 (Activation)     (None, 16, 16, 256)  0           bn4ebranch2b[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "res4ebranch2c (Conv2D)          (None, 16, 16, 1024) 263168      activation_478[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn4ebranch2c (BatchNormalizatio (None, 16, 16, 1024) 4096        res4ebranch2c[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_156 (Add)                   (None, 16, 16, 1024) 0           bn4ebranch2c[0][0]               \n",
            "                                                                 activation_476[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_479 (Activation)     (None, 16, 16, 1024) 0           add_156[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "res4fbranch2a (Conv2D)          (None, 16, 16, 256)  262400      activation_479[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn4fbranch2a (BatchNormalizatio (None, 16, 16, 256)  1024        res4fbranch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_480 (Activation)     (None, 16, 16, 256)  0           bn4fbranch2a[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "res4fbranch2b (Conv2D)          (None, 16, 16, 256)  590080      activation_480[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn4fbranch2b (BatchNormalizatio (None, 16, 16, 256)  1024        res4fbranch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_481 (Activation)     (None, 16, 16, 256)  0           bn4fbranch2b[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "res4fbranch2c (Conv2D)          (None, 16, 16, 1024) 263168      activation_481[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn4fbranch2c (BatchNormalizatio (None, 16, 16, 1024) 4096        res4fbranch2c[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_157 (Add)                   (None, 16, 16, 1024) 0           bn4fbranch2c[0][0]               \n",
            "                                                                 activation_479[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_482 (Activation)     (None, 16, 16, 1024) 0           add_157[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "res5abranch2a (Conv2D)          (None, 8, 8, 512)    524800      activation_482[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn5abranch2a (BatchNormalizatio (None, 8, 8, 512)    2048        res5abranch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_483 (Activation)     (None, 8, 8, 512)    0           bn5abranch2a[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "res5abranch2b (Conv2D)          (None, 8, 8, 512)    2359808     activation_483[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn5abranch2b (BatchNormalizatio (None, 8, 8, 512)    2048        res5abranch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_484 (Activation)     (None, 8, 8, 512)    0           bn5abranch2b[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "res5abranch2c (Conv2D)          (None, 8, 8, 2048)   1050624     activation_484[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res5abranch1 (Conv2D)           (None, 8, 8, 2048)   2099200     activation_482[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn5abranch2c (BatchNormalizatio (None, 8, 8, 2048)   8192        res5abranch2c[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn5abranch1 (BatchNormalization (None, 8, 8, 2048)   8192        res5abranch1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add_158 (Add)                   (None, 8, 8, 2048)   0           bn5abranch2c[0][0]               \n",
            "                                                                 bn5abranch1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_485 (Activation)     (None, 8, 8, 2048)   0           add_158[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "res5bbranch2a (Conv2D)          (None, 8, 8, 512)    1049088     activation_485[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn5bbranch2a (BatchNormalizatio (None, 8, 8, 512)    2048        res5bbranch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_486 (Activation)     (None, 8, 8, 512)    0           bn5bbranch2a[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "res5bbranch2b (Conv2D)          (None, 8, 8, 512)    2359808     activation_486[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn5bbranch2b (BatchNormalizatio (None, 8, 8, 512)    2048        res5bbranch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_487 (Activation)     (None, 8, 8, 512)    0           bn5bbranch2b[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "res5bbranch2c (Conv2D)          (None, 8, 8, 2048)   1050624     activation_487[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn5bbranch2c (BatchNormalizatio (None, 8, 8, 2048)   8192        res5bbranch2c[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_159 (Add)                   (None, 8, 8, 2048)   0           bn5bbranch2c[0][0]               \n",
            "                                                                 activation_485[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_488 (Activation)     (None, 8, 8, 2048)   0           add_159[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "res5cbranch2a (Conv2D)          (None, 8, 8, 512)    1049088     activation_488[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn5cbranch2a (BatchNormalizatio (None, 8, 8, 512)    2048        res5cbranch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_489 (Activation)     (None, 8, 8, 512)    0           bn5cbranch2a[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "res5cbranch2b (Conv2D)          (None, 8, 8, 512)    2359808     activation_489[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn5cbranch2b (BatchNormalizatio (None, 8, 8, 512)    2048        res5cbranch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_490 (Activation)     (None, 8, 8, 512)    0           bn5cbranch2b[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "res5cbranch2c (Conv2D)          (None, 8, 8, 2048)   1050624     activation_490[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn5cbranch2c (BatchNormalizatio (None, 8, 8, 2048)   8192        res5cbranch2c[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_160 (Add)                   (None, 8, 8, 2048)   0           bn5cbranch2c[0][0]               \n",
            "                                                                 activation_488[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_491 (Activation)     (None, 8, 8, 2048)   0           add_160[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "global_avg_pool (GlobalAverageP (None, 2048)         0           activation_491[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 512)          1049088     global_avg_pool[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_492 (Activation)     (None, 512)          0           dense_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 512)          0           activation_492[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "fc7 (Dense)                     (None, 7)            3591        dropout_2[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 24,640,391\n",
            "Trainable params: 24,587,271\n",
            "Non-trainable params: 53,120\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UbqFT1UEXCDj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a9400a99-ba42-46ac-c161-baf40117a99e"
      },
      "source": [
        "model = ResNet50_train_top()\n",
        "print('Number of layers: ', len(model.layers))\n",
        "print('------------------------------------------------------------')\n",
        "model.summary()"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of layers:  180\n",
            "------------------------------------------------------------\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_5 (InputLayer)            (None, 256, 256, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1_pad (ZeroPadding2D)       (None, 262, 262, 3)  0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1 (Conv2D)                  (None, 128, 128, 64) 9472        conv1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "bn_conv1 (BatchNormalization)   (None, 128, 128, 64) 256         conv1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_197 (Activation)     (None, 128, 128, 64) 0           bn_conv1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pad (ZeroPadding2D)       (None, 130, 130, 64) 0           activation_197[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2D)  (None, 64, 64, 64)   0           pool1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "res2a_branch2a (Conv2D)         (None, 64, 64, 64)   4160        max_pooling2d_5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "bn2a_branch2a (BatchNormalizati (None, 64, 64, 64)   256         res2a_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_198 (Activation)     (None, 64, 64, 64)   0           bn2a_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res2a_branch2b (Conv2D)         (None, 64, 64, 64)   36928       activation_198[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn2a_branch2b (BatchNormalizati (None, 64, 64, 64)   256         res2a_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_199 (Activation)     (None, 64, 64, 64)   0           bn2a_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res2a_branch2c (Conv2D)         (None, 64, 64, 256)  16640       activation_199[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res2a_branch1 (Conv2D)          (None, 64, 64, 256)  16640       max_pooling2d_5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "bn2a_branch2c (BatchNormalizati (None, 64, 64, 256)  1024        res2a_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn2a_branch1 (BatchNormalizatio (None, 64, 64, 256)  1024        res2a_branch1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_65 (Add)                    (None, 64, 64, 256)  0           bn2a_branch2c[0][0]              \n",
            "                                                                 bn2a_branch1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "activation_200 (Activation)     (None, 64, 64, 256)  0           add_65[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "res2b_branch2a (Conv2D)         (None, 64, 64, 64)   16448       activation_200[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn2b_branch2a (BatchNormalizati (None, 64, 64, 64)   256         res2b_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_201 (Activation)     (None, 64, 64, 64)   0           bn2b_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res2b_branch2b (Conv2D)         (None, 64, 64, 64)   36928       activation_201[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn2b_branch2b (BatchNormalizati (None, 64, 64, 64)   256         res2b_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_202 (Activation)     (None, 64, 64, 64)   0           bn2b_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res2b_branch2c (Conv2D)         (None, 64, 64, 256)  16640       activation_202[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn2b_branch2c (BatchNormalizati (None, 64, 64, 256)  1024        res2b_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_66 (Add)                    (None, 64, 64, 256)  0           bn2b_branch2c[0][0]              \n",
            "                                                                 activation_200[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_203 (Activation)     (None, 64, 64, 256)  0           add_66[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "res2c_branch2a (Conv2D)         (None, 64, 64, 64)   16448       activation_203[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn2c_branch2a (BatchNormalizati (None, 64, 64, 64)   256         res2c_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_204 (Activation)     (None, 64, 64, 64)   0           bn2c_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res2c_branch2b (Conv2D)         (None, 64, 64, 64)   36928       activation_204[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn2c_branch2b (BatchNormalizati (None, 64, 64, 64)   256         res2c_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_205 (Activation)     (None, 64, 64, 64)   0           bn2c_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res2c_branch2c (Conv2D)         (None, 64, 64, 256)  16640       activation_205[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn2c_branch2c (BatchNormalizati (None, 64, 64, 256)  1024        res2c_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_67 (Add)                    (None, 64, 64, 256)  0           bn2c_branch2c[0][0]              \n",
            "                                                                 activation_203[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_206 (Activation)     (None, 64, 64, 256)  0           add_67[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "res3a_branch2a (Conv2D)         (None, 32, 32, 128)  32896       activation_206[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn3a_branch2a (BatchNormalizati (None, 32, 32, 128)  512         res3a_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_207 (Activation)     (None, 32, 32, 128)  0           bn3a_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res3a_branch2b (Conv2D)         (None, 32, 32, 128)  147584      activation_207[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn3a_branch2b (BatchNormalizati (None, 32, 32, 128)  512         res3a_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_208 (Activation)     (None, 32, 32, 128)  0           bn3a_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res3a_branch2c (Conv2D)         (None, 32, 32, 512)  66048       activation_208[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res3a_branch1 (Conv2D)          (None, 32, 32, 512)  131584      activation_206[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn3a_branch2c (BatchNormalizati (None, 32, 32, 512)  2048        res3a_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn3a_branch1 (BatchNormalizatio (None, 32, 32, 512)  2048        res3a_branch1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_68 (Add)                    (None, 32, 32, 512)  0           bn3a_branch2c[0][0]              \n",
            "                                                                 bn3a_branch1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "activation_209 (Activation)     (None, 32, 32, 512)  0           add_68[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "res3b_branch2a (Conv2D)         (None, 32, 32, 128)  65664       activation_209[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn3b_branch2a (BatchNormalizati (None, 32, 32, 128)  512         res3b_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_210 (Activation)     (None, 32, 32, 128)  0           bn3b_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res3b_branch2b (Conv2D)         (None, 32, 32, 128)  147584      activation_210[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn3b_branch2b (BatchNormalizati (None, 32, 32, 128)  512         res3b_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_211 (Activation)     (None, 32, 32, 128)  0           bn3b_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res3b_branch2c (Conv2D)         (None, 32, 32, 512)  66048       activation_211[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn3b_branch2c (BatchNormalizati (None, 32, 32, 512)  2048        res3b_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_69 (Add)                    (None, 32, 32, 512)  0           bn3b_branch2c[0][0]              \n",
            "                                                                 activation_209[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_212 (Activation)     (None, 32, 32, 512)  0           add_69[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "res3c_branch2a (Conv2D)         (None, 32, 32, 128)  65664       activation_212[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn3c_branch2a (BatchNormalizati (None, 32, 32, 128)  512         res3c_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_213 (Activation)     (None, 32, 32, 128)  0           bn3c_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res3c_branch2b (Conv2D)         (None, 32, 32, 128)  147584      activation_213[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn3c_branch2b (BatchNormalizati (None, 32, 32, 128)  512         res3c_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_214 (Activation)     (None, 32, 32, 128)  0           bn3c_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res3c_branch2c (Conv2D)         (None, 32, 32, 512)  66048       activation_214[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn3c_branch2c (BatchNormalizati (None, 32, 32, 512)  2048        res3c_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_70 (Add)                    (None, 32, 32, 512)  0           bn3c_branch2c[0][0]              \n",
            "                                                                 activation_212[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_215 (Activation)     (None, 32, 32, 512)  0           add_70[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "res3d_branch2a (Conv2D)         (None, 32, 32, 128)  65664       activation_215[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn3d_branch2a (BatchNormalizati (None, 32, 32, 128)  512         res3d_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_216 (Activation)     (None, 32, 32, 128)  0           bn3d_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res3d_branch2b (Conv2D)         (None, 32, 32, 128)  147584      activation_216[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn3d_branch2b (BatchNormalizati (None, 32, 32, 128)  512         res3d_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_217 (Activation)     (None, 32, 32, 128)  0           bn3d_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res3d_branch2c (Conv2D)         (None, 32, 32, 512)  66048       activation_217[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn3d_branch2c (BatchNormalizati (None, 32, 32, 512)  2048        res3d_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_71 (Add)                    (None, 32, 32, 512)  0           bn3d_branch2c[0][0]              \n",
            "                                                                 activation_215[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_218 (Activation)     (None, 32, 32, 512)  0           add_71[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "res4a_branch2a (Conv2D)         (None, 16, 16, 256)  131328      activation_218[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn4a_branch2a (BatchNormalizati (None, 16, 16, 256)  1024        res4a_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_219 (Activation)     (None, 16, 16, 256)  0           bn4a_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res4a_branch2b (Conv2D)         (None, 16, 16, 256)  590080      activation_219[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn4a_branch2b (BatchNormalizati (None, 16, 16, 256)  1024        res4a_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_220 (Activation)     (None, 16, 16, 256)  0           bn4a_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res4a_branch2c (Conv2D)         (None, 16, 16, 1024) 263168      activation_220[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res4a_branch1 (Conv2D)          (None, 16, 16, 1024) 525312      activation_218[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn4a_branch2c (BatchNormalizati (None, 16, 16, 1024) 4096        res4a_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn4a_branch1 (BatchNormalizatio (None, 16, 16, 1024) 4096        res4a_branch1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_72 (Add)                    (None, 16, 16, 1024) 0           bn4a_branch2c[0][0]              \n",
            "                                                                 bn4a_branch1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "activation_221 (Activation)     (None, 16, 16, 1024) 0           add_72[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "res4b_branch2a (Conv2D)         (None, 16, 16, 256)  262400      activation_221[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn4b_branch2a (BatchNormalizati (None, 16, 16, 256)  1024        res4b_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_222 (Activation)     (None, 16, 16, 256)  0           bn4b_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res4b_branch2b (Conv2D)         (None, 16, 16, 256)  590080      activation_222[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn4b_branch2b (BatchNormalizati (None, 16, 16, 256)  1024        res4b_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_223 (Activation)     (None, 16, 16, 256)  0           bn4b_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res4b_branch2c (Conv2D)         (None, 16, 16, 1024) 263168      activation_223[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn4b_branch2c (BatchNormalizati (None, 16, 16, 1024) 4096        res4b_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_73 (Add)                    (None, 16, 16, 1024) 0           bn4b_branch2c[0][0]              \n",
            "                                                                 activation_221[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_224 (Activation)     (None, 16, 16, 1024) 0           add_73[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "res4c_branch2a (Conv2D)         (None, 16, 16, 256)  262400      activation_224[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn4c_branch2a (BatchNormalizati (None, 16, 16, 256)  1024        res4c_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_225 (Activation)     (None, 16, 16, 256)  0           bn4c_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res4c_branch2b (Conv2D)         (None, 16, 16, 256)  590080      activation_225[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn4c_branch2b (BatchNormalizati (None, 16, 16, 256)  1024        res4c_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_226 (Activation)     (None, 16, 16, 256)  0           bn4c_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res4c_branch2c (Conv2D)         (None, 16, 16, 1024) 263168      activation_226[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn4c_branch2c (BatchNormalizati (None, 16, 16, 1024) 4096        res4c_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_74 (Add)                    (None, 16, 16, 1024) 0           bn4c_branch2c[0][0]              \n",
            "                                                                 activation_224[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_227 (Activation)     (None, 16, 16, 1024) 0           add_74[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "res4d_branch2a (Conv2D)         (None, 16, 16, 256)  262400      activation_227[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn4d_branch2a (BatchNormalizati (None, 16, 16, 256)  1024        res4d_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_228 (Activation)     (None, 16, 16, 256)  0           bn4d_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res4d_branch2b (Conv2D)         (None, 16, 16, 256)  590080      activation_228[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn4d_branch2b (BatchNormalizati (None, 16, 16, 256)  1024        res4d_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_229 (Activation)     (None, 16, 16, 256)  0           bn4d_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res4d_branch2c (Conv2D)         (None, 16, 16, 1024) 263168      activation_229[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn4d_branch2c (BatchNormalizati (None, 16, 16, 1024) 4096        res4d_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_75 (Add)                    (None, 16, 16, 1024) 0           bn4d_branch2c[0][0]              \n",
            "                                                                 activation_227[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_230 (Activation)     (None, 16, 16, 1024) 0           add_75[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "res4e_branch2a (Conv2D)         (None, 16, 16, 256)  262400      activation_230[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn4e_branch2a (BatchNormalizati (None, 16, 16, 256)  1024        res4e_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_231 (Activation)     (None, 16, 16, 256)  0           bn4e_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res4e_branch2b (Conv2D)         (None, 16, 16, 256)  590080      activation_231[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn4e_branch2b (BatchNormalizati (None, 16, 16, 256)  1024        res4e_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_232 (Activation)     (None, 16, 16, 256)  0           bn4e_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res4e_branch2c (Conv2D)         (None, 16, 16, 1024) 263168      activation_232[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn4e_branch2c (BatchNormalizati (None, 16, 16, 1024) 4096        res4e_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_76 (Add)                    (None, 16, 16, 1024) 0           bn4e_branch2c[0][0]              \n",
            "                                                                 activation_230[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_233 (Activation)     (None, 16, 16, 1024) 0           add_76[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "res4f_branch2a (Conv2D)         (None, 16, 16, 256)  262400      activation_233[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn4f_branch2a (BatchNormalizati (None, 16, 16, 256)  1024        res4f_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_234 (Activation)     (None, 16, 16, 256)  0           bn4f_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res4f_branch2b (Conv2D)         (None, 16, 16, 256)  590080      activation_234[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn4f_branch2b (BatchNormalizati (None, 16, 16, 256)  1024        res4f_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_235 (Activation)     (None, 16, 16, 256)  0           bn4f_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res4f_branch2c (Conv2D)         (None, 16, 16, 1024) 263168      activation_235[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn4f_branch2c (BatchNormalizati (None, 16, 16, 1024) 4096        res4f_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_77 (Add)                    (None, 16, 16, 1024) 0           bn4f_branch2c[0][0]              \n",
            "                                                                 activation_233[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_236 (Activation)     (None, 16, 16, 1024) 0           add_77[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "res5a_branch2a (Conv2D)         (None, 8, 8, 512)    524800      activation_236[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn5a_branch2a (BatchNormalizati (None, 8, 8, 512)    2048        res5a_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_237 (Activation)     (None, 8, 8, 512)    0           bn5a_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res5a_branch2b (Conv2D)         (None, 8, 8, 512)    2359808     activation_237[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn5a_branch2b (BatchNormalizati (None, 8, 8, 512)    2048        res5a_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_238 (Activation)     (None, 8, 8, 512)    0           bn5a_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res5a_branch2c (Conv2D)         (None, 8, 8, 2048)   1050624     activation_238[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res5a_branch1 (Conv2D)          (None, 8, 8, 2048)   2099200     activation_236[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn5a_branch2c (BatchNormalizati (None, 8, 8, 2048)   8192        res5a_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn5a_branch1 (BatchNormalizatio (None, 8, 8, 2048)   8192        res5a_branch1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_78 (Add)                    (None, 8, 8, 2048)   0           bn5a_branch2c[0][0]              \n",
            "                                                                 bn5a_branch1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "activation_239 (Activation)     (None, 8, 8, 2048)   0           add_78[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "res5b_branch2a (Conv2D)         (None, 8, 8, 512)    1049088     activation_239[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn5b_branch2a (BatchNormalizati (None, 8, 8, 512)    2048        res5b_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_240 (Activation)     (None, 8, 8, 512)    0           bn5b_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res5b_branch2b (Conv2D)         (None, 8, 8, 512)    2359808     activation_240[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn5b_branch2b (BatchNormalizati (None, 8, 8, 512)    2048        res5b_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_241 (Activation)     (None, 8, 8, 512)    0           bn5b_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res5b_branch2c (Conv2D)         (None, 8, 8, 2048)   1050624     activation_241[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn5b_branch2c (BatchNormalizati (None, 8, 8, 2048)   8192        res5b_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_79 (Add)                    (None, 8, 8, 2048)   0           bn5b_branch2c[0][0]              \n",
            "                                                                 activation_239[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_242 (Activation)     (None, 8, 8, 2048)   0           add_79[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "res5c_branch2a (Conv2D)         (None, 8, 8, 512)    1049088     activation_242[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn5c_branch2a (BatchNormalizati (None, 8, 8, 512)    2048        res5c_branch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_243 (Activation)     (None, 8, 8, 512)    0           bn5c_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res5c_branch2b (Conv2D)         (None, 8, 8, 512)    2359808     activation_243[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn5c_branch2b (BatchNormalizati (None, 8, 8, 512)    2048        res5c_branch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_244 (Activation)     (None, 8, 8, 512)    0           bn5c_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res5c_branch2c (Conv2D)         (None, 8, 8, 2048)   1050624     activation_244[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn5c_branch2c (BatchNormalizati (None, 8, 8, 2048)   8192        res5c_branch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_80 (Add)                    (None, 8, 8, 2048)   0           bn5c_branch2c[0][0]              \n",
            "                                                                 activation_242[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_245 (Activation)     (None, 8, 8, 2048)   0           add_80[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_1 (Glo (None, 2048)         0           activation_245[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 512)          1049088     global_average_pooling2d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "activation_246 (Activation)     (None, 512)          0           dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 512)          0           activation_246[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 7)            3591        dropout_1[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 24,640,391\n",
            "Trainable params: 24,587,271\n",
            "Non-trainable params: 53,120\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxm2_jJVrBZ0",
        "colab_type": "text"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atR72cry9U9P",
        "colab_type": "text"
      },
      "source": [
        "### Compile from scrach model\n",
        "Comment following cell when loading model from disk"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_kegT_VhYcI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "STEP_SIZE_TRAIN=train_gen.n//train_gen.batch_size\n",
        "STEP_SIZE_VALID=val_gen.n//val_gen.batch_size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iP2dtnh69YLh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train_model = ResNet50_scrach()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6CWnoC49mTb",
        "colab_type": "text"
      },
      "source": [
        "### Compile pretrained from disk\n",
        "Comment following cell when training from beginning with models described in this notebook above"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1iaYZJRgffx5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loaded_model = model_from_drive('models/ResNet50_300/ResNet50_300+100+400.h5')\n",
        "loaded_model_weights = loaded_model.get_weights()\n",
        "\n",
        "train_model = ResNet50_scrach()\n",
        "train_model.set_weights(loaded_model_weights)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLKAH8San6wc",
        "colab_type": "text"
      },
      "source": [
        "### Fit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5kKs7LGhYcM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "67b1139e-2b62-4d8d-85ef-23b44a7c499f"
      },
      "source": [
        "history = train_model.fit_generator(generator=train_gen,\n",
        "                              steps_per_epoch=STEP_SIZE_TRAIN,\n",
        "                              validation_data=val_gen,\n",
        "                              validation_steps=STEP_SIZE_VALID,\n",
        "                              epochs=400)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/400\n",
            "52/52 [==============================] - 43s 826ms/step - loss: 0.1611 - acc: 0.9543 - val_loss: 3.4632 - val_acc: 0.4363\n",
            "Epoch 2/400\n",
            "52/52 [==============================] - 32s 612ms/step - loss: 0.2034 - acc: 0.9322 - val_loss: 2.0076 - val_acc: 0.5581\n",
            "Epoch 3/400\n",
            "52/52 [==============================] - 32s 613ms/step - loss: 0.1852 - acc: 0.9361 - val_loss: 2.3609 - val_acc: 0.5779\n",
            "Epoch 4/400\n",
            "52/52 [==============================] - 32s 614ms/step - loss: 0.2038 - acc: 0.9263 - val_loss: 2.0583 - val_acc: 0.5609\n",
            "Epoch 5/400\n",
            "52/52 [==============================] - 32s 611ms/step - loss: 0.1723 - acc: 0.9394 - val_loss: 2.3375 - val_acc: 0.5552\n",
            "Epoch 6/400\n",
            "52/52 [==============================] - 32s 613ms/step - loss: 0.1595 - acc: 0.9518 - val_loss: 1.6785 - val_acc: 0.6232\n",
            "Epoch 7/400\n",
            "52/52 [==============================] - 32s 613ms/step - loss: 0.1544 - acc: 0.9518 - val_loss: 2.6019 - val_acc: 0.5156\n",
            "Epoch 8/400\n",
            "52/52 [==============================] - 32s 613ms/step - loss: 0.2816 - acc: 0.9217 - val_loss: 2.2135 - val_acc: 0.5978\n",
            "Epoch 9/400\n",
            "52/52 [==============================] - 32s 612ms/step - loss: 0.2090 - acc: 0.9373 - val_loss: 1.8856 - val_acc: 0.5892\n",
            "Epoch 10/400\n",
            "52/52 [==============================] - 32s 610ms/step - loss: 0.1510 - acc: 0.9385 - val_loss: 2.2418 - val_acc: 0.5864\n",
            "Epoch 11/400\n",
            "52/52 [==============================] - 32s 610ms/step - loss: 0.1626 - acc: 0.9519 - val_loss: 2.6864 - val_acc: 0.5411\n",
            "Epoch 12/400\n",
            "52/52 [==============================] - 32s 612ms/step - loss: 0.2310 - acc: 0.9231 - val_loss: 2.5737 - val_acc: 0.5439\n",
            "Epoch 13/400\n",
            "52/52 [==============================] - 32s 612ms/step - loss: 0.2084 - acc: 0.9385 - val_loss: 2.3473 - val_acc: 0.5297\n",
            "Epoch 14/400\n",
            "52/52 [==============================] - 32s 612ms/step - loss: 0.2170 - acc: 0.9361 - val_loss: 1.9871 - val_acc: 0.5722\n",
            "Epoch 15/400\n",
            "52/52 [==============================] - 32s 611ms/step - loss: 0.1811 - acc: 0.9363 - val_loss: 2.5355 - val_acc: 0.5439\n",
            "Epoch 16/400\n",
            "52/52 [==============================] - 32s 612ms/step - loss: 0.1786 - acc: 0.9514 - val_loss: 2.2736 - val_acc: 0.5326\n",
            "Epoch 17/400\n",
            "52/52 [==============================] - 32s 611ms/step - loss: 0.1580 - acc: 0.9481 - val_loss: 1.6999 - val_acc: 0.5807\n",
            "Epoch 18/400\n",
            "52/52 [==============================] - 32s 613ms/step - loss: 0.1255 - acc: 0.9591 - val_loss: 2.8629 - val_acc: 0.4731\n",
            "Epoch 19/400\n",
            "52/52 [==============================] - 32s 611ms/step - loss: 0.1600 - acc: 0.9518 - val_loss: 2.2056 - val_acc: 0.5864\n",
            "Epoch 20/400\n",
            "52/52 [==============================] - 32s 611ms/step - loss: 0.1848 - acc: 0.9447 - val_loss: 2.9757 - val_acc: 0.4816\n",
            "Epoch 21/400\n",
            "52/52 [==============================] - 32s 613ms/step - loss: 0.1782 - acc: 0.9447 - val_loss: 2.3199 - val_acc: 0.6062\n",
            "Epoch 22/400\n",
            "52/52 [==============================] - 32s 611ms/step - loss: 0.1379 - acc: 0.9506 - val_loss: 2.4771 - val_acc: 0.5779\n",
            "Epoch 23/400\n",
            "52/52 [==============================] - 32s 613ms/step - loss: 0.2223 - acc: 0.9336 - val_loss: 2.9148 - val_acc: 0.5411\n",
            "Epoch 24/400\n",
            "52/52 [==============================] - 32s 612ms/step - loss: 0.1414 - acc: 0.9506 - val_loss: 2.4063 - val_acc: 0.5694\n",
            "Epoch 25/400\n",
            "52/52 [==============================] - 32s 611ms/step - loss: 0.2209 - acc: 0.9243 - val_loss: 1.9489 - val_acc: 0.6232\n",
            "Epoch 26/400\n",
            "52/52 [==============================] - 32s 610ms/step - loss: 0.1712 - acc: 0.9507 - val_loss: 2.5919 - val_acc: 0.5184\n",
            "Epoch 27/400\n",
            "52/52 [==============================] - 32s 610ms/step - loss: 0.1881 - acc: 0.9471 - val_loss: 2.1029 - val_acc: 0.5921\n",
            "Epoch 28/400\n",
            "52/52 [==============================] - 32s 611ms/step - loss: 0.1481 - acc: 0.9493 - val_loss: 2.0747 - val_acc: 0.6062\n",
            "Epoch 29/400\n",
            "52/52 [==============================] - 32s 612ms/step - loss: 0.1297 - acc: 0.9675 - val_loss: 2.6486 - val_acc: 0.5326\n",
            "Epoch 30/400\n",
            "52/52 [==============================] - 32s 612ms/step - loss: 0.1782 - acc: 0.9385 - val_loss: 3.1681 - val_acc: 0.4901\n",
            "Epoch 31/400\n",
            "52/52 [==============================] - 32s 611ms/step - loss: 0.1806 - acc: 0.9483 - val_loss: 3.1229 - val_acc: 0.4901\n",
            "Epoch 32/400\n",
            "52/52 [==============================] - 32s 616ms/step - loss: 0.1825 - acc: 0.9530 - val_loss: 2.4710 - val_acc: 0.5625\n",
            "Epoch 33/400\n",
            "52/52 [==============================] - 32s 613ms/step - loss: 0.2510 - acc: 0.9346 - val_loss: 2.2530 - val_acc: 0.5354\n",
            "Epoch 34/400\n",
            "52/52 [==============================] - 32s 615ms/step - loss: 0.1740 - acc: 0.9409 - val_loss: 2.2466 - val_acc: 0.5099\n",
            "Epoch 35/400\n",
            "52/52 [==============================] - 32s 615ms/step - loss: 0.1613 - acc: 0.9447 - val_loss: 2.0084 - val_acc: 0.5581\n",
            "Epoch 36/400\n",
            "52/52 [==============================] - 32s 614ms/step - loss: 0.1632 - acc: 0.9432 - val_loss: 2.4153 - val_acc: 0.5354\n",
            "Epoch 37/400\n",
            "52/52 [==============================] - 32s 614ms/step - loss: 0.1411 - acc: 0.9518 - val_loss: 1.7264 - val_acc: 0.5722\n",
            "Epoch 38/400\n",
            "52/52 [==============================] - 32s 614ms/step - loss: 0.1282 - acc: 0.9603 - val_loss: 2.3945 - val_acc: 0.5524\n",
            "Epoch 39/400\n",
            "52/52 [==============================] - 32s 613ms/step - loss: 0.1426 - acc: 0.9506 - val_loss: 1.8424 - val_acc: 0.5807\n",
            "Epoch 40/400\n",
            "52/52 [==============================] - 32s 617ms/step - loss: 0.2449 - acc: 0.9251 - val_loss: 2.1306 - val_acc: 0.5751\n",
            "Epoch 41/400\n",
            "52/52 [==============================] - 32s 615ms/step - loss: 0.1471 - acc: 0.9506 - val_loss: 2.1968 - val_acc: 0.5637\n",
            "Epoch 42/400\n",
            "52/52 [==============================] - 32s 616ms/step - loss: 0.1644 - acc: 0.9531 - val_loss: 2.1400 - val_acc: 0.6176\n",
            "Epoch 43/400\n",
            "52/52 [==============================] - 32s 612ms/step - loss: 0.1693 - acc: 0.9399 - val_loss: 2.3722 - val_acc: 0.5694\n",
            "Epoch 44/400\n",
            "52/52 [==============================] - 32s 615ms/step - loss: 0.1388 - acc: 0.9603 - val_loss: 2.2741 - val_acc: 0.5212\n",
            "Epoch 45/400\n",
            "52/52 [==============================] - 32s 612ms/step - loss: 0.1685 - acc: 0.9411 - val_loss: 2.9476 - val_acc: 0.4476\n",
            "Epoch 46/400\n",
            "52/52 [==============================] - 32s 613ms/step - loss: 0.1195 - acc: 0.9602 - val_loss: 1.6141 - val_acc: 0.6601\n",
            "Epoch 47/400\n",
            "52/52 [==============================] - 32s 615ms/step - loss: 0.1468 - acc: 0.9530 - val_loss: 2.6772 - val_acc: 0.5212\n",
            "Epoch 48/400\n",
            "52/52 [==============================] - 32s 613ms/step - loss: 0.1301 - acc: 0.9554 - val_loss: 2.1358 - val_acc: 0.6062\n",
            "Epoch 49/400\n",
            "52/52 [==============================] - 32s 616ms/step - loss: 0.1938 - acc: 0.9360 - val_loss: 2.2600 - val_acc: 0.5666\n",
            "Epoch 50/400\n",
            "52/52 [==============================] - 32s 614ms/step - loss: 0.1296 - acc: 0.9590 - val_loss: 1.9906 - val_acc: 0.5864\n",
            "Epoch 51/400\n",
            "52/52 [==============================] - 32s 613ms/step - loss: 0.1446 - acc: 0.9543 - val_loss: 2.3893 - val_acc: 0.5637\n",
            "Epoch 52/400\n",
            "52/52 [==============================] - 32s 612ms/step - loss: 0.1404 - acc: 0.9579 - val_loss: 2.9007 - val_acc: 0.5156\n",
            "Epoch 53/400\n",
            "52/52 [==============================] - 32s 614ms/step - loss: 0.1410 - acc: 0.9602 - val_loss: 1.9964 - val_acc: 0.6402\n",
            "Epoch 54/400\n",
            "52/52 [==============================] - 32s 612ms/step - loss: 0.1906 - acc: 0.9435 - val_loss: 2.4262 - val_acc: 0.5496\n",
            "Epoch 55/400\n",
            "52/52 [==============================] - 32s 612ms/step - loss: 0.1659 - acc: 0.9445 - val_loss: 2.4242 - val_acc: 0.5297\n",
            "Epoch 56/400\n",
            "52/52 [==============================] - 32s 614ms/step - loss: 0.1787 - acc: 0.9469 - val_loss: 2.3529 - val_acc: 0.5707\n",
            "Epoch 57/400\n",
            "52/52 [==============================] - 32s 610ms/step - loss: 0.1244 - acc: 0.9602 - val_loss: 2.1977 - val_acc: 0.5637\n",
            "Epoch 58/400\n",
            "52/52 [==============================] - 32s 610ms/step - loss: 0.1597 - acc: 0.9518 - val_loss: 2.4654 - val_acc: 0.5581\n",
            "Epoch 59/400\n",
            "52/52 [==============================] - 32s 612ms/step - loss: 0.1442 - acc: 0.9624 - val_loss: 2.5209 - val_acc: 0.5637\n",
            "Epoch 60/400\n",
            "52/52 [==============================] - 32s 611ms/step - loss: 0.1882 - acc: 0.9435 - val_loss: 1.8867 - val_acc: 0.6119\n",
            "Epoch 61/400\n",
            "52/52 [==============================] - 32s 611ms/step - loss: 0.2015 - acc: 0.9373 - val_loss: 1.8016 - val_acc: 0.5977\n",
            "Epoch 62/400\n",
            "52/52 [==============================] - 32s 610ms/step - loss: 0.1743 - acc: 0.9433 - val_loss: 1.7933 - val_acc: 0.6176\n",
            "Epoch 63/400\n",
            "52/52 [==============================] - 32s 612ms/step - loss: 0.1392 - acc: 0.9590 - val_loss: 2.4620 - val_acc: 0.5864\n",
            "Epoch 64/400\n",
            "52/52 [==============================] - 32s 610ms/step - loss: 0.1225 - acc: 0.9566 - val_loss: 2.6220 - val_acc: 0.5524\n",
            "Epoch 65/400\n",
            "52/52 [==============================] - 32s 611ms/step - loss: 0.1220 - acc: 0.9578 - val_loss: 2.4082 - val_acc: 0.5836\n",
            "Epoch 66/400\n",
            "52/52 [==============================] - 32s 610ms/step - loss: 0.1385 - acc: 0.9518 - val_loss: 2.3131 - val_acc: 0.5637\n",
            "Epoch 67/400\n",
            "52/52 [==============================] - 32s 610ms/step - loss: 0.1864 - acc: 0.9421 - val_loss: 2.4242 - val_acc: 0.5807\n",
            "Epoch 68/400\n",
            "52/52 [==============================] - 32s 610ms/step - loss: 0.1471 - acc: 0.9518 - val_loss: 2.2709 - val_acc: 0.5439\n",
            "Epoch 69/400\n",
            "52/52 [==============================] - 32s 611ms/step - loss: 0.1846 - acc: 0.9420 - val_loss: 1.6687 - val_acc: 0.6572\n",
            "Epoch 70/400\n",
            "52/52 [==============================] - 32s 613ms/step - loss: 0.1316 - acc: 0.9662 - val_loss: 2.0048 - val_acc: 0.6119\n",
            "Epoch 71/400\n",
            "52/52 [==============================] - 32s 612ms/step - loss: 0.1635 - acc: 0.9519 - val_loss: 2.3648 - val_acc: 0.5382\n",
            "Epoch 72/400\n",
            "52/52 [==============================] - 32s 611ms/step - loss: 0.1386 - acc: 0.9566 - val_loss: 2.0938 - val_acc: 0.5666\n",
            "Epoch 73/400\n",
            "52/52 [==============================] - 32s 612ms/step - loss: 0.1632 - acc: 0.9493 - val_loss: 1.6761 - val_acc: 0.6232\n",
            "Epoch 74/400\n",
            "52/52 [==============================] - 32s 611ms/step - loss: 0.1495 - acc: 0.9526 - val_loss: 2.9272 - val_acc: 0.4929\n",
            "Epoch 75/400\n",
            "52/52 [==============================] - 32s 612ms/step - loss: 0.1287 - acc: 0.9614 - val_loss: 2.9385 - val_acc: 0.5071\n",
            "Epoch 76/400\n",
            "52/52 [==============================] - 32s 611ms/step - loss: 0.1795 - acc: 0.9447 - val_loss: 2.0727 - val_acc: 0.5864\n",
            "Epoch 77/400\n",
            "52/52 [==============================] - 32s 610ms/step - loss: 0.1065 - acc: 0.9627 - val_loss: 2.0521 - val_acc: 0.5666\n",
            "Epoch 78/400\n",
            "52/52 [==============================] - 32s 614ms/step - loss: 0.1516 - acc: 0.9469 - val_loss: 2.1445 - val_acc: 0.5864\n",
            "Epoch 79/400\n",
            "52/52 [==============================] - 32s 612ms/step - loss: 0.1423 - acc: 0.9528 - val_loss: 2.1722 - val_acc: 0.5864\n",
            "Epoch 80/400\n",
            "52/52 [==============================] - 32s 616ms/step - loss: 0.1365 - acc: 0.9650 - val_loss: 2.9730 - val_acc: 0.5000\n",
            "Epoch 81/400\n",
            "52/52 [==============================] - 32s 613ms/step - loss: 0.1233 - acc: 0.9588 - val_loss: 2.0164 - val_acc: 0.5864\n",
            "Epoch 82/400\n",
            "52/52 [==============================] - 32s 614ms/step - loss: 0.2140 - acc: 0.9373 - val_loss: 1.9625 - val_acc: 0.6147\n",
            "Epoch 83/400\n",
            "52/52 [==============================] - 32s 612ms/step - loss: 0.1426 - acc: 0.9602 - val_loss: 2.1403 - val_acc: 0.5496\n",
            "Epoch 84/400\n",
            "52/52 [==============================] - 32s 612ms/step - loss: 0.1427 - acc: 0.9542 - val_loss: 1.8943 - val_acc: 0.5722\n",
            "Epoch 85/400\n",
            "52/52 [==============================] - 32s 614ms/step - loss: 0.1388 - acc: 0.9591 - val_loss: 2.2983 - val_acc: 0.5609\n",
            "Epoch 86/400\n",
            "52/52 [==============================] - 32s 611ms/step - loss: 0.1174 - acc: 0.9639 - val_loss: 1.6547 - val_acc: 0.6487\n",
            "Epoch 87/400\n",
            "52/52 [==============================] - 32s 614ms/step - loss: 0.1038 - acc: 0.9663 - val_loss: 2.3997 - val_acc: 0.5722\n",
            "Epoch 88/400\n",
            "52/52 [==============================] - 32s 610ms/step - loss: 0.1346 - acc: 0.9579 - val_loss: 2.9545 - val_acc: 0.5156\n",
            "Epoch 89/400\n",
            "52/52 [==============================] - 32s 611ms/step - loss: 0.0828 - acc: 0.9770 - val_loss: 2.6822 - val_acc: 0.5637\n",
            "Epoch 90/400\n",
            "52/52 [==============================] - 32s 613ms/step - loss: 0.1188 - acc: 0.9651 - val_loss: 2.3145 - val_acc: 0.5411\n",
            "Epoch 91/400\n",
            "52/52 [==============================] - 32s 612ms/step - loss: 0.1006 - acc: 0.9706 - val_loss: 1.9439 - val_acc: 0.6516\n",
            "Epoch 92/400\n",
            "52/52 [==============================] - 32s 611ms/step - loss: 0.1486 - acc: 0.9507 - val_loss: 3.3066 - val_acc: 0.5099\n",
            "Epoch 93/400\n",
            "52/52 [==============================] - 32s 612ms/step - loss: 0.1177 - acc: 0.9555 - val_loss: 2.0260 - val_acc: 0.6091\n",
            "Epoch 94/400\n",
            "52/52 [==============================] - 32s 612ms/step - loss: 0.0795 - acc: 0.9736 - val_loss: 2.1755 - val_acc: 0.6034\n",
            "Epoch 95/400\n",
            "52/52 [==============================] - 32s 611ms/step - loss: 0.0688 - acc: 0.9784 - val_loss: 2.1376 - val_acc: 0.6091\n",
            "Epoch 96/400\n",
            "52/52 [==============================] - 32s 610ms/step - loss: 0.1681 - acc: 0.9447 - val_loss: 2.5777 - val_acc: 0.5751\n",
            "Epoch 97/400\n",
            "52/52 [==============================] - 32s 611ms/step - loss: 0.1814 - acc: 0.9459 - val_loss: 2.3511 - val_acc: 0.6062\n",
            "Epoch 98/400\n",
            "52/52 [==============================] - 32s 614ms/step - loss: 0.1327 - acc: 0.9567 - val_loss: 2.2709 - val_acc: 0.5892\n",
            "Epoch 99/400\n",
            "52/52 [==============================] - 32s 613ms/step - loss: 0.1197 - acc: 0.9651 - val_loss: 2.2370 - val_acc: 0.5864\n",
            "Epoch 100/400\n",
            "52/52 [==============================] - 32s 609ms/step - loss: 0.0858 - acc: 0.9784 - val_loss: 2.1548 - val_acc: 0.5722\n",
            "Epoch 101/400\n",
            "52/52 [==============================] - 32s 611ms/step - loss: 0.1154 - acc: 0.9615 - val_loss: 2.3125 - val_acc: 0.5666\n",
            "Epoch 102/400\n",
            "52/52 [==============================] - 32s 608ms/step - loss: 0.1388 - acc: 0.9578 - val_loss: 2.5743 - val_acc: 0.5467\n",
            "Epoch 103/400\n",
            "52/52 [==============================] - 32s 611ms/step - loss: 0.0943 - acc: 0.9736 - val_loss: 1.7921 - val_acc: 0.6346\n",
            "Epoch 104/400\n",
            "52/52 [==============================] - 32s 613ms/step - loss: 0.1073 - acc: 0.9686 - val_loss: 2.3217 - val_acc: 0.5924\n",
            "Epoch 105/400\n",
            "52/52 [==============================] - 32s 610ms/step - loss: 0.1053 - acc: 0.9760 - val_loss: 2.0147 - val_acc: 0.6204\n",
            "Epoch 106/400\n",
            "52/52 [==============================] - 32s 610ms/step - loss: 0.1042 - acc: 0.9687 - val_loss: 2.0350 - val_acc: 0.6091\n",
            "Epoch 107/400\n",
            "52/52 [==============================] - 32s 610ms/step - loss: 0.0969 - acc: 0.9662 - val_loss: 2.6865 - val_acc: 0.5552\n",
            "Epoch 108/400\n",
            "52/52 [==============================] - 32s 610ms/step - loss: 0.1029 - acc: 0.9651 - val_loss: 2.9757 - val_acc: 0.5071\n",
            "Epoch 109/400\n",
            "52/52 [==============================] - 32s 610ms/step - loss: 0.1607 - acc: 0.9445 - val_loss: 2.1985 - val_acc: 0.5694\n",
            "Epoch 110/400\n",
            "52/52 [==============================] - 32s 611ms/step - loss: 0.1224 - acc: 0.9627 - val_loss: 2.0603 - val_acc: 0.6006\n",
            "Epoch 111/400\n",
            "52/52 [==============================] - 32s 611ms/step - loss: 0.1255 - acc: 0.9662 - val_loss: 2.1692 - val_acc: 0.6006\n",
            "Epoch 112/400\n",
            "52/52 [==============================] - 32s 612ms/step - loss: 0.1253 - acc: 0.9624 - val_loss: 1.8899 - val_acc: 0.6176\n",
            "Epoch 113/400\n",
            "52/52 [==============================] - 32s 612ms/step - loss: 0.0911 - acc: 0.9724 - val_loss: 3.1493 - val_acc: 0.4674\n",
            "Epoch 114/400\n",
            "52/52 [==============================] - 32s 611ms/step - loss: 0.1086 - acc: 0.9658 - val_loss: 2.5491 - val_acc: 0.5637\n",
            "Epoch 115/400\n",
            "52/52 [==============================] - 32s 612ms/step - loss: 0.0904 - acc: 0.9772 - val_loss: 2.5160 - val_acc: 0.5382\n",
            "Epoch 116/400\n",
            "52/52 [==============================] - 32s 612ms/step - loss: 0.0905 - acc: 0.9760 - val_loss: 2.2369 - val_acc: 0.6402\n",
            "Epoch 117/400\n",
            "52/52 [==============================] - 32s 611ms/step - loss: 0.1407 - acc: 0.9626 - val_loss: 2.9283 - val_acc: 0.5581\n",
            "Epoch 118/400\n",
            "52/52 [==============================] - 32s 611ms/step - loss: 0.1149 - acc: 0.9651 - val_loss: 1.8145 - val_acc: 0.6374\n",
            "Epoch 119/400\n",
            "52/52 [==============================] - 32s 613ms/step - loss: 0.1371 - acc: 0.9579 - val_loss: 2.5967 - val_acc: 0.5666\n",
            "Epoch 120/400\n",
            "52/52 [==============================] - 32s 612ms/step - loss: 0.1479 - acc: 0.9519 - val_loss: 1.7532 - val_acc: 0.6006\n",
            "Epoch 121/400\n",
            "52/52 [==============================] - 32s 613ms/step - loss: 0.0820 - acc: 0.9758 - val_loss: 2.3354 - val_acc: 0.5751\n",
            "Epoch 122/400\n",
            "52/52 [==============================] - 32s 612ms/step - loss: 0.1406 - acc: 0.9507 - val_loss: 2.1744 - val_acc: 0.5949\n",
            "Epoch 123/400\n",
            "52/52 [==============================] - 32s 608ms/step - loss: 0.1775 - acc: 0.9382 - val_loss: 2.1350 - val_acc: 0.5977\n",
            "Epoch 124/400\n",
            "52/52 [==============================] - 32s 611ms/step - loss: 0.2091 - acc: 0.9351 - val_loss: 3.0162 - val_acc: 0.5496\n",
            "Epoch 125/400\n",
            "52/52 [==============================] - 32s 611ms/step - loss: 0.1338 - acc: 0.9675 - val_loss: 1.7908 - val_acc: 0.5892\n",
            "Epoch 126/400\n",
            "52/52 [==============================] - 32s 613ms/step - loss: 0.1151 - acc: 0.9663 - val_loss: 2.1545 - val_acc: 0.5949\n",
            "Epoch 127/400\n",
            "52/52 [==============================] - 32s 610ms/step - loss: 0.0935 - acc: 0.9736 - val_loss: 2.1854 - val_acc: 0.5836\n",
            "Epoch 128/400\n",
            "52/52 [==============================] - 32s 614ms/step - loss: 0.0741 - acc: 0.9770 - val_loss: 2.0457 - val_acc: 0.6277\n",
            "Epoch 129/400\n",
            "52/52 [==============================] - 32s 611ms/step - loss: 0.1270 - acc: 0.9555 - val_loss: 2.3301 - val_acc: 0.5864\n",
            "Epoch 130/400\n",
            "52/52 [==============================] - 32s 611ms/step - loss: 0.1099 - acc: 0.9638 - val_loss: 2.3759 - val_acc: 0.5751\n",
            "Epoch 131/400\n",
            "52/52 [==============================] - 32s 612ms/step - loss: 0.1480 - acc: 0.9543 - val_loss: 3.3771 - val_acc: 0.5524\n",
            "Epoch 132/400\n",
            "52/52 [==============================] - 32s 610ms/step - loss: 0.1172 - acc: 0.9638 - val_loss: 2.9546 - val_acc: 0.5326\n",
            "Epoch 133/400\n",
            "52/52 [==============================] - 32s 609ms/step - loss: 0.0988 - acc: 0.9627 - val_loss: 2.1324 - val_acc: 0.5807\n",
            "Epoch 134/400\n",
            "52/52 [==============================] - 32s 611ms/step - loss: 0.1094 - acc: 0.9675 - val_loss: 2.7217 - val_acc: 0.5779\n",
            "Epoch 135/400\n",
            "52/52 [==============================] - 32s 612ms/step - loss: 0.2051 - acc: 0.9384 - val_loss: 1.7118 - val_acc: 0.6686\n",
            "Epoch 136/400\n",
            "52/52 [==============================] - 32s 612ms/step - loss: 0.1371 - acc: 0.9579 - val_loss: 1.6139 - val_acc: 0.6516\n",
            "Epoch 137/400\n",
            "52/52 [==============================] - 32s 615ms/step - loss: 0.1302 - acc: 0.9651 - val_loss: 4.2344 - val_acc: 0.4618\n",
            "Epoch 138/400\n",
            "52/52 [==============================] - 32s 614ms/step - loss: 0.1214 - acc: 0.9639 - val_loss: 2.0011 - val_acc: 0.5949\n",
            "Epoch 139/400\n",
            "52/52 [==============================] - 32s 615ms/step - loss: 0.0913 - acc: 0.9700 - val_loss: 1.7844 - val_acc: 0.6091\n",
            "Epoch 140/400\n",
            "52/52 [==============================] - 32s 612ms/step - loss: 0.0880 - acc: 0.9712 - val_loss: 2.2122 - val_acc: 0.6034\n",
            "Epoch 141/400\n",
            "52/52 [==============================] - 32s 614ms/step - loss: 0.1684 - acc: 0.9445 - val_loss: 1.7959 - val_acc: 0.6402\n",
            "Epoch 142/400\n",
            "52/52 [==============================] - 32s 615ms/step - loss: 0.1120 - acc: 0.9686 - val_loss: 2.4317 - val_acc: 0.5496\n",
            "Epoch 143/400\n",
            "52/52 [==============================] - 32s 616ms/step - loss: 0.1518 - acc: 0.9542 - val_loss: 1.6926 - val_acc: 0.6431\n",
            "Epoch 144/400\n",
            "52/52 [==============================] - 32s 615ms/step - loss: 0.1066 - acc: 0.9675 - val_loss: 2.7454 - val_acc: 0.4901\n",
            "Epoch 145/400\n",
            "52/52 [==============================] - 32s 613ms/step - loss: 0.0807 - acc: 0.9748 - val_loss: 2.1162 - val_acc: 0.5864\n",
            "Epoch 146/400\n",
            "52/52 [==============================] - 32s 611ms/step - loss: 0.0705 - acc: 0.9734 - val_loss: 2.2520 - val_acc: 0.6459\n",
            "Epoch 147/400\n",
            "52/52 [==============================] - 32s 612ms/step - loss: 0.1413 - acc: 0.9603 - val_loss: 2.0967 - val_acc: 0.5807\n",
            "Epoch 148/400\n",
            "52/52 [==============================] - 32s 611ms/step - loss: 0.1172 - acc: 0.9615 - val_loss: 1.9753 - val_acc: 0.6232\n",
            "Epoch 149/400\n",
            "52/52 [==============================] - 32s 615ms/step - loss: 0.0799 - acc: 0.9734 - val_loss: 2.2820 - val_acc: 0.5949\n",
            "Epoch 150/400\n",
            "52/52 [==============================] - 32s 612ms/step - loss: 0.0599 - acc: 0.9844 - val_loss: 2.4936 - val_acc: 0.5581\n",
            "Epoch 151/400\n",
            "52/52 [==============================] - 32s 612ms/step - loss: 0.1708 - acc: 0.9507 - val_loss: 1.8368 - val_acc: 0.6176\n",
            "Epoch 152/400\n",
            "52/52 [==============================] - 32s 616ms/step - loss: 0.1172 - acc: 0.9686 - val_loss: 1.8265 - val_acc: 0.6304\n",
            "Epoch 153/400\n",
            "52/52 [==============================] - 32s 613ms/step - loss: 0.1455 - acc: 0.9555 - val_loss: 2.5268 - val_acc: 0.5524\n",
            "Epoch 154/400\n",
            "52/52 [==============================] - 32s 611ms/step - loss: 0.1359 - acc: 0.9578 - val_loss: 2.2017 - val_acc: 0.5637\n",
            "Epoch 155/400\n",
            "52/52 [==============================] - 32s 614ms/step - loss: 0.0904 - acc: 0.9712 - val_loss: 1.9218 - val_acc: 0.6232\n",
            "Epoch 156/400\n",
            "52/52 [==============================] - 32s 612ms/step - loss: 0.0774 - acc: 0.9830 - val_loss: 1.9847 - val_acc: 0.6091\n",
            "Epoch 157/400\n",
            "52/52 [==============================] - 32s 613ms/step - loss: 0.0800 - acc: 0.9772 - val_loss: 3.5565 - val_acc: 0.5099\n",
            "Epoch 158/400\n",
            "52/52 [==============================] - 32s 617ms/step - loss: 0.0962 - acc: 0.9663 - val_loss: 2.9759 - val_acc: 0.5609\n",
            "Epoch 159/400\n",
            "52/52 [==============================] - 32s 616ms/step - loss: 0.1180 - acc: 0.9698 - val_loss: 2.3582 - val_acc: 0.5864\n",
            "Epoch 160/400\n",
            "52/52 [==============================] - 32s 617ms/step - loss: 0.0911 - acc: 0.9734 - val_loss: 2.3989 - val_acc: 0.5552\n",
            "Epoch 161/400\n",
            "52/52 [==============================] - 32s 616ms/step - loss: 0.1300 - acc: 0.9566 - val_loss: 2.0448 - val_acc: 0.6147\n",
            "Epoch 162/400\n",
            "52/52 [==============================] - 32s 617ms/step - loss: 0.0853 - acc: 0.9736 - val_loss: 2.5908 - val_acc: 0.5212\n",
            "Epoch 163/400\n",
            "52/52 [==============================] - 32s 615ms/step - loss: 0.0694 - acc: 0.9782 - val_loss: 2.5100 - val_acc: 0.5552\n",
            "Epoch 164/400\n",
            "52/52 [==============================] - 32s 617ms/step - loss: 0.1023 - acc: 0.9639 - val_loss: 2.4765 - val_acc: 0.5382\n",
            "Epoch 165/400\n",
            "52/52 [==============================] - 32s 615ms/step - loss: 0.1325 - acc: 0.9567 - val_loss: 2.8919 - val_acc: 0.4958\n",
            "Epoch 166/400\n",
            "52/52 [==============================] - 32s 616ms/step - loss: 0.0986 - acc: 0.9663 - val_loss: 3.0901 - val_acc: 0.5127\n",
            "Epoch 167/400\n",
            "52/52 [==============================] - 32s 616ms/step - loss: 0.1103 - acc: 0.9650 - val_loss: 1.9470 - val_acc: 0.6317\n",
            "Epoch 168/400\n",
            "52/52 [==============================] - 32s 615ms/step - loss: 0.1079 - acc: 0.9650 - val_loss: 1.8739 - val_acc: 0.5892\n",
            "Epoch 169/400\n",
            "52/52 [==============================] - 32s 615ms/step - loss: 0.1013 - acc: 0.9651 - val_loss: 2.3529 - val_acc: 0.5921\n",
            "Epoch 170/400\n",
            "52/52 [==============================] - 32s 615ms/step - loss: 0.0753 - acc: 0.9794 - val_loss: 1.9550 - val_acc: 0.6402\n",
            "Epoch 171/400\n",
            "52/52 [==============================] - 32s 615ms/step - loss: 0.0591 - acc: 0.9794 - val_loss: 2.5125 - val_acc: 0.5467\n",
            "Epoch 172/400\n",
            "52/52 [==============================] - 32s 615ms/step - loss: 0.1578 - acc: 0.9483 - val_loss: 2.2158 - val_acc: 0.5864\n",
            "Epoch 173/400\n",
            "52/52 [==============================] - 32s 615ms/step - loss: 0.0954 - acc: 0.9579 - val_loss: 2.0514 - val_acc: 0.6062\n",
            "Epoch 174/400\n",
            "52/52 [==============================] - 32s 616ms/step - loss: 0.0742 - acc: 0.9784 - val_loss: 2.1621 - val_acc: 0.6091\n",
            "Epoch 175/400\n",
            "52/52 [==============================] - 32s 614ms/step - loss: 0.1058 - acc: 0.9722 - val_loss: 2.6158 - val_acc: 0.5751\n",
            "Epoch 176/400\n",
            "52/52 [==============================] - 32s 618ms/step - loss: 0.0953 - acc: 0.9650 - val_loss: 1.9815 - val_acc: 0.6168\n",
            "Epoch 177/400\n",
            "52/52 [==============================] - 32s 616ms/step - loss: 0.1115 - acc: 0.9591 - val_loss: 1.8869 - val_acc: 0.6006\n",
            "Epoch 178/400\n",
            "52/52 [==============================] - 32s 615ms/step - loss: 0.0931 - acc: 0.9712 - val_loss: 2.0126 - val_acc: 0.6091\n",
            "Epoch 179/400\n",
            "52/52 [==============================] - 32s 617ms/step - loss: 0.0875 - acc: 0.9675 - val_loss: 2.1937 - val_acc: 0.6062\n",
            "Epoch 180/400\n",
            "52/52 [==============================] - 32s 613ms/step - loss: 0.1216 - acc: 0.9636 - val_loss: 2.5310 - val_acc: 0.5722\n",
            "Epoch 181/400\n",
            "52/52 [==============================] - 32s 612ms/step - loss: 0.1325 - acc: 0.9612 - val_loss: 2.0963 - val_acc: 0.5836\n",
            "Epoch 182/400\n",
            "52/52 [==============================] - 32s 612ms/step - loss: 0.1053 - acc: 0.9674 - val_loss: 2.3921 - val_acc: 0.5496\n",
            "Epoch 183/400\n",
            "52/52 [==============================] - 32s 612ms/step - loss: 0.0339 - acc: 0.9902 - val_loss: 1.8678 - val_acc: 0.6459\n",
            "Epoch 184/400\n",
            "52/52 [==============================] - 32s 613ms/step - loss: 0.0596 - acc: 0.9832 - val_loss: 1.9243 - val_acc: 0.5864\n",
            "Epoch 185/400\n",
            "52/52 [==============================] - 32s 612ms/step - loss: 0.1073 - acc: 0.9724 - val_loss: 2.4499 - val_acc: 0.5779\n",
            "Epoch 186/400\n",
            "52/52 [==============================] - 32s 613ms/step - loss: 0.0965 - acc: 0.9700 - val_loss: 2.5816 - val_acc: 0.5694\n",
            "Epoch 187/400\n",
            "52/52 [==============================] - 32s 613ms/step - loss: 0.0761 - acc: 0.9772 - val_loss: 2.8250 - val_acc: 0.5269\n",
            "Epoch 188/400\n",
            "52/52 [==============================] - 32s 614ms/step - loss: 0.0835 - acc: 0.9748 - val_loss: 2.6321 - val_acc: 0.5836\n",
            "Epoch 189/400\n",
            "52/52 [==============================] - 32s 612ms/step - loss: 0.0739 - acc: 0.9770 - val_loss: 2.3105 - val_acc: 0.6204\n",
            "Epoch 190/400\n",
            "52/52 [==============================] - 32s 614ms/step - loss: 0.0763 - acc: 0.9675 - val_loss: 2.8432 - val_acc: 0.5637\n",
            "Epoch 191/400\n",
            "52/52 [==============================] - 32s 613ms/step - loss: 0.1199 - acc: 0.9591 - val_loss: 2.4064 - val_acc: 0.5836\n",
            "Epoch 192/400\n",
            "52/52 [==============================] - 32s 613ms/step - loss: 0.0831 - acc: 0.9710 - val_loss: 2.4563 - val_acc: 0.5949\n",
            "Epoch 193/400\n",
            "52/52 [==============================] - 32s 613ms/step - loss: 0.1658 - acc: 0.9531 - val_loss: 2.2222 - val_acc: 0.5779\n",
            "Epoch 194/400\n",
            "52/52 [==============================] - 32s 615ms/step - loss: 0.1241 - acc: 0.9567 - val_loss: 2.3185 - val_acc: 0.5921\n",
            "Epoch 195/400\n",
            "52/52 [==============================] - 32s 616ms/step - loss: 0.0966 - acc: 0.9687 - val_loss: 2.2042 - val_acc: 0.6204\n",
            "Epoch 196/400\n",
            "52/52 [==============================] - 32s 617ms/step - loss: 0.0764 - acc: 0.9772 - val_loss: 2.3980 - val_acc: 0.5892\n",
            "Epoch 197/400\n",
            "52/52 [==============================] - 32s 616ms/step - loss: 0.1020 - acc: 0.9710 - val_loss: 2.5402 - val_acc: 0.5496\n",
            "Epoch 198/400\n",
            "52/52 [==============================] - 32s 616ms/step - loss: 0.1010 - acc: 0.9658 - val_loss: 2.4419 - val_acc: 0.5892\n",
            "Epoch 199/400\n",
            "52/52 [==============================] - 32s 614ms/step - loss: 0.1148 - acc: 0.9636 - val_loss: 2.0613 - val_acc: 0.5694\n",
            "Epoch 200/400\n",
            "52/52 [==============================] - 32s 619ms/step - loss: 0.1259 - acc: 0.9674 - val_loss: 2.4165 - val_acc: 0.5815\n",
            "Epoch 201/400\n",
            "52/52 [==============================] - 32s 614ms/step - loss: 0.1023 - acc: 0.9627 - val_loss: 2.3107 - val_acc: 0.5751\n",
            "Epoch 202/400\n",
            "52/52 [==============================] - 32s 614ms/step - loss: 0.1159 - acc: 0.9684 - val_loss: 2.0513 - val_acc: 0.6232\n",
            "Epoch 203/400\n",
            "52/52 [==============================] - 32s 612ms/step - loss: 0.0802 - acc: 0.9722 - val_loss: 2.1317 - val_acc: 0.6346\n",
            "Epoch 204/400\n",
            "52/52 [==============================] - 32s 614ms/step - loss: 0.0656 - acc: 0.9784 - val_loss: 2.9743 - val_acc: 0.5751\n",
            "Epoch 205/400\n",
            "52/52 [==============================] - 32s 611ms/step - loss: 0.0998 - acc: 0.9675 - val_loss: 2.8526 - val_acc: 0.5921\n",
            "Epoch 206/400\n",
            "52/52 [==============================] - 32s 615ms/step - loss: 0.0922 - acc: 0.9712 - val_loss: 3.1915 - val_acc: 0.5156\n",
            "Epoch 207/400\n",
            "52/52 [==============================] - 32s 615ms/step - loss: 0.1494 - acc: 0.9588 - val_loss: 2.3179 - val_acc: 0.5836\n",
            "Epoch 208/400\n",
            "52/52 [==============================] - 32s 613ms/step - loss: 0.0603 - acc: 0.9832 - val_loss: 1.8677 - val_acc: 0.6572\n",
            "Epoch 209/400\n",
            "52/52 [==============================] - 32s 615ms/step - loss: 0.1063 - acc: 0.9698 - val_loss: 2.1466 - val_acc: 0.6261\n",
            "Epoch 210/400\n",
            "52/52 [==============================] - 32s 616ms/step - loss: 0.0866 - acc: 0.9687 - val_loss: 2.8722 - val_acc: 0.5836\n",
            "Epoch 211/400\n",
            "52/52 [==============================] - 32s 614ms/step - loss: 0.0716 - acc: 0.9820 - val_loss: 2.2214 - val_acc: 0.5977\n",
            "Epoch 212/400\n",
            "52/52 [==============================] - 32s 615ms/step - loss: 0.0731 - acc: 0.9808 - val_loss: 1.7663 - val_acc: 0.6544\n",
            "Epoch 213/400\n",
            "52/52 [==============================] - 32s 613ms/step - loss: 0.0645 - acc: 0.9770 - val_loss: 1.8886 - val_acc: 0.6176\n",
            "Epoch 214/400\n",
            "52/52 [==============================] - 32s 615ms/step - loss: 0.0963 - acc: 0.9710 - val_loss: 2.0889 - val_acc: 0.6289\n",
            "Epoch 215/400\n",
            "52/52 [==============================] - 32s 615ms/step - loss: 0.1086 - acc: 0.9615 - val_loss: 1.9988 - val_acc: 0.6062\n",
            "Epoch 216/400\n",
            "52/52 [==============================] - 32s 614ms/step - loss: 0.1314 - acc: 0.9603 - val_loss: 2.0664 - val_acc: 0.6147\n",
            "Epoch 217/400\n",
            "52/52 [==============================] - 32s 615ms/step - loss: 0.0674 - acc: 0.9832 - val_loss: 2.5090 - val_acc: 0.5779\n",
            "Epoch 218/400\n",
            "52/52 [==============================] - 32s 616ms/step - loss: 0.1005 - acc: 0.9748 - val_loss: 2.8015 - val_acc: 0.5524\n",
            "Epoch 219/400\n",
            "52/52 [==============================] - 32s 613ms/step - loss: 0.1384 - acc: 0.9626 - val_loss: 1.9989 - val_acc: 0.6204\n",
            "Epoch 220/400\n",
            "52/52 [==============================] - 32s 613ms/step - loss: 0.0587 - acc: 0.9815 - val_loss: 1.9095 - val_acc: 0.6459\n",
            "Epoch 221/400\n",
            "52/52 [==============================] - 32s 614ms/step - loss: 0.0921 - acc: 0.9684 - val_loss: 2.5389 - val_acc: 0.5751\n",
            "Epoch 222/400\n",
            "52/52 [==============================] - 32s 614ms/step - loss: 0.0693 - acc: 0.9820 - val_loss: 2.2546 - val_acc: 0.6147\n",
            "Epoch 223/400\n",
            "52/52 [==============================] - 32s 613ms/step - loss: 0.0842 - acc: 0.9806 - val_loss: 2.5621 - val_acc: 0.5694\n",
            "Epoch 224/400\n",
            "52/52 [==============================] - 32s 617ms/step - loss: 0.0987 - acc: 0.9698 - val_loss: 2.5896 - val_acc: 0.5842\n",
            "Epoch 225/400\n",
            "52/52 [==============================] - 32s 613ms/step - loss: 0.1003 - acc: 0.9662 - val_loss: 1.9486 - val_acc: 0.6261\n",
            "Epoch 226/400\n",
            "52/52 [==============================] - 32s 614ms/step - loss: 0.1056 - acc: 0.9675 - val_loss: 2.1632 - val_acc: 0.5864\n",
            "Epoch 227/400\n",
            "52/52 [==============================] - 32s 613ms/step - loss: 0.0916 - acc: 0.9736 - val_loss: 2.5165 - val_acc: 0.5977\n",
            "Epoch 228/400\n",
            "52/52 [==============================] - 32s 614ms/step - loss: 0.0793 - acc: 0.9736 - val_loss: 1.9013 - val_acc: 0.6346\n",
            "Epoch 229/400\n",
            "52/52 [==============================] - 32s 613ms/step - loss: 0.0459 - acc: 0.9854 - val_loss: 2.2330 - val_acc: 0.6232\n",
            "Epoch 230/400\n",
            "52/52 [==============================] - 32s 616ms/step - loss: 0.0396 - acc: 0.9892 - val_loss: 2.5182 - val_acc: 0.6119\n",
            "Epoch 231/400\n",
            "52/52 [==============================] - 32s 613ms/step - loss: 0.0615 - acc: 0.9803 - val_loss: 1.9043 - val_acc: 0.6402\n",
            "Epoch 232/400\n",
            "52/52 [==============================] - 32s 614ms/step - loss: 0.0389 - acc: 0.9856 - val_loss: 1.9380 - val_acc: 0.6176\n",
            "Epoch 233/400\n",
            "52/52 [==============================] - 32s 612ms/step - loss: 0.0692 - acc: 0.9748 - val_loss: 2.6511 - val_acc: 0.5977\n",
            "Epoch 234/400\n",
            "52/52 [==============================] - 32s 612ms/step - loss: 0.0858 - acc: 0.9724 - val_loss: 2.9064 - val_acc: 0.5751\n",
            "Epoch 235/400\n",
            "52/52 [==============================] - 32s 612ms/step - loss: 0.1192 - acc: 0.9603 - val_loss: 3.5218 - val_acc: 0.4703\n",
            "Epoch 236/400\n",
            "52/52 [==============================] - 32s 613ms/step - loss: 0.1266 - acc: 0.9648 - val_loss: 2.2591 - val_acc: 0.5949\n",
            "Epoch 237/400\n",
            "52/52 [==============================] - 32s 613ms/step - loss: 0.1027 - acc: 0.9663 - val_loss: 2.6636 - val_acc: 0.5779\n",
            "Epoch 238/400\n",
            "52/52 [==============================] - 32s 617ms/step - loss: 0.0856 - acc: 0.9748 - val_loss: 2.4545 - val_acc: 0.5722\n",
            "Epoch 239/400\n",
            "52/52 [==============================] - 32s 617ms/step - loss: 0.1483 - acc: 0.9590 - val_loss: 2.9458 - val_acc: 0.5496\n",
            "Epoch 240/400\n",
            "52/52 [==============================] - 32s 618ms/step - loss: 0.0860 - acc: 0.9746 - val_loss: 3.0226 - val_acc: 0.5326\n",
            "Epoch 241/400\n",
            "52/52 [==============================] - 32s 616ms/step - loss: 0.0856 - acc: 0.9772 - val_loss: 2.6982 - val_acc: 0.5921\n",
            "Epoch 242/400\n",
            "52/52 [==============================] - 32s 617ms/step - loss: 0.0992 - acc: 0.9687 - val_loss: 1.9418 - val_acc: 0.6232\n",
            "Epoch 243/400\n",
            "52/52 [==============================] - 32s 615ms/step - loss: 0.0801 - acc: 0.9784 - val_loss: 2.1176 - val_acc: 0.6289\n",
            "Epoch 244/400\n",
            "52/52 [==============================] - 32s 616ms/step - loss: 0.0702 - acc: 0.9748 - val_loss: 2.4061 - val_acc: 0.5694\n",
            "Epoch 245/400\n",
            "52/52 [==============================] - 32s 616ms/step - loss: 0.0570 - acc: 0.9806 - val_loss: 2.1499 - val_acc: 0.5977\n",
            "Epoch 246/400\n",
            "52/52 [==============================] - 32s 617ms/step - loss: 0.0506 - acc: 0.9806 - val_loss: 2.1946 - val_acc: 0.6091\n",
            "Epoch 247/400\n",
            "52/52 [==============================] - 32s 615ms/step - loss: 0.0804 - acc: 0.9687 - val_loss: 2.1842 - val_acc: 0.5892\n",
            "Epoch 248/400\n",
            "52/52 [==============================] - 32s 618ms/step - loss: 0.0999 - acc: 0.9712 - val_loss: 2.0273 - val_acc: 0.6332\n",
            "Epoch 249/400\n",
            "52/52 [==============================] - 32s 612ms/step - loss: 0.1057 - acc: 0.9675 - val_loss: 2.3724 - val_acc: 0.5892\n",
            "Epoch 250/400\n",
            "52/52 [==============================] - 32s 614ms/step - loss: 0.0675 - acc: 0.9736 - val_loss: 2.1222 - val_acc: 0.6232\n",
            "Epoch 251/400\n",
            "52/52 [==============================] - 32s 613ms/step - loss: 0.0605 - acc: 0.9832 - val_loss: 2.0191 - val_acc: 0.5864\n",
            "Epoch 252/400\n",
            "52/52 [==============================] - 32s 614ms/step - loss: 0.0570 - acc: 0.9868 - val_loss: 2.0899 - val_acc: 0.6176\n",
            "Epoch 253/400\n",
            "52/52 [==============================] - 32s 614ms/step - loss: 0.0740 - acc: 0.9808 - val_loss: 2.1292 - val_acc: 0.5751\n",
            "Epoch 254/400\n",
            "52/52 [==============================] - 32s 614ms/step - loss: 0.0990 - acc: 0.9724 - val_loss: 1.9478 - val_acc: 0.6544\n",
            "Epoch 255/400\n",
            "52/52 [==============================] - 32s 613ms/step - loss: 0.0654 - acc: 0.9830 - val_loss: 1.8574 - val_acc: 0.6572\n",
            "Epoch 256/400\n",
            "52/52 [==============================] - 32s 613ms/step - loss: 0.0942 - acc: 0.9705 - val_loss: 2.0952 - val_acc: 0.6062\n",
            "Epoch 257/400\n",
            "52/52 [==============================] - 32s 613ms/step - loss: 0.0633 - acc: 0.9818 - val_loss: 2.0338 - val_acc: 0.6601\n",
            "Epoch 258/400\n",
            "52/52 [==============================] - 32s 615ms/step - loss: 0.0603 - acc: 0.9758 - val_loss: 2.0755 - val_acc: 0.6006\n",
            "Epoch 259/400\n",
            "52/52 [==============================] - 32s 614ms/step - loss: 0.0630 - acc: 0.9816 - val_loss: 2.0531 - val_acc: 0.6516\n",
            "Epoch 260/400\n",
            "52/52 [==============================] - 32s 613ms/step - loss: 0.0398 - acc: 0.9856 - val_loss: 2.4669 - val_acc: 0.5836\n",
            "Epoch 261/400\n",
            "52/52 [==============================] - 32s 614ms/step - loss: 0.0576 - acc: 0.9796 - val_loss: 2.5247 - val_acc: 0.6034\n",
            "Epoch 262/400\n",
            "52/52 [==============================] - 32s 613ms/step - loss: 0.0897 - acc: 0.9724 - val_loss: 2.5090 - val_acc: 0.5722\n",
            "Epoch 263/400\n",
            "52/52 [==============================] - 32s 615ms/step - loss: 0.0665 - acc: 0.9808 - val_loss: 1.9024 - val_acc: 0.6487\n",
            "Epoch 264/400\n",
            "52/52 [==============================] - 32s 613ms/step - loss: 0.0480 - acc: 0.9820 - val_loss: 2.2651 - val_acc: 0.6147\n",
            "Epoch 265/400\n",
            "52/52 [==============================] - 32s 612ms/step - loss: 0.0556 - acc: 0.9748 - val_loss: 2.7351 - val_acc: 0.6232\n",
            "Epoch 266/400\n",
            "52/52 [==============================] - 32s 616ms/step - loss: 0.0804 - acc: 0.9760 - val_loss: 1.9465 - val_acc: 0.6516\n",
            "Epoch 267/400\n",
            "52/52 [==============================] - 32s 612ms/step - loss: 0.0739 - acc: 0.9806 - val_loss: 2.1690 - val_acc: 0.6147\n",
            "Epoch 268/400\n",
            "52/52 [==============================] - 32s 615ms/step - loss: 0.0493 - acc: 0.9818 - val_loss: 2.4520 - val_acc: 0.5807\n",
            "Epoch 269/400\n",
            "52/52 [==============================] - 32s 613ms/step - loss: 0.0442 - acc: 0.9868 - val_loss: 2.2641 - val_acc: 0.6232\n",
            "Epoch 270/400\n",
            "52/52 [==============================] - 32s 616ms/step - loss: 0.0848 - acc: 0.9808 - val_loss: 2.1321 - val_acc: 0.6431\n",
            "Epoch 271/400\n",
            "52/52 [==============================] - 32s 614ms/step - loss: 0.1152 - acc: 0.9724 - val_loss: 2.1250 - val_acc: 0.6062\n",
            "Epoch 272/400\n",
            "52/52 [==============================] - 32s 616ms/step - loss: 0.0701 - acc: 0.9748 - val_loss: 2.0597 - val_acc: 0.6060\n",
            "Epoch 273/400\n",
            "52/52 [==============================] - 32s 615ms/step - loss: 0.0595 - acc: 0.9820 - val_loss: 2.1428 - val_acc: 0.6346\n",
            "Epoch 274/400\n",
            "52/52 [==============================] - 32s 615ms/step - loss: 0.0660 - acc: 0.9784 - val_loss: 4.4656 - val_acc: 0.4419\n",
            "Epoch 275/400\n",
            "52/52 [==============================] - 32s 614ms/step - loss: 0.1521 - acc: 0.9566 - val_loss: 2.1719 - val_acc: 0.5921\n",
            "Epoch 276/400\n",
            "52/52 [==============================] - 32s 615ms/step - loss: 0.0617 - acc: 0.9772 - val_loss: 1.9729 - val_acc: 0.6232\n",
            "Epoch 277/400\n",
            "52/52 [==============================] - 32s 613ms/step - loss: 0.0693 - acc: 0.9784 - val_loss: 1.9514 - val_acc: 0.6289\n",
            "Epoch 278/400\n",
            "52/52 [==============================] - 32s 615ms/step - loss: 0.0888 - acc: 0.9710 - val_loss: 2.3647 - val_acc: 0.5892\n",
            "Epoch 279/400\n",
            "52/52 [==============================] - 32s 614ms/step - loss: 0.0758 - acc: 0.9784 - val_loss: 2.8991 - val_acc: 0.5751\n",
            "Epoch 280/400\n",
            "52/52 [==============================] - 32s 615ms/step - loss: 0.0886 - acc: 0.9722 - val_loss: 2.1409 - val_acc: 0.6034\n",
            "Epoch 281/400\n",
            "52/52 [==============================] - 32s 615ms/step - loss: 0.0743 - acc: 0.9784 - val_loss: 1.7887 - val_acc: 0.6232\n",
            "Epoch 282/400\n",
            "52/52 [==============================] - 32s 614ms/step - loss: 0.0448 - acc: 0.9868 - val_loss: 1.9245 - val_acc: 0.6544\n",
            "Epoch 283/400\n",
            "52/52 [==============================] - 32s 614ms/step - loss: 0.0793 - acc: 0.9736 - val_loss: 2.3017 - val_acc: 0.5921\n",
            "Epoch 284/400\n",
            "52/52 [==============================] - 32s 617ms/step - loss: 0.0507 - acc: 0.9868 - val_loss: 1.8466 - val_acc: 0.6459\n",
            "Epoch 285/400\n",
            "52/52 [==============================] - 32s 616ms/step - loss: 0.0929 - acc: 0.9712 - val_loss: 2.2417 - val_acc: 0.5779\n",
            "Epoch 286/400\n",
            "52/52 [==============================] - 32s 616ms/step - loss: 0.0667 - acc: 0.9746 - val_loss: 2.2149 - val_acc: 0.6204\n",
            "Epoch 287/400\n",
            "52/52 [==============================] - 32s 617ms/step - loss: 0.0704 - acc: 0.9760 - val_loss: 1.9460 - val_acc: 0.6431\n",
            "Epoch 288/400\n",
            "52/52 [==============================] - 32s 617ms/step - loss: 0.0374 - acc: 0.9904 - val_loss: 2.1101 - val_acc: 0.6346\n",
            "Epoch 289/400\n",
            "52/52 [==============================] - 32s 614ms/step - loss: 0.0729 - acc: 0.9796 - val_loss: 2.4299 - val_acc: 0.5977\n",
            "Epoch 290/400\n",
            "52/52 [==============================] - 32s 615ms/step - loss: 0.0586 - acc: 0.9820 - val_loss: 2.3632 - val_acc: 0.5892\n",
            "Epoch 291/400\n",
            "52/52 [==============================] - 32s 617ms/step - loss: 0.0709 - acc: 0.9770 - val_loss: 2.5475 - val_acc: 0.5921\n",
            "Epoch 292/400\n",
            "52/52 [==============================] - 32s 617ms/step - loss: 0.0525 - acc: 0.9844 - val_loss: 2.3334 - val_acc: 0.5779\n",
            "Epoch 293/400\n",
            "52/52 [==============================] - 32s 617ms/step - loss: 0.1003 - acc: 0.9651 - val_loss: 3.0390 - val_acc: 0.5722\n",
            "Epoch 294/400\n",
            "52/52 [==============================] - 32s 615ms/step - loss: 0.0631 - acc: 0.9796 - val_loss: 2.5941 - val_acc: 0.5637\n",
            "Epoch 295/400\n",
            "52/52 [==============================] - 32s 615ms/step - loss: 0.0884 - acc: 0.9796 - val_loss: 2.5379 - val_acc: 0.5836\n",
            "Epoch 296/400\n",
            "52/52 [==============================] - 32s 619ms/step - loss: 0.0963 - acc: 0.9784 - val_loss: 2.6858 - val_acc: 0.5489\n",
            "Epoch 297/400\n",
            "52/52 [==============================] - 32s 615ms/step - loss: 0.0678 - acc: 0.9808 - val_loss: 2.3764 - val_acc: 0.6261\n",
            "Epoch 298/400\n",
            "52/52 [==============================] - 32s 616ms/step - loss: 0.0712 - acc: 0.9770 - val_loss: 2.3093 - val_acc: 0.6261\n",
            "Epoch 299/400\n",
            "52/52 [==============================] - 32s 616ms/step - loss: 0.0815 - acc: 0.9760 - val_loss: 2.8328 - val_acc: 0.5411\n",
            "Epoch 300/400\n",
            "52/52 [==============================] - 32s 616ms/step - loss: 0.0654 - acc: 0.9796 - val_loss: 3.1353 - val_acc: 0.5071\n",
            "Epoch 301/400\n",
            "52/52 [==============================] - 32s 616ms/step - loss: 0.0716 - acc: 0.9808 - val_loss: 3.1710 - val_acc: 0.5184\n",
            "Epoch 302/400\n",
            "52/52 [==============================] - 32s 616ms/step - loss: 0.0488 - acc: 0.9868 - val_loss: 2.6690 - val_acc: 0.6232\n",
            "Epoch 303/400\n",
            "52/52 [==============================] - 32s 616ms/step - loss: 0.0717 - acc: 0.9772 - val_loss: 2.1621 - val_acc: 0.6176\n",
            "Epoch 304/400\n",
            "52/52 [==============================] - 32s 616ms/step - loss: 0.0925 - acc: 0.9748 - val_loss: 2.6316 - val_acc: 0.5836\n",
            "Epoch 305/400\n",
            "52/52 [==============================] - 32s 615ms/step - loss: 0.1011 - acc: 0.9624 - val_loss: 2.2352 - val_acc: 0.6544\n",
            "Epoch 306/400\n",
            "52/52 [==============================] - 32s 613ms/step - loss: 0.0337 - acc: 0.9892 - val_loss: 2.2131 - val_acc: 0.6742\n",
            "Epoch 307/400\n",
            "52/52 [==============================] - 32s 612ms/step - loss: 0.0779 - acc: 0.9710 - val_loss: 2.6466 - val_acc: 0.5977\n",
            "Epoch 308/400\n",
            "52/52 [==============================] - 32s 613ms/step - loss: 0.0708 - acc: 0.9760 - val_loss: 2.5998 - val_acc: 0.5949\n",
            "Epoch 309/400\n",
            "52/52 [==============================] - 32s 612ms/step - loss: 0.0515 - acc: 0.9820 - val_loss: 2.3620 - val_acc: 0.6091\n",
            "Epoch 310/400\n",
            "52/52 [==============================] - 32s 612ms/step - loss: 0.0539 - acc: 0.9842 - val_loss: 2.9345 - val_acc: 0.5807\n",
            "Epoch 311/400\n",
            "52/52 [==============================] - 32s 613ms/step - loss: 0.0583 - acc: 0.9844 - val_loss: 2.7026 - val_acc: 0.5779\n",
            "Epoch 312/400\n",
            "52/52 [==============================] - 32s 612ms/step - loss: 0.1251 - acc: 0.9627 - val_loss: 3.0764 - val_acc: 0.4759\n",
            "Epoch 313/400\n",
            "52/52 [==============================] - 32s 612ms/step - loss: 0.0937 - acc: 0.9760 - val_loss: 1.7819 - val_acc: 0.6601\n",
            "Epoch 314/400\n",
            "52/52 [==============================] - 32s 612ms/step - loss: 0.0546 - acc: 0.9866 - val_loss: 2.3355 - val_acc: 0.5807\n",
            "Epoch 315/400\n",
            "52/52 [==============================] - 32s 613ms/step - loss: 0.0878 - acc: 0.9724 - val_loss: 2.1947 - val_acc: 0.6091\n",
            "Epoch 316/400\n",
            "52/52 [==============================] - 32s 612ms/step - loss: 0.0740 - acc: 0.9784 - val_loss: 2.1817 - val_acc: 0.5552\n",
            "Epoch 317/400\n",
            "52/52 [==============================] - 32s 614ms/step - loss: 0.0458 - acc: 0.9868 - val_loss: 1.8037 - val_acc: 0.6431\n",
            "Epoch 318/400\n",
            "52/52 [==============================] - 32s 612ms/step - loss: 0.0576 - acc: 0.9832 - val_loss: 2.1624 - val_acc: 0.6176\n",
            "Epoch 319/400\n",
            "52/52 [==============================] - 32s 613ms/step - loss: 0.0574 - acc: 0.9820 - val_loss: 1.9722 - val_acc: 0.6261\n",
            "Epoch 320/400\n",
            "52/52 [==============================] - 32s 613ms/step - loss: 0.0559 - acc: 0.9866 - val_loss: 1.9286 - val_acc: 0.6114\n",
            "Epoch 321/400\n",
            "52/52 [==============================] - 32s 611ms/step - loss: 0.0817 - acc: 0.9746 - val_loss: 2.4684 - val_acc: 0.5836\n",
            "Epoch 322/400\n",
            "52/52 [==============================] - 32s 610ms/step - loss: 0.0982 - acc: 0.9687 - val_loss: 2.3510 - val_acc: 0.5892\n",
            "Epoch 323/400\n",
            "52/52 [==============================] - 32s 612ms/step - loss: 0.0904 - acc: 0.9687 - val_loss: 2.2948 - val_acc: 0.6289\n",
            "Epoch 324/400\n",
            "52/52 [==============================] - 32s 610ms/step - loss: 0.0420 - acc: 0.9866 - val_loss: 2.1015 - val_acc: 0.6204\n",
            "Epoch 325/400\n",
            "52/52 [==============================] - 32s 610ms/step - loss: 0.0240 - acc: 0.9938 - val_loss: 2.0682 - val_acc: 0.6261\n",
            "Epoch 326/400\n",
            "52/52 [==============================] - 32s 612ms/step - loss: 0.0831 - acc: 0.9736 - val_loss: 2.1802 - val_acc: 0.6062\n",
            "Epoch 327/400\n",
            "52/52 [==============================] - 32s 611ms/step - loss: 0.0941 - acc: 0.9758 - val_loss: 2.7894 - val_acc: 0.5581\n",
            "Epoch 328/400\n",
            "52/52 [==============================] - 32s 610ms/step - loss: 0.0410 - acc: 0.9868 - val_loss: 2.2774 - val_acc: 0.6091\n",
            "Epoch 329/400\n",
            "52/52 [==============================] - 32s 612ms/step - loss: 0.0825 - acc: 0.9736 - val_loss: 2.5221 - val_acc: 0.5921\n",
            "Epoch 330/400\n",
            "52/52 [==============================] - 32s 611ms/step - loss: 0.0560 - acc: 0.9818 - val_loss: 2.3260 - val_acc: 0.6346\n",
            "Epoch 331/400\n",
            "52/52 [==============================] - 32s 611ms/step - loss: 0.0765 - acc: 0.9770 - val_loss: 2.6638 - val_acc: 0.5297\n",
            "Epoch 332/400\n",
            "52/52 [==============================] - 32s 611ms/step - loss: 0.0931 - acc: 0.9736 - val_loss: 2.5073 - val_acc: 0.5694\n",
            "Epoch 333/400\n",
            "52/52 [==============================] - 32s 612ms/step - loss: 0.0452 - acc: 0.9880 - val_loss: 2.2052 - val_acc: 0.6374\n",
            "Epoch 334/400\n",
            "52/52 [==============================] - 32s 611ms/step - loss: 0.0833 - acc: 0.9772 - val_loss: 2.7259 - val_acc: 0.5864\n",
            "Epoch 335/400\n",
            "52/52 [==============================] - 32s 611ms/step - loss: 0.0881 - acc: 0.9818 - val_loss: 2.2809 - val_acc: 0.5807\n",
            "Epoch 336/400\n",
            "52/52 [==============================] - 32s 613ms/step - loss: 0.0552 - acc: 0.9844 - val_loss: 2.2134 - val_acc: 0.5977\n",
            "Epoch 337/400\n",
            "52/52 [==============================] - 32s 612ms/step - loss: 0.0483 - acc: 0.9868 - val_loss: 1.6410 - val_acc: 0.6856\n",
            "Epoch 338/400\n",
            "52/52 [==============================] - 32s 609ms/step - loss: 0.0544 - acc: 0.9856 - val_loss: 2.1869 - val_acc: 0.5779\n",
            "Epoch 339/400\n",
            "52/52 [==============================] - 32s 612ms/step - loss: 0.0661 - acc: 0.9736 - val_loss: 2.6392 - val_acc: 0.5864\n",
            "Epoch 340/400\n",
            "52/52 [==============================] - 32s 611ms/step - loss: 0.0228 - acc: 0.9952 - val_loss: 2.1734 - val_acc: 0.6601\n",
            "Epoch 341/400\n",
            "52/52 [==============================] - 32s 610ms/step - loss: 0.0467 - acc: 0.9902 - val_loss: 2.2895 - val_acc: 0.6232\n",
            "Epoch 342/400\n",
            "52/52 [==============================] - 32s 613ms/step - loss: 0.0769 - acc: 0.9724 - val_loss: 2.0384 - val_acc: 0.6346\n",
            "Epoch 343/400\n",
            "52/52 [==============================] - 32s 612ms/step - loss: 0.0643 - acc: 0.9782 - val_loss: 2.3242 - val_acc: 0.6204\n",
            "Epoch 344/400\n",
            "52/52 [==============================] - 32s 615ms/step - loss: 0.0870 - acc: 0.9782 - val_loss: 2.6173 - val_acc: 0.5897\n",
            "Epoch 345/400\n",
            "52/52 [==============================] - 32s 611ms/step - loss: 0.0535 - acc: 0.9892 - val_loss: 2.0902 - val_acc: 0.6487\n",
            "Epoch 346/400\n",
            "52/52 [==============================] - 32s 611ms/step - loss: 0.0460 - acc: 0.9844 - val_loss: 2.2834 - val_acc: 0.6147\n",
            "Epoch 347/400\n",
            "52/52 [==============================] - 32s 612ms/step - loss: 0.0671 - acc: 0.9770 - val_loss: 2.4473 - val_acc: 0.6204\n",
            "Epoch 348/400\n",
            "52/52 [==============================] - 32s 611ms/step - loss: 0.0737 - acc: 0.9758 - val_loss: 3.0085 - val_acc: 0.5609\n",
            "Epoch 349/400\n",
            "52/52 [==============================] - 32s 612ms/step - loss: 0.1145 - acc: 0.9660 - val_loss: 3.0235 - val_acc: 0.5297\n",
            "Epoch 350/400\n",
            "52/52 [==============================] - 32s 611ms/step - loss: 0.0587 - acc: 0.9784 - val_loss: 1.8862 - val_acc: 0.6317\n",
            "Epoch 351/400\n",
            "52/52 [==============================] - 32s 610ms/step - loss: 0.0761 - acc: 0.9746 - val_loss: 2.6025 - val_acc: 0.5864\n",
            "Epoch 352/400\n",
            "52/52 [==============================] - 32s 614ms/step - loss: 0.0726 - acc: 0.9724 - val_loss: 1.8401 - val_acc: 0.6317\n",
            "Epoch 353/400\n",
            "52/52 [==============================] - 32s 614ms/step - loss: 0.0407 - acc: 0.9890 - val_loss: 2.4941 - val_acc: 0.5722\n",
            "Epoch 354/400\n",
            "52/52 [==============================] - 32s 616ms/step - loss: 0.0486 - acc: 0.9844 - val_loss: 2.3032 - val_acc: 0.6402\n",
            "Epoch 355/400\n",
            "52/52 [==============================] - 32s 613ms/step - loss: 0.1090 - acc: 0.9663 - val_loss: 1.9224 - val_acc: 0.6232\n",
            "Epoch 356/400\n",
            "52/52 [==============================] - 32s 615ms/step - loss: 0.0835 - acc: 0.9794 - val_loss: 2.7417 - val_acc: 0.5977\n",
            "Epoch 357/400\n",
            "52/52 [==============================] - 32s 614ms/step - loss: 0.0611 - acc: 0.9878 - val_loss: 1.6838 - val_acc: 0.6686\n",
            "Epoch 358/400\n",
            "52/52 [==============================] - 32s 614ms/step - loss: 0.0562 - acc: 0.9866 - val_loss: 2.3571 - val_acc: 0.6176\n",
            "Epoch 359/400\n",
            "52/52 [==============================] - 32s 612ms/step - loss: 0.0315 - acc: 0.9916 - val_loss: 2.2326 - val_acc: 0.6091\n",
            "Epoch 360/400\n",
            "52/52 [==============================] - 32s 613ms/step - loss: 0.0885 - acc: 0.9724 - val_loss: 2.1772 - val_acc: 0.5127\n",
            "Epoch 361/400\n",
            "52/52 [==============================] - 32s 615ms/step - loss: 0.0794 - acc: 0.9770 - val_loss: 2.3136 - val_acc: 0.6091\n",
            "Epoch 362/400\n",
            "52/52 [==============================] - 32s 614ms/step - loss: 0.0725 - acc: 0.9806 - val_loss: 2.0413 - val_acc: 0.6261\n",
            "Epoch 363/400\n",
            "52/52 [==============================] - 32s 611ms/step - loss: 0.0440 - acc: 0.9842 - val_loss: 2.1148 - val_acc: 0.6459\n",
            "Epoch 364/400\n",
            "52/52 [==============================] - 32s 615ms/step - loss: 0.0516 - acc: 0.9854 - val_loss: 2.0087 - val_acc: 0.6431\n",
            "Epoch 365/400\n",
            "52/52 [==============================] - 32s 613ms/step - loss: 0.0695 - acc: 0.9748 - val_loss: 3.7108 - val_acc: 0.5184\n",
            "Epoch 366/400\n",
            "52/52 [==============================] - 32s 615ms/step - loss: 0.0438 - acc: 0.9856 - val_loss: 2.2541 - val_acc: 0.5949\n",
            "Epoch 367/400\n",
            "52/52 [==============================] - 32s 614ms/step - loss: 0.0400 - acc: 0.9866 - val_loss: 1.9991 - val_acc: 0.6346\n",
            "Epoch 368/400\n",
            "52/52 [==============================] - 32s 618ms/step - loss: 0.0724 - acc: 0.9736 - val_loss: 2.0447 - val_acc: 0.6223\n",
            "Epoch 369/400\n",
            "52/52 [==============================] - 32s 614ms/step - loss: 0.0660 - acc: 0.9830 - val_loss: 2.1785 - val_acc: 0.6034\n",
            "Epoch 370/400\n",
            "52/52 [==============================] - 32s 612ms/step - loss: 0.0598 - acc: 0.9808 - val_loss: 3.0039 - val_acc: 0.5637\n",
            "Epoch 371/400\n",
            "52/52 [==============================] - 32s 614ms/step - loss: 0.0258 - acc: 0.9868 - val_loss: 2.1908 - val_acc: 0.6119\n",
            "Epoch 372/400\n",
            "52/52 [==============================] - 32s 614ms/step - loss: 0.0624 - acc: 0.9796 - val_loss: 1.9611 - val_acc: 0.6374\n",
            "Epoch 373/400\n",
            "52/52 [==============================] - 32s 614ms/step - loss: 0.0720 - acc: 0.9782 - val_loss: 2.6303 - val_acc: 0.5977\n",
            "Epoch 374/400\n",
            "52/52 [==============================] - 32s 612ms/step - loss: 0.0444 - acc: 0.9844 - val_loss: 2.4136 - val_acc: 0.6317\n",
            "Epoch 375/400\n",
            "52/52 [==============================] - 32s 610ms/step - loss: 0.0612 - acc: 0.9784 - val_loss: 1.9193 - val_acc: 0.6317\n",
            "Epoch 376/400\n",
            "52/52 [==============================] - 32s 612ms/step - loss: 0.0640 - acc: 0.9820 - val_loss: 2.3531 - val_acc: 0.6119\n",
            "Epoch 377/400\n",
            "52/52 [==============================] - 32s 611ms/step - loss: 0.0806 - acc: 0.9796 - val_loss: 2.3376 - val_acc: 0.6006\n",
            "Epoch 378/400\n",
            "52/52 [==============================] - 32s 611ms/step - loss: 0.0468 - acc: 0.9808 - val_loss: 1.9995 - val_acc: 0.6374\n",
            "Epoch 379/400\n",
            "52/52 [==============================] - 32s 611ms/step - loss: 0.0685 - acc: 0.9820 - val_loss: 2.0993 - val_acc: 0.6459\n",
            "Epoch 380/400\n",
            "52/52 [==============================] - 32s 611ms/step - loss: 0.0592 - acc: 0.9820 - val_loss: 2.0632 - val_acc: 0.6346\n",
            "Epoch 381/400\n",
            "52/52 [==============================] - 32s 611ms/step - loss: 0.0769 - acc: 0.9782 - val_loss: 2.1917 - val_acc: 0.6119\n",
            "Epoch 382/400\n",
            "52/52 [==============================] - 32s 611ms/step - loss: 0.0545 - acc: 0.9832 - val_loss: 1.8523 - val_acc: 0.6771\n",
            "Epoch 383/400\n",
            "52/52 [==============================] - 32s 611ms/step - loss: 0.0557 - acc: 0.9806 - val_loss: 2.5126 - val_acc: 0.5666\n",
            "Epoch 384/400\n",
            "52/52 [==============================] - 32s 611ms/step - loss: 0.0457 - acc: 0.9904 - val_loss: 2.5425 - val_acc: 0.6091\n",
            "Epoch 385/400\n",
            "52/52 [==============================] - 32s 611ms/step - loss: 0.0353 - acc: 0.9928 - val_loss: 2.5465 - val_acc: 0.6034\n",
            "Epoch 386/400\n",
            "52/52 [==============================] - 32s 612ms/step - loss: 0.1049 - acc: 0.9579 - val_loss: 2.7532 - val_acc: 0.5637\n",
            "Epoch 387/400\n",
            "52/52 [==============================] - 32s 614ms/step - loss: 0.0839 - acc: 0.9712 - val_loss: 2.3225 - val_acc: 0.5921\n",
            "Epoch 388/400\n",
            "52/52 [==============================] - 32s 614ms/step - loss: 0.0908 - acc: 0.9746 - val_loss: 2.1809 - val_acc: 0.6204\n",
            "Epoch 389/400\n",
            "52/52 [==============================] - 32s 616ms/step - loss: 0.0562 - acc: 0.9820 - val_loss: 2.0041 - val_acc: 0.6147\n",
            "Epoch 390/400\n",
            "52/52 [==============================] - 32s 614ms/step - loss: 0.0375 - acc: 0.9866 - val_loss: 1.9099 - val_acc: 0.6374\n",
            "Epoch 391/400\n",
            "52/52 [==============================] - 32s 615ms/step - loss: 0.0228 - acc: 0.9938 - val_loss: 2.0371 - val_acc: 0.6402\n",
            "Epoch 392/400\n",
            "52/52 [==============================] - 32s 617ms/step - loss: 0.0457 - acc: 0.9856 - val_loss: 2.0926 - val_acc: 0.6685\n",
            "Epoch 393/400\n",
            "52/52 [==============================] - 32s 613ms/step - loss: 0.0229 - acc: 0.9916 - val_loss: 2.3985 - val_acc: 0.6459\n",
            "Epoch 394/400\n",
            "52/52 [==============================] - 32s 614ms/step - loss: 0.0549 - acc: 0.9830 - val_loss: 2.5270 - val_acc: 0.6374\n",
            "Epoch 395/400\n",
            "52/52 [==============================] - 32s 613ms/step - loss: 0.0574 - acc: 0.9820 - val_loss: 2.3226 - val_acc: 0.5864\n",
            "Epoch 396/400\n",
            "52/52 [==============================] - 32s 613ms/step - loss: 0.0627 - acc: 0.9784 - val_loss: 1.8998 - val_acc: 0.6487\n",
            "Epoch 397/400\n",
            "52/52 [==============================] - 32s 612ms/step - loss: 0.0439 - acc: 0.9856 - val_loss: 1.8017 - val_acc: 0.6204\n",
            "Epoch 398/400\n",
            "52/52 [==============================] - 32s 612ms/step - loss: 0.0721 - acc: 0.9784 - val_loss: 2.3059 - val_acc: 0.5977\n",
            "Epoch 399/400\n",
            "52/52 [==============================] - 32s 610ms/step - loss: 0.0889 - acc: 0.9712 - val_loss: 3.2552 - val_acc: 0.5439\n",
            "Epoch 400/400\n",
            "52/52 [==============================] - 32s 611ms/step - loss: 0.0573 - acc: 0.9844 - val_loss: 2.0412 - val_acc: 0.6572\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUs3VDQHzNPw",
        "colab_type": "text"
      },
      "source": [
        "### Save"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVWtl437zPcc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "save_model(train_model, model_name = 'resnet.h5', folder_name = 'models/ResNet50_300/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuNxtx2IzUiB",
        "colab_type": "text"
      },
      "source": [
        "### Plot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Sl9JWwezVwQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_history(True, train_model, folder_name = 'models/ResNet50_300/', img_name = 'resnet')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eda9WGuf7r8k",
        "colab_type": "text"
      },
      "source": [
        "##Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XpkjynepOaYL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c503be0b-6c1a-49b8-8009-8859243c61ec"
      },
      "source": [
        "j = 0\n",
        "for file_name in os.listdir(working_path + \"data/test/\"):\n",
        "\timg = image.load_img(working_path + \"data/test/\" + file_name);\n",
        "\n",
        "\timg_1 = image.img_to_array(img)\n",
        "\timg_1 = cv2.resize(img_1, (IMAGE_SIZE, IMAGE_SIZE), \n",
        "\t\tinterpolation = cv2.INTER_AREA)\n",
        "\timg_1 = np.expand_dims(img_1, axis=0) / 255.\n",
        "\n",
        "\ty_pred = train_model.predict(img_1)\n",
        "\n",
        "\t# get 5 best predictions\n",
        "\ty_pred_ids = y_pred[0].argsort()[-5:][::-1]\n",
        "\n",
        "\tprint(file_name)\n",
        "\tfor i in range(len(y_pred_ids)):\n",
        "\t\tprint(\"\\n\\t\" + breed_dictionary[y_pred_ids[i]]\n",
        "\t\t\t+ \" (\" \n",
        "\t\t\t+ str(y_pred[0][y_pred_ids[i]]) + \")\")\n",
        "\n",
        "\tprint(\"--------------------\\n\")\n",
        "\n",
        "\tj = j + 1"
      ],
      "execution_count": 192,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "beagle.jpg\n",
            "\n",
            "\tbeagle (1.0)\n",
            "\n",
            "\tGerman_shepherd (6.681615e-13)\n",
            "\n",
            "\tLabrador_retriever (5.3549103e-13)\n",
            "\n",
            "\tpug (1.3439683e-13)\n",
            "\n",
            "\tRottweiler (6.9247566e-14)\n",
            "--------------------\n",
            "\n",
            "Yorkshire_terrier.jpg\n",
            "\n",
            "\tYorkshire_terrier (0.998437)\n",
            "\n",
            "\tGerman_shepherd (0.0012143932)\n",
            "\n",
            "\tbeagle (0.00034048577)\n",
            "\n",
            "\tpug (7.179968e-06)\n",
            "\n",
            "\tLabrador_retriever (9.256487e-07)\n",
            "--------------------\n",
            "\n",
            "Labrador_retriever.jpg\n",
            "\n",
            "\tGerman_shepherd (0.52900016)\n",
            "\n",
            "\tpug (0.25237232)\n",
            "\n",
            "\tYorkshire_terrier (0.09541196)\n",
            "\n",
            "\tbeagle (0.0714587)\n",
            "\n",
            "\tRottweiler (0.030620309)\n",
            "--------------------\n",
            "\n",
            "German_shepherd.jpg\n",
            "\n",
            "\tGerman_shepherd (0.99984765)\n",
            "\n",
            "\tpug (0.00015029685)\n",
            "\n",
            "\tRottweiler (9.59514e-07)\n",
            "\n",
            "\tLabrador_retriever (5.714829e-07)\n",
            "\n",
            "\tbeagle (5.3331553e-07)\n",
            "--------------------\n",
            "\n",
            "pug.jpg\n",
            "\n",
            "\tpug (1.0)\n",
            "\n",
            "\tRottweiler (2.5658957e-09)\n",
            "\n",
            "\tbeagle (8.423335e-12)\n",
            "\n",
            "\tLabrador_retriever (4.9307776e-12)\n",
            "\n",
            "\tGerman_shepherd (3.634722e-12)\n",
            "--------------------\n",
            "\n",
            "standard_poodle.jpg\n",
            "\n",
            "\tstandard_poodle (0.99940836)\n",
            "\n",
            "\tGerman_shepherd (0.0002453238)\n",
            "\n",
            "\tLabrador_retriever (0.00012945496)\n",
            "\n",
            "\tpug (8.552484e-05)\n",
            "\n",
            "\tRottweiler (5.8841615e-05)\n",
            "--------------------\n",
            "\n",
            "Rottweiler.jpg\n",
            "\n",
            "\tRottweiler (1.0)\n",
            "\n",
            "\tLabrador_retriever (2.101333e-09)\n",
            "\n",
            "\tGerman_shepherd (5.965292e-10)\n",
            "\n",
            "\tbeagle (2.0101253e-10)\n",
            "\n",
            "\tstandard_poodle (1.08585696e-10)\n",
            "--------------------\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYyCMDORz9lP",
        "colab_type": "text"
      },
      "source": [
        "# Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yXqx4ucrE07",
        "colab_type": "text"
      },
      "source": [
        "## Plot\n",
        "Plotting training process history: accuracy, loss for both - training and validation sets, saving plots to disk"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gc8ekdp0hYcP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_history(save, history, folder_name, img_name):\n",
        "\n",
        "  # summarize history for accuracy\n",
        "  plt.plot(history.history['acc'])\n",
        "  plt.plot(history.history['val_acc'])\n",
        "  plt.title('model accuracy')\n",
        "  plt.ylabel('accuracy')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['acc', 'val_acc'], loc='upper left')\n",
        "  if save:\n",
        "    plt.savefig(working_path + folder_name + img_name + '_acc.png', bbox_inches='tight')\n",
        "    print('Image have been saved.')\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "  # summarize history for loss\n",
        "  plt.plot(history.history['loss'])\n",
        "  plt.plot(history.history['val_loss'])\n",
        "  plt.title('model loss')\n",
        "  plt.ylabel('loss')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['loss', 'val_loss'], loc='upper left')\n",
        "  if save:\n",
        "    plt.savefig(working_path + folder_name + img_name + '_loss.png', bbox_inches='tight')\n",
        "    print('Image have been saved.')\n",
        "  plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OP6nq44WIoys",
        "colab_type": "text"
      },
      "source": [
        "## Save model\n",
        "Save trained model to disk"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1apzgyDDJoA4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_model(model, model_name, folder_name):\n",
        "  model.save(working_path + folder_name + model_name)\n",
        "  print('Saved model')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AATpl181ad-7",
        "colab_type": "text"
      },
      "source": [
        "## Load model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DidEE7N-eH-8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model_from_drive(model_path):\n",
        "  model = load_model(working_path + model_path)\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbqGFEuAPWjS",
        "colab_type": "text"
      },
      "source": [
        "# Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CV7cscZbfIfB",
        "colab_type": "text"
      },
      "source": [
        "## Load, compile model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXaxKwI-PYNj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "outputId": "23ecd5a9-c382-4c07-9d17-3baf79adc21a"
      },
      "source": [
        "loaded_model = model_from_drive('models/ResNet50_300/ResNet50_300+100+400.h5')\n",
        "loaded_model_weights = loaded_model.get_weights()\n",
        "\n",
        "train_model = ResNet50_scrach()\n",
        "train_model.set_weights(loaded_model_weights)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0812 07:45:21.097849 140619089725312 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0812 07:45:21.189027 140619089725312 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0812 07:45:21.234680 140619089725312 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:245: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0812 07:45:21.236083 140619089725312 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "W0812 07:45:21.238699 140619089725312 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "W0812 07:45:24.240198 140619089725312 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "W0812 07:45:24.332637 140619089725312 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "W0812 07:45:30.093682 140619089725312 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "W0812 07:45:34.571493 140619089725312 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0812 07:45:34.760328 140619089725312 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmXiTmQFgPZL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "c834f197-3008-4f83-a0a3-f05478674bd7"
      },
      "source": [
        "X_regr = Dense(4, activation='linear')(train_model.layers[-5].output)\n",
        "\n",
        "regr_model = Model(input = train_model.input, output = X_regr)\n",
        "\n",
        "opt = Adam(lr=0.0001)\n",
        "regr_model.compile(optimizer=opt, loss=\"mse\", metrics=['accuracy'])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9nfC_1RiRT_U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "f90c69bc-0215-4a8f-87c1-4c02abcbe89c"
      },
      "source": [
        "# regr_model.summary()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 256, 256, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d_1 (ZeroPadding2D (None, 262, 262, 3)  0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1 (Conv2D)                  (None, 128, 128, 64) 9472        zero_padding2d_1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn_conv1 (BatchNormalization)   (None, 128, 128, 64) 256         conv1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 128, 128, 64) 0           bn_conv1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d_2 (ZeroPadding2D (None, 130, 130, 64) 0           activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 64, 64, 64)   0           zero_padding2d_2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "res2abranch2a (Conv2D)          (None, 64, 64, 64)   4160        max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "bn2abranch2a (BatchNormalizatio (None, 64, 64, 64)   256         res2abranch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 64, 64, 64)   0           bn2abranch2a[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "res2abranch2b (Conv2D)          (None, 64, 64, 64)   36928       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "bn2abranch2b (BatchNormalizatio (None, 64, 64, 64)   256         res2abranch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 64, 64, 64)   0           bn2abranch2b[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "res2abranch2c (Conv2D)          (None, 64, 64, 256)  16640       activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "res2abranch1 (Conv2D)           (None, 64, 64, 256)  16640       max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "bn2abranch2c (BatchNormalizatio (None, 64, 64, 256)  1024        res2abranch2c[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn2abranch1 (BatchNormalization (None, 64, 64, 256)  1024        res2abranch1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 64, 64, 256)  0           bn2abranch2c[0][0]               \n",
            "                                                                 bn2abranch1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 64, 64, 256)  0           add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "res2bbranch2a (Conv2D)          (None, 64, 64, 64)   16448       activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "bn2bbranch2a (BatchNormalizatio (None, 64, 64, 64)   256         res2bbranch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 64, 64, 64)   0           bn2bbranch2a[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "res2bbranch2b (Conv2D)          (None, 64, 64, 64)   36928       activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "bn2bbranch2b (BatchNormalizatio (None, 64, 64, 64)   256         res2bbranch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 64, 64, 64)   0           bn2bbranch2b[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "res2bbranch2c (Conv2D)          (None, 64, 64, 256)  16640       activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "bn2bbranch2c (BatchNormalizatio (None, 64, 64, 256)  1024        res2bbranch2c[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_2 (Add)                     (None, 64, 64, 256)  0           bn2bbranch2c[0][0]               \n",
            "                                                                 activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 64, 64, 256)  0           add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "res2cbranch2a (Conv2D)          (None, 64, 64, 64)   16448       activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "bn2cbranch2a (BatchNormalizatio (None, 64, 64, 64)   256         res2cbranch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 64, 64, 64)   0           bn2cbranch2a[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "res2cbranch2b (Conv2D)          (None, 64, 64, 64)   36928       activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "bn2cbranch2b (BatchNormalizatio (None, 64, 64, 64)   256         res2cbranch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 64, 64, 64)   0           bn2cbranch2b[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "res2cbranch2c (Conv2D)          (None, 64, 64, 256)  16640       activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "bn2cbranch2c (BatchNormalizatio (None, 64, 64, 256)  1024        res2cbranch2c[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_3 (Add)                     (None, 64, 64, 256)  0           bn2cbranch2c[0][0]               \n",
            "                                                                 activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 64, 64, 256)  0           add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "res3abranch2a (Conv2D)          (None, 32, 32, 128)  32896       activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn3abranch2a (BatchNormalizatio (None, 32, 32, 128)  512         res3abranch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 32, 32, 128)  0           bn3abranch2a[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "res3abranch2b (Conv2D)          (None, 32, 32, 128)  147584      activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn3abranch2b (BatchNormalizatio (None, 32, 32, 128)  512         res3abranch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 32, 32, 128)  0           bn3abranch2b[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "res3abranch2c (Conv2D)          (None, 32, 32, 512)  66048       activation_12[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res3abranch1 (Conv2D)           (None, 32, 32, 512)  131584      activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn3abranch2c (BatchNormalizatio (None, 32, 32, 512)  2048        res3abranch2c[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn3abranch1 (BatchNormalization (None, 32, 32, 512)  2048        res3abranch1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add_4 (Add)                     (None, 32, 32, 512)  0           bn3abranch2c[0][0]               \n",
            "                                                                 bn3abranch1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 32, 32, 512)  0           add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "res3bbranch2a (Conv2D)          (None, 32, 32, 128)  65664       activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn3bbranch2a (BatchNormalizatio (None, 32, 32, 128)  512         res3bbranch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 32, 32, 128)  0           bn3bbranch2a[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "res3bbranch2b (Conv2D)          (None, 32, 32, 128)  147584      activation_14[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn3bbranch2b (BatchNormalizatio (None, 32, 32, 128)  512         res3bbranch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 32, 32, 128)  0           bn3bbranch2b[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "res3bbranch2c (Conv2D)          (None, 32, 32, 512)  66048       activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn3bbranch2c (BatchNormalizatio (None, 32, 32, 512)  2048        res3bbranch2c[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_5 (Add)                     (None, 32, 32, 512)  0           bn3bbranch2c[0][0]               \n",
            "                                                                 activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 32, 32, 512)  0           add_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "res3cbranch2a (Conv2D)          (None, 32, 32, 128)  65664       activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn3cbranch2a (BatchNormalizatio (None, 32, 32, 128)  512         res3cbranch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, 32, 32, 128)  0           bn3cbranch2a[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "res3cbranch2b (Conv2D)          (None, 32, 32, 128)  147584      activation_17[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn3cbranch2b (BatchNormalizatio (None, 32, 32, 128)  512         res3cbranch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_18 (Activation)      (None, 32, 32, 128)  0           bn3cbranch2b[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "res3cbranch2c (Conv2D)          (None, 32, 32, 512)  66048       activation_18[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn3cbranch2c (BatchNormalizatio (None, 32, 32, 512)  2048        res3cbranch2c[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_6 (Add)                     (None, 32, 32, 512)  0           bn3cbranch2c[0][0]               \n",
            "                                                                 activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_19 (Activation)      (None, 32, 32, 512)  0           add_6[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "res3dbranch2a (Conv2D)          (None, 32, 32, 128)  65664       activation_19[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn3dbranch2a (BatchNormalizatio (None, 32, 32, 128)  512         res3dbranch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_20 (Activation)      (None, 32, 32, 128)  0           bn3dbranch2a[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "res3dbranch2b (Conv2D)          (None, 32, 32, 128)  147584      activation_20[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn3dbranch2b (BatchNormalizatio (None, 32, 32, 128)  512         res3dbranch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_21 (Activation)      (None, 32, 32, 128)  0           bn3dbranch2b[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "res3dbranch2c (Conv2D)          (None, 32, 32, 512)  66048       activation_21[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn3dbranch2c (BatchNormalizatio (None, 32, 32, 512)  2048        res3dbranch2c[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_7 (Add)                     (None, 32, 32, 512)  0           bn3dbranch2c[0][0]               \n",
            "                                                                 activation_19[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_22 (Activation)      (None, 32, 32, 512)  0           add_7[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "res4abranch2a (Conv2D)          (None, 16, 16, 256)  131328      activation_22[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn4abranch2a (BatchNormalizatio (None, 16, 16, 256)  1024        res4abranch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_23 (Activation)      (None, 16, 16, 256)  0           bn4abranch2a[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "res4abranch2b (Conv2D)          (None, 16, 16, 256)  590080      activation_23[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn4abranch2b (BatchNormalizatio (None, 16, 16, 256)  1024        res4abranch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_24 (Activation)      (None, 16, 16, 256)  0           bn4abranch2b[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "res4abranch2c (Conv2D)          (None, 16, 16, 1024) 263168      activation_24[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res4abranch1 (Conv2D)           (None, 16, 16, 1024) 525312      activation_22[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn4abranch2c (BatchNormalizatio (None, 16, 16, 1024) 4096        res4abranch2c[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn4abranch1 (BatchNormalization (None, 16, 16, 1024) 4096        res4abranch1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add_8 (Add)                     (None, 16, 16, 1024) 0           bn4abranch2c[0][0]               \n",
            "                                                                 bn4abranch1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_25 (Activation)      (None, 16, 16, 1024) 0           add_8[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "res4bbranch2a (Conv2D)          (None, 16, 16, 256)  262400      activation_25[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn4bbranch2a (BatchNormalizatio (None, 16, 16, 256)  1024        res4bbranch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_26 (Activation)      (None, 16, 16, 256)  0           bn4bbranch2a[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "res4bbranch2b (Conv2D)          (None, 16, 16, 256)  590080      activation_26[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn4bbranch2b (BatchNormalizatio (None, 16, 16, 256)  1024        res4bbranch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_27 (Activation)      (None, 16, 16, 256)  0           bn4bbranch2b[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "res4bbranch2c (Conv2D)          (None, 16, 16, 1024) 263168      activation_27[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn4bbranch2c (BatchNormalizatio (None, 16, 16, 1024) 4096        res4bbranch2c[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_9 (Add)                     (None, 16, 16, 1024) 0           bn4bbranch2c[0][0]               \n",
            "                                                                 activation_25[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_28 (Activation)      (None, 16, 16, 1024) 0           add_9[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "res4cbranch2a (Conv2D)          (None, 16, 16, 256)  262400      activation_28[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn4cbranch2a (BatchNormalizatio (None, 16, 16, 256)  1024        res4cbranch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_29 (Activation)      (None, 16, 16, 256)  0           bn4cbranch2a[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "res4cbranch2b (Conv2D)          (None, 16, 16, 256)  590080      activation_29[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn4cbranch2b (BatchNormalizatio (None, 16, 16, 256)  1024        res4cbranch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_30 (Activation)      (None, 16, 16, 256)  0           bn4cbranch2b[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "res4cbranch2c (Conv2D)          (None, 16, 16, 1024) 263168      activation_30[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn4cbranch2c (BatchNormalizatio (None, 16, 16, 1024) 4096        res4cbranch2c[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_10 (Add)                    (None, 16, 16, 1024) 0           bn4cbranch2c[0][0]               \n",
            "                                                                 activation_28[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_31 (Activation)      (None, 16, 16, 1024) 0           add_10[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "res4dbranch2a (Conv2D)          (None, 16, 16, 256)  262400      activation_31[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn4dbranch2a (BatchNormalizatio (None, 16, 16, 256)  1024        res4dbranch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_32 (Activation)      (None, 16, 16, 256)  0           bn4dbranch2a[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "res4dbranch2b (Conv2D)          (None, 16, 16, 256)  590080      activation_32[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn4dbranch2b (BatchNormalizatio (None, 16, 16, 256)  1024        res4dbranch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_33 (Activation)      (None, 16, 16, 256)  0           bn4dbranch2b[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "res4dbranch2c (Conv2D)          (None, 16, 16, 1024) 263168      activation_33[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn4dbranch2c (BatchNormalizatio (None, 16, 16, 1024) 4096        res4dbranch2c[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_11 (Add)                    (None, 16, 16, 1024) 0           bn4dbranch2c[0][0]               \n",
            "                                                                 activation_31[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_34 (Activation)      (None, 16, 16, 1024) 0           add_11[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "res4ebranch2a (Conv2D)          (None, 16, 16, 256)  262400      activation_34[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn4ebranch2a (BatchNormalizatio (None, 16, 16, 256)  1024        res4ebranch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_35 (Activation)      (None, 16, 16, 256)  0           bn4ebranch2a[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "res4ebranch2b (Conv2D)          (None, 16, 16, 256)  590080      activation_35[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn4ebranch2b (BatchNormalizatio (None, 16, 16, 256)  1024        res4ebranch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_36 (Activation)      (None, 16, 16, 256)  0           bn4ebranch2b[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "res4ebranch2c (Conv2D)          (None, 16, 16, 1024) 263168      activation_36[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn4ebranch2c (BatchNormalizatio (None, 16, 16, 1024) 4096        res4ebranch2c[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_12 (Add)                    (None, 16, 16, 1024) 0           bn4ebranch2c[0][0]               \n",
            "                                                                 activation_34[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_37 (Activation)      (None, 16, 16, 1024) 0           add_12[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "res4fbranch2a (Conv2D)          (None, 16, 16, 256)  262400      activation_37[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn4fbranch2a (BatchNormalizatio (None, 16, 16, 256)  1024        res4fbranch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_38 (Activation)      (None, 16, 16, 256)  0           bn4fbranch2a[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "res4fbranch2b (Conv2D)          (None, 16, 16, 256)  590080      activation_38[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn4fbranch2b (BatchNormalizatio (None, 16, 16, 256)  1024        res4fbranch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_39 (Activation)      (None, 16, 16, 256)  0           bn4fbranch2b[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "res4fbranch2c (Conv2D)          (None, 16, 16, 1024) 263168      activation_39[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn4fbranch2c (BatchNormalizatio (None, 16, 16, 1024) 4096        res4fbranch2c[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_13 (Add)                    (None, 16, 16, 1024) 0           bn4fbranch2c[0][0]               \n",
            "                                                                 activation_37[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_40 (Activation)      (None, 16, 16, 1024) 0           add_13[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "res5abranch2a (Conv2D)          (None, 8, 8, 512)    524800      activation_40[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn5abranch2a (BatchNormalizatio (None, 8, 8, 512)    2048        res5abranch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_41 (Activation)      (None, 8, 8, 512)    0           bn5abranch2a[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "res5abranch2b (Conv2D)          (None, 8, 8, 512)    2359808     activation_41[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn5abranch2b (BatchNormalizatio (None, 8, 8, 512)    2048        res5abranch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_42 (Activation)      (None, 8, 8, 512)    0           bn5abranch2b[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "res5abranch2c (Conv2D)          (None, 8, 8, 2048)   1050624     activation_42[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "res5abranch1 (Conv2D)           (None, 8, 8, 2048)   2099200     activation_40[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn5abranch2c (BatchNormalizatio (None, 8, 8, 2048)   8192        res5abranch2c[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn5abranch1 (BatchNormalization (None, 8, 8, 2048)   8192        res5abranch1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add_14 (Add)                    (None, 8, 8, 2048)   0           bn5abranch2c[0][0]               \n",
            "                                                                 bn5abranch1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_43 (Activation)      (None, 8, 8, 2048)   0           add_14[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "res5bbranch2a (Conv2D)          (None, 8, 8, 512)    1049088     activation_43[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn5bbranch2a (BatchNormalizatio (None, 8, 8, 512)    2048        res5bbranch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_44 (Activation)      (None, 8, 8, 512)    0           bn5bbranch2a[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "res5bbranch2b (Conv2D)          (None, 8, 8, 512)    2359808     activation_44[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn5bbranch2b (BatchNormalizatio (None, 8, 8, 512)    2048        res5bbranch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_45 (Activation)      (None, 8, 8, 512)    0           bn5bbranch2b[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "res5bbranch2c (Conv2D)          (None, 8, 8, 2048)   1050624     activation_45[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn5bbranch2c (BatchNormalizatio (None, 8, 8, 2048)   8192        res5bbranch2c[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_15 (Add)                    (None, 8, 8, 2048)   0           bn5bbranch2c[0][0]               \n",
            "                                                                 activation_43[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_46 (Activation)      (None, 8, 8, 2048)   0           add_15[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "res5cbranch2a (Conv2D)          (None, 8, 8, 512)    1049088     activation_46[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn5cbranch2a (BatchNormalizatio (None, 8, 8, 512)    2048        res5cbranch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_47 (Activation)      (None, 8, 8, 512)    0           bn5cbranch2a[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "res5cbranch2b (Conv2D)          (None, 8, 8, 512)    2359808     activation_47[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn5cbranch2b (BatchNormalizatio (None, 8, 8, 512)    2048        res5cbranch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_48 (Activation)      (None, 8, 8, 512)    0           bn5cbranch2b[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "res5cbranch2c (Conv2D)          (None, 8, 8, 2048)   1050624     activation_48[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn5cbranch2c (BatchNormalizatio (None, 8, 8, 2048)   8192        res5cbranch2c[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_16 (Add)                    (None, 8, 8, 2048)   0           bn5cbranch2c[0][0]               \n",
            "                                                                 activation_46[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_49 (Activation)      (None, 8, 8, 2048)   0           add_16[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_avg_pool (GlobalAverageP (None, 2048)         0           activation_49[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 4)            8196        global_avg_pool[0][0]            \n",
            "==================================================================================================\n",
            "Total params: 23,595,908\n",
            "Trainable params: 23,542,788\n",
            "Non-trainable params: 53,120\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nExCgJ7RqReh",
        "colab_type": "text"
      },
      "source": [
        "## Freeze shared layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7Y1V3hGqf6O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1f3d2cbc-de6d-4e0f-ead2-718c95ffcffe"
      },
      "source": [
        "# for layer in regr_model.layers[:-1]:\n",
        "#     layer.trainable = False\n",
        "    \n",
        "for layer in regr_model.layers:\n",
        "  print(layer, layer.trainable)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<keras.engine.input_layer.InputLayer object at 0x7fe3c362b400> False\n",
            "<keras.layers.convolutional.ZeroPadding2D object at 0x7fe415105898> True\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fe413090f28> True\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fe4130a9c88> True\n",
            "<keras.layers.core.Activation object at 0x7fe4130a9ba8> True\n",
            "<keras.layers.convolutional.ZeroPadding2D object at 0x7fe412e4e320> True\n",
            "<keras.layers.pooling.MaxPooling2D object at 0x7fe412e32a90> True\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fe412dfbe80> True\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fe412e37748> True\n",
            "<keras.layers.core.Activation object at 0x7fe412e37f98> True\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fe3c0b8f358> True\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fe3c0beb7b8> True\n",
            "<keras.layers.core.Activation object at 0x7fe3c0aed2b0> True\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fe3c0a56e48> True\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fe3c0934be0> True\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fe3c0a07f98> True\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fe3c0964ac8> True\n",
            "<keras.layers.merge.Add object at 0x7fe3c091bf60> True\n",
            "<keras.layers.core.Activation object at 0x7fe3c088d940> True\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fe3c0840f28> True\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fe3c0824c88> True\n",
            "<keras.layers.core.Activation object at 0x7fe3c07e8198> True\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fe3c07846a0> True\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fe3c0731f98> True\n",
            "<keras.layers.core.Activation object at 0x7fe3c0769ac8> True\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fe3c06dfeb8> True\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fe3c0614fd0> True\n",
            "<keras.layers.merge.Add object at 0x7fe3c0642b00> True\n",
            "<keras.layers.core.Activation object at 0x7fe3c05e9f60> True\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fe3c05d0400> True\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fe3c0538828> True\n",
            "<keras.layers.core.Activation object at 0x7fe3c0506630> True\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fe3c052bc18> True\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fe3c04de710> True\n",
            "<keras.layers.core.Activation object at 0x7fe3c0496940> True\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fe3c04219b0> True\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fe3c0421940> True\n",
            "<keras.layers.merge.Add object at 0x7fe3c03886d8> True\n",
            "<keras.layers.core.Activation object at 0x7fe3c02f7e48> True\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fe3c0326470> True\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fe3c02e4278> True\n",
            "<keras.layers.core.Activation object at 0x7fe3c02aea90> True\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fe3c01f3978> True\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fe3c021ef60> True\n",
            "<keras.layers.core.Activation object at 0x7fe3c01d73c8> True\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fe3c0147710> True\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fe3c00a7c88> True\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fe3c0113b00> True\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fe3c0057be0> True\n",
            "<keras.layers.merge.Add object at 0x7fe3c000ef60> True\n",
            "<keras.layers.core.Activation object at 0x7fe3bff7ba58> True\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fe3bff97828> True\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fe3bff052e8> True\n",
            "<keras.layers.core.Activation object at 0x7fe3bfecf2b0> True\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fe3bfe75860> True\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fe3bfea07b8> True\n",
            "<keras.layers.core.Activation object at 0x7fe3bfdcef60> True\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fe3bfdecc50> True\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fe3bfd332b0> True\n",
            "<keras.layers.merge.Add object at 0x7fe3bfd523c8> True\n",
            "<keras.layers.core.Activation object at 0x7fe3bfcd9cc0> True\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fe3bfcc0518> True\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fe3bfcac860> True\n",
            "<keras.layers.core.Activation object at 0x7fe3bfbf8748> True\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fe3bfc1fc18> True\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fe3bfbce208> True\n",
            "<keras.layers.core.Activation object at 0x7fe3bfb7ff98> True\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fe3bfb0fc50> True\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fe3bfabfdd8> True\n",
            "<keras.layers.merge.Add object at 0x7fe3bfa79dd8> True\n",
            "<keras.layers.core.Activation object at 0x7fe3bfa69978> True\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fe3bfa19588> True\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fe3bf9d5c88> True\n",
            "<keras.layers.core.Activation object at 0x7fe3bf99cba8> True\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fe3bf963a58> True\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fe3bf90cf98> True\n",
            "<keras.layers.core.Activation object at 0x7fe3bf8c7668> True\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fe3bf8357f0> True\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fe3bf867f28> True\n",
            "<keras.layers.merge.Add object at 0x7fe3bf821b00> True\n",
            "<keras.layers.core.Activation object at 0x7fe3bf798ef0> True\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fe3bf7492e8> True\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fe3bf7192b0> True\n",
            "<keras.layers.core.Activation object at 0x7fe3bf6ed048> True\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fe3bf687f98> True\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fe3bf636fd0> True\n",
            "<keras.layers.core.Activation object at 0x7fe3bf5f2940> True\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fe3bf5e4a58> True\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fe3bf4d4b70> True\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fe3bf592710> True\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fe3bf4ecd30> True\n",
            "<keras.layers.merge.Add object at 0x7fe3bf43f588> True\n",
            "<keras.layers.core.Activation object at 0x7fe3bf3b4438> True\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fe3bf3e4a90> True\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fe3bf39b7f0> True\n",
            "<keras.layers.core.Activation object at 0x7fe3bf2ff4a8> True\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fe3bf325ba8> True\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fe3bf2d46a0> True\n",
            "<keras.layers.core.Activation object at 0x7fe3bf28fc50> True\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fe3bf21f9b0> True\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fe3bf230cc0> True\n",
            "<keras.layers.merge.Add object at 0x7fe3bf1875c0> True\n",
            "<keras.layers.core.Activation object at 0x7fe3bf0bff60> True\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fe3bf0f4748> True\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fe3bf0df320> True\n",
            "<keras.layers.core.Activation object at 0x7fe3bf04c4a8> True\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fe3bf065e48> True\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fe3befb5908> True\n",
            "<keras.layers.core.Activation object at 0x7fe3befd39b0> True\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fe3bef43b70> True\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fe3beef3668> True\n",
            "<keras.layers.merge.Add object at 0x7fe3bef2a8d0> True\n",
            "<keras.layers.core.Activation object at 0x7fe3bee38978> True\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fe3bee48c88> True\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fe3bee1f6a0> True\n",
            "<keras.layers.core.Activation object at 0x7fe3bede9dd8> True\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fe3bedad8d0> True\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fe3bed427f0> True\n",
            "<keras.layers.core.Activation object at 0x7fe3bed15320> True\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fe3bec82f98> True\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fe3bec34fd0> True\n",
            "<keras.layers.merge.Add object at 0x7fe3bec6f940> True\n",
            "<keras.layers.core.Activation object at 0x7fe3bebe2c50> True\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fe3beb92828> True\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fe3beb4a7f0> True\n",
            "<keras.layers.core.Activation object at 0x7fe3beb2f7f0> True\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fe3bead5b00> True\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fe3bea83630> True\n",
            "<keras.layers.core.Activation object at 0x7fe3bea3fbe0> True\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fe3be9cd0b8> True\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fe3be9790f0> True\n",
            "<keras.layers.merge.Add object at 0x7fe3be944dd8> True\n",
            "<keras.layers.core.Activation object at 0x7fe3be8e5b00> True\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fe3be91c828> True\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fe3be885908> True\n",
            "<keras.layers.core.Activation object at 0x7fe3be7f2278> True\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fe3be80cda0> True\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fe3be7bcf60> True\n",
            "<keras.layers.core.Activation object at 0x7fe3be7787f0> True\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fe3be768b00> True\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fe3be7166d8> True\n",
            "<keras.layers.merge.Add object at 0x7fe3be6d0828> True\n",
            "<keras.layers.core.Activation object at 0x7fe3be6598d0> True\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fe3be670be0> True\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fe3be5c7358> True\n",
            "<keras.layers.core.Activation object at 0x7fe3be591d30> True\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fe3be5528d0> True\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fe3be568ef0> True\n",
            "<keras.layers.core.Activation object at 0x7fe3be4bb278> True\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fe3be4a9e48> True\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fe3be388b70> True\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fe3be459f60> True\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fe3be336a90> True\n",
            "<keras.layers.merge.Add object at 0x7fe3be36ffd0> True\n",
            "<keras.layers.core.Activation object at 0x7fe3be291cf8> True\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fe3be274278> True\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fe3be2648d0> True\n",
            "<keras.layers.core.Activation object at 0x7fe3be1d4128> True\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fe3be1ee8d0> True\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fe3be19ce48> True\n",
            "<keras.layers.core.Activation object at 0x7fe3be159e10> True\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fe3be0c8ba8> True\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fe3be0795c0> True\n",
            "<keras.layers.merge.Add object at 0x7fe3be033b70> True\n",
            "<keras.layers.core.Activation object at 0x7fe3bdfc0780> True\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fe3bdfd1f98> True\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fe3bdfa74e0> True\n",
            "<keras.layers.core.Activation object at 0x7fe3bdef1c18> True\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fe3bdeb15c0> True\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fe3bde7e1d0> True\n",
            "<keras.layers.core.Activation object at 0x7fe3bde9e2e8> True\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fe3bde0bd30> True\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fe3bddb7e80> True\n",
            "<keras.layers.merge.Add object at 0x7fe3bdd75780> True\n",
            "<keras.layers.core.Activation object at 0x7fe3bdd67ac8> True\n",
            "<keras.layers.pooling.GlobalAveragePooling2D object at 0x7fe3bdd67b70> True\n",
            "<keras.layers.core.Dense object at 0x7fe3bd362630> True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ocMnmNaqWZT",
        "colab_type": "text"
      },
      "source": [
        "## Prepare dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3DTX9BhrgB2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "16aaa7f2-71e0-42d6-8145-63c2cb33c58a"
      },
      "source": [
        "img_path = working_path + 'all_images/'\n",
        "dirs = os.listdir(img_path)\n",
        "\n",
        "img_list = []\n",
        "for img in dirs:\n",
        "  img = cv2.imread(img_path + img)\n",
        "  img_1 = image.img_to_array(img)\n",
        "  img_1 = cv2.resize(img_1, (IMAGE_SIZE, IMAGE_SIZE), interpolation = cv2.INTER_AREA)\n",
        "  img_1 = img_1/.255\n",
        "  img_list.append(img_1)\n",
        "\n",
        "X_train = np.array(img_list)\n",
        "X_train.shape\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1193, 256, 256, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpPK5LtF981e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "7f507eae-a5aa-46b5-eb3d-dc837d6708b6"
      },
      "source": [
        "full_labels = pd.read_csv(working_path + 'data/256_labels.csv')\n",
        "full_labels.head()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>filename</th>\n",
              "      <th>width</th>\n",
              "      <th>height</th>\n",
              "      <th>class</th>\n",
              "      <th>xmin</th>\n",
              "      <th>ymin</th>\n",
              "      <th>xmax</th>\n",
              "      <th>ymax</th>\n",
              "      <th>x_koef</th>\n",
              "      <th>y_koef</th>\n",
              "      <th>xmin_resized</th>\n",
              "      <th>ymin_resized</th>\n",
              "      <th>xmax_resized</th>\n",
              "      <th>ymax_resized</th>\n",
              "      <th>xmin_new</th>\n",
              "      <th>ymin_new</th>\n",
              "      <th>xmax_new</th>\n",
              "      <th>ymax_new</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>n02088364_10108</td>\n",
              "      <td>375</td>\n",
              "      <td>500</td>\n",
              "      <td>beagle</td>\n",
              "      <td>75</td>\n",
              "      <td>56</td>\n",
              "      <td>281</td>\n",
              "      <td>498</td>\n",
              "      <td>1.464844</td>\n",
              "      <td>1.953125</td>\n",
              "      <td>51.200000</td>\n",
              "      <td>28.672000</td>\n",
              "      <td>191.829333</td>\n",
              "      <td>254.976000</td>\n",
              "      <td>51</td>\n",
              "      <td>29</td>\n",
              "      <td>192</td>\n",
              "      <td>255</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>n02088364_10206</td>\n",
              "      <td>500</td>\n",
              "      <td>400</td>\n",
              "      <td>beagle</td>\n",
              "      <td>76</td>\n",
              "      <td>99</td>\n",
              "      <td>464</td>\n",
              "      <td>377</td>\n",
              "      <td>1.953125</td>\n",
              "      <td>1.562500</td>\n",
              "      <td>38.912000</td>\n",
              "      <td>63.360000</td>\n",
              "      <td>237.568000</td>\n",
              "      <td>241.280000</td>\n",
              "      <td>39</td>\n",
              "      <td>63</td>\n",
              "      <td>238</td>\n",
              "      <td>241</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>n02088364_10296</td>\n",
              "      <td>500</td>\n",
              "      <td>375</td>\n",
              "      <td>beagle</td>\n",
              "      <td>240</td>\n",
              "      <td>140</td>\n",
              "      <td>499</td>\n",
              "      <td>368</td>\n",
              "      <td>1.953125</td>\n",
              "      <td>1.464844</td>\n",
              "      <td>122.880000</td>\n",
              "      <td>95.573333</td>\n",
              "      <td>255.488000</td>\n",
              "      <td>251.221333</td>\n",
              "      <td>123</td>\n",
              "      <td>96</td>\n",
              "      <td>255</td>\n",
              "      <td>251</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>n02088364_10354</td>\n",
              "      <td>500</td>\n",
              "      <td>375</td>\n",
              "      <td>beagle</td>\n",
              "      <td>30</td>\n",
              "      <td>54</td>\n",
              "      <td>458</td>\n",
              "      <td>374</td>\n",
              "      <td>1.953125</td>\n",
              "      <td>1.464844</td>\n",
              "      <td>15.360000</td>\n",
              "      <td>36.864000</td>\n",
              "      <td>234.496000</td>\n",
              "      <td>255.317333</td>\n",
              "      <td>15</td>\n",
              "      <td>37</td>\n",
              "      <td>234</td>\n",
              "      <td>255</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>n02088364_10362</td>\n",
              "      <td>335</td>\n",
              "      <td>199</td>\n",
              "      <td>beagle</td>\n",
              "      <td>165</td>\n",
              "      <td>22</td>\n",
              "      <td>331</td>\n",
              "      <td>188</td>\n",
              "      <td>1.308594</td>\n",
              "      <td>0.777344</td>\n",
              "      <td>126.089552</td>\n",
              "      <td>28.301508</td>\n",
              "      <td>252.943284</td>\n",
              "      <td>241.849246</td>\n",
              "      <td>126</td>\n",
              "      <td>28</td>\n",
              "      <td>253</td>\n",
              "      <td>242</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          filename  width  height  ... ymin_new  xmax_new  ymax_new\n",
              "0  n02088364_10108    375     500  ...       29       192       255\n",
              "1  n02088364_10206    500     400  ...       63       238       241\n",
              "2  n02088364_10296    500     375  ...       96       255       251\n",
              "3  n02088364_10354    500     375  ...       37       234       255\n",
              "4  n02088364_10362    335     199  ...       28       253       242\n",
              "\n",
              "[5 rows x 18 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HVSVImg7Ukx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e726e7c6-8833-4622-d75c-a6a6e36ab0ab"
      },
      "source": [
        "Y_train = []\n",
        "\n",
        "for img_name in dirs:\n",
        "  img = img_name.split('.')\n",
        "  row = full_labels.loc[full_labels['filename'] == img[0]]\n",
        "  for index, row in row.iterrows():\n",
        "    x1 = row['xmin_new']\n",
        "    y1 = row['ymin_new']\n",
        "    x2 = row['xmax_new']\n",
        "    y2 = row['ymax_new']\n",
        "    nump_array = ([x1, y1, x2, y2])\n",
        "  Y_train.append(nump_array)\n",
        "\n",
        "Y_train = np.asarray(Y_train)\n",
        "Y_train.shape  "
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1193, 4)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UA43odNw6y55",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "random.seed(30)\n",
        "random.shuffle([X_train, Y_train])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WR1mi8oGAL70",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "029bf996-d0d0-478b-c8f0-3f72ccd2d0b8"
      },
      "source": [
        "print(X_train.shape)\n",
        "print(Y_train.shape)\n",
        "\n",
        "print(type(X_train))\n",
        "print(type(Y_train))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1193, 256, 256, 3)\n",
            "(1193, 4)\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XTWHdVOqaYe",
        "colab_type": "text"
      },
      "source": [
        "## Fit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uV32Ta7IHJKS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9938386e-08d2-4f70-80bf-b010cb1a57a2"
      },
      "source": [
        "regr_history = regr_model.fit(\n",
        "    X_train, Y_train, epochs=100, verbose=1, validation_split=0.3)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 835 samples, validate on 358 samples\n",
            "Epoch 1/100\n",
            "835/835 [==============================] - 47s 57ms/step - loss: 24624.9201 - acc: 0.5808 - val_loss: 9025.6148 - val_acc: 0.6676\n",
            "Epoch 2/100\n",
            "835/835 [==============================] - 31s 37ms/step - loss: 20086.0688 - acc: 0.6455 - val_loss: 9974.8747 - val_acc: 0.5475\n",
            "Epoch 3/100\n",
            "835/835 [==============================] - 31s 37ms/step - loss: 17861.0382 - acc: 0.6455 - val_loss: 11855.9241 - val_acc: 0.5251\n",
            "Epoch 4/100\n",
            "835/835 [==============================] - 30s 36ms/step - loss: 16133.0508 - acc: 0.6455 - val_loss: 15411.7998 - val_acc: 0.6620\n",
            "Epoch 5/100\n",
            "835/835 [==============================] - 31s 37ms/step - loss: 14614.7516 - acc: 0.6479 - val_loss: 9721.2163 - val_acc: 0.5978\n",
            "Epoch 6/100\n",
            "835/835 [==============================] - 31s 37ms/step - loss: 13205.8359 - acc: 0.6455 - val_loss: 5543.1776 - val_acc: 0.4916\n",
            "Epoch 7/100\n",
            "835/835 [==============================] - 31s 37ms/step - loss: 11929.1609 - acc: 0.6455 - val_loss: 7379.0972 - val_acc: 0.6453\n",
            "Epoch 8/100\n",
            "835/835 [==============================] - 30s 37ms/step - loss: 10659.6287 - acc: 0.6443 - val_loss: 4945.3755 - val_acc: 0.6257\n",
            "Epoch 9/100\n",
            "835/835 [==============================] - 30s 37ms/step - loss: 9461.2684 - acc: 0.6431 - val_loss: 5311.6601 - val_acc: 0.5978\n",
            "Epoch 10/100\n",
            "835/835 [==============================] - 31s 37ms/step - loss: 8384.8782 - acc: 0.6407 - val_loss: 4882.0730 - val_acc: 0.6453\n",
            "Epoch 11/100\n",
            "835/835 [==============================] - 31s 37ms/step - loss: 7397.8275 - acc: 0.6455 - val_loss: 3421.5574 - val_acc: 0.6620\n",
            "Epoch 12/100\n",
            "835/835 [==============================] - 31s 37ms/step - loss: 6443.6702 - acc: 0.6455 - val_loss: 3387.3026 - val_acc: 0.6564\n",
            "Epoch 13/100\n",
            "835/835 [==============================] - 31s 37ms/step - loss: 5638.2692 - acc: 0.6455 - val_loss: 3461.8611 - val_acc: 0.6369\n",
            "Epoch 14/100\n",
            "835/835 [==============================] - 30s 37ms/step - loss: 5029.5621 - acc: 0.6455 - val_loss: 3671.5165 - val_acc: 0.6564\n",
            "Epoch 15/100\n",
            "835/835 [==============================] - 30s 36ms/step - loss: 4434.3006 - acc: 0.6455 - val_loss: 2154.6087 - val_acc: 0.5810\n",
            "Epoch 16/100\n",
            "835/835 [==============================] - 30s 36ms/step - loss: 3859.9865 - acc: 0.6455 - val_loss: 2413.8022 - val_acc: 0.6341\n",
            "Epoch 17/100\n",
            "835/835 [==============================] - 31s 37ms/step - loss: 3343.7248 - acc: 0.6455 - val_loss: 2720.3308 - val_acc: 0.6676\n",
            "Epoch 18/100\n",
            "835/835 [==============================] - 31s 37ms/step - loss: 2841.9648 - acc: 0.6455 - val_loss: 3471.1222 - val_acc: 0.6676\n",
            "Epoch 19/100\n",
            "835/835 [==============================] - 31s 37ms/step - loss: 2587.5955 - acc: 0.6455 - val_loss: 3357.2172 - val_acc: 0.6676\n",
            "Epoch 20/100\n",
            "835/835 [==============================] - 31s 37ms/step - loss: 2171.7123 - acc: 0.6455 - val_loss: 2239.3571 - val_acc: 0.6676\n",
            "Epoch 21/100\n",
            "835/835 [==============================] - 30s 37ms/step - loss: 1913.6788 - acc: 0.6455 - val_loss: 3360.5632 - val_acc: 0.6676\n",
            "Epoch 22/100\n",
            "835/835 [==============================] - 30s 36ms/step - loss: 1785.7076 - acc: 0.6455 - val_loss: 2519.9147 - val_acc: 0.6676\n",
            "Epoch 23/100\n",
            "835/835 [==============================] - 31s 37ms/step - loss: 1628.6036 - acc: 0.6455 - val_loss: 1468.2966 - val_acc: 0.6676\n",
            "Epoch 24/100\n",
            "835/835 [==============================] - 30s 36ms/step - loss: 1449.0268 - acc: 0.6455 - val_loss: 2353.7166 - val_acc: 0.6676\n",
            "Epoch 25/100\n",
            "835/835 [==============================] - 30s 36ms/step - loss: 1370.0522 - acc: 0.6455 - val_loss: 2000.5718 - val_acc: 0.6676\n",
            "Epoch 26/100\n",
            "835/835 [==============================] - 30s 37ms/step - loss: 1280.6023 - acc: 0.6455 - val_loss: 2785.8417 - val_acc: 0.6676\n",
            "Epoch 27/100\n",
            "835/835 [==============================] - 30s 36ms/step - loss: 1177.0954 - acc: 0.6455 - val_loss: 2743.5480 - val_acc: 0.6676\n",
            "Epoch 28/100\n",
            "835/835 [==============================] - 30s 36ms/step - loss: 1126.8952 - acc: 0.6455 - val_loss: 1194.4945 - val_acc: 0.6676\n",
            "Epoch 29/100\n",
            "835/835 [==============================] - 30s 36ms/step - loss: 1066.5155 - acc: 0.6467 - val_loss: 1191.0798 - val_acc: 0.6676\n",
            "Epoch 30/100\n",
            "835/835 [==============================] - 30s 36ms/step - loss: 1009.1753 - acc: 0.6455 - val_loss: 1394.0682 - val_acc: 0.6676\n",
            "Epoch 31/100\n",
            "835/835 [==============================] - 30s 36ms/step - loss: 992.9087 - acc: 0.6479 - val_loss: 1069.9746 - val_acc: 0.6676\n",
            "Epoch 32/100\n",
            "835/835 [==============================] - 30s 36ms/step - loss: 1007.0600 - acc: 0.6455 - val_loss: 1214.8483 - val_acc: 0.6620\n",
            "Epoch 33/100\n",
            "835/835 [==============================] - 30s 36ms/step - loss: 975.6865 - acc: 0.6467 - val_loss: 1177.3710 - val_acc: 0.6676\n",
            "Epoch 34/100\n",
            "835/835 [==============================] - 30s 36ms/step - loss: 947.4634 - acc: 0.6491 - val_loss: 1356.5115 - val_acc: 0.6648\n",
            "Epoch 35/100\n",
            "835/835 [==============================] - 30s 36ms/step - loss: 916.6760 - acc: 0.6467 - val_loss: 1327.7175 - val_acc: 0.6676\n",
            "Epoch 36/100\n",
            "835/835 [==============================] - 30s 36ms/step - loss: 892.9015 - acc: 0.6491 - val_loss: 1455.7032 - val_acc: 0.6676\n",
            "Epoch 37/100\n",
            "835/835 [==============================] - 30s 36ms/step - loss: 867.4962 - acc: 0.6455 - val_loss: 1246.1001 - val_acc: 0.6676\n",
            "Epoch 38/100\n",
            "835/835 [==============================] - 30s 36ms/step - loss: 867.4948 - acc: 0.6443 - val_loss: 1775.0947 - val_acc: 0.6648\n",
            "Epoch 39/100\n",
            "835/835 [==============================] - 30s 36ms/step - loss: 844.9363 - acc: 0.6587 - val_loss: 1346.5661 - val_acc: 0.6648\n",
            "Epoch 40/100\n",
            "835/835 [==============================] - 30s 36ms/step - loss: 832.6309 - acc: 0.6527 - val_loss: 2015.9792 - val_acc: 0.6676\n",
            "Epoch 41/100\n",
            "835/835 [==============================] - 30s 36ms/step - loss: 801.8541 - acc: 0.6503 - val_loss: 1959.3369 - val_acc: 0.6676\n",
            "Epoch 42/100\n",
            "835/835 [==============================] - 30s 36ms/step - loss: 732.9316 - acc: 0.6611 - val_loss: 1325.4273 - val_acc: 0.6704\n",
            "Epoch 43/100\n",
            "835/835 [==============================] - 30s 36ms/step - loss: 691.0349 - acc: 0.6467 - val_loss: 1089.0538 - val_acc: 0.6788\n",
            "Epoch 44/100\n",
            "835/835 [==============================] - 30s 36ms/step - loss: 703.1066 - acc: 0.6671 - val_loss: 2224.7569 - val_acc: 0.6732\n",
            "Epoch 45/100\n",
            "835/835 [==============================] - 30s 36ms/step - loss: 660.5533 - acc: 0.6802 - val_loss: 1584.6076 - val_acc: 0.6508\n",
            "Epoch 46/100\n",
            "835/835 [==============================] - 30s 36ms/step - loss: 590.4763 - acc: 0.6731 - val_loss: 1522.1277 - val_acc: 0.6425\n",
            "Epoch 47/100\n",
            "835/835 [==============================] - 30s 36ms/step - loss: 666.1120 - acc: 0.6850 - val_loss: 1426.6720 - val_acc: 0.5503\n",
            "Epoch 48/100\n",
            "835/835 [==============================] - 30s 36ms/step - loss: 638.5336 - acc: 0.6671 - val_loss: 2068.8977 - val_acc: 0.5726\n",
            "Epoch 49/100\n",
            "835/835 [==============================] - 30s 36ms/step - loss: 533.7707 - acc: 0.7030 - val_loss: 2112.8973 - val_acc: 0.5251\n",
            "Epoch 50/100\n",
            "835/835 [==============================] - 30s 36ms/step - loss: 491.8850 - acc: 0.7054 - val_loss: 1793.7422 - val_acc: 0.5726\n",
            "Epoch 51/100\n",
            "835/835 [==============================] - 31s 37ms/step - loss: 443.5484 - acc: 0.7281 - val_loss: 1419.8348 - val_acc: 0.6145\n",
            "Epoch 52/100\n",
            "835/835 [==============================] - 31s 37ms/step - loss: 426.9324 - acc: 0.7162 - val_loss: 1512.3343 - val_acc: 0.6564\n",
            "Epoch 53/100\n",
            "835/835 [==============================] - 30s 37ms/step - loss: 394.1842 - acc: 0.7377 - val_loss: 1397.8876 - val_acc: 0.6816\n",
            "Epoch 54/100\n",
            "835/835 [==============================] - 31s 37ms/step - loss: 411.7549 - acc: 0.7461 - val_loss: 1167.9632 - val_acc: 0.7039\n",
            "Epoch 55/100\n",
            "835/835 [==============================] - 30s 36ms/step - loss: 350.7520 - acc: 0.7629 - val_loss: 1274.5181 - val_acc: 0.6844\n",
            "Epoch 56/100\n",
            "835/835 [==============================] - 30s 36ms/step - loss: 313.6208 - acc: 0.7557 - val_loss: 1255.6602 - val_acc: 0.6425\n",
            "Epoch 57/100\n",
            "835/835 [==============================] - 30s 37ms/step - loss: 268.1263 - acc: 0.7653 - val_loss: 1131.1997 - val_acc: 0.6983\n",
            "Epoch 58/100\n",
            "835/835 [==============================] - 31s 37ms/step - loss: 255.0302 - acc: 0.7808 - val_loss: 1455.7567 - val_acc: 0.6844\n",
            "Epoch 59/100\n",
            "835/835 [==============================] - 31s 37ms/step - loss: 242.4982 - acc: 0.7964 - val_loss: 1112.6319 - val_acc: 0.6899\n",
            "Epoch 60/100\n",
            "835/835 [==============================] - 31s 37ms/step - loss: 200.0774 - acc: 0.8012 - val_loss: 1115.3940 - val_acc: 0.6592\n",
            "Epoch 61/100\n",
            "835/835 [==============================] - 30s 37ms/step - loss: 192.2319 - acc: 0.7713 - val_loss: 1301.8955 - val_acc: 0.7011\n",
            "Epoch 62/100\n",
            "835/835 [==============================] - 30s 36ms/step - loss: 187.9649 - acc: 0.7880 - val_loss: 1016.1055 - val_acc: 0.6927\n",
            "Epoch 63/100\n",
            "835/835 [==============================] - 31s 37ms/step - loss: 163.0333 - acc: 0.8156 - val_loss: 1091.0796 - val_acc: 0.6369\n",
            "Epoch 64/100\n",
            "835/835 [==============================] - 31s 37ms/step - loss: 162.1370 - acc: 0.7892 - val_loss: 1188.2474 - val_acc: 0.6564\n",
            "Epoch 65/100\n",
            "835/835 [==============================] - 30s 37ms/step - loss: 196.0303 - acc: 0.8335 - val_loss: 1405.9721 - val_acc: 0.5028\n",
            "Epoch 66/100\n",
            "835/835 [==============================] - 30s 36ms/step - loss: 193.1932 - acc: 0.8012 - val_loss: 1232.2655 - val_acc: 0.6117\n",
            "Epoch 67/100\n",
            "835/835 [==============================] - 30s 36ms/step - loss: 170.6166 - acc: 0.8144 - val_loss: 1204.6253 - val_acc: 0.6732\n",
            "Epoch 68/100\n",
            "835/835 [==============================] - 31s 37ms/step - loss: 158.5461 - acc: 0.8251 - val_loss: 1177.5412 - val_acc: 0.7039\n",
            "Epoch 69/100\n",
            "835/835 [==============================] - 30s 36ms/step - loss: 109.1393 - acc: 0.8395 - val_loss: 1066.2665 - val_acc: 0.7011\n",
            "Epoch 70/100\n",
            "835/835 [==============================] - 30s 36ms/step - loss: 117.2676 - acc: 0.8192 - val_loss: 1115.7177 - val_acc: 0.7011\n",
            "Epoch 71/100\n",
            "835/835 [==============================] - 30s 36ms/step - loss: 106.5722 - acc: 0.8431 - val_loss: 1017.7188 - val_acc: 0.7263\n",
            "Epoch 72/100\n",
            "835/835 [==============================] - 30s 36ms/step - loss: 109.2903 - acc: 0.8371 - val_loss: 1179.2078 - val_acc: 0.7039\n",
            "Epoch 73/100\n",
            "835/835 [==============================] - 30s 36ms/step - loss: 151.1098 - acc: 0.8263 - val_loss: 1197.0823 - val_acc: 0.4832\n",
            "Epoch 74/100\n",
            "835/835 [==============================] - 30s 36ms/step - loss: 135.3101 - acc: 0.8012 - val_loss: 1002.6867 - val_acc: 0.6704\n",
            "Epoch 75/100\n",
            "835/835 [==============================] - 31s 37ms/step - loss: 112.6153 - acc: 0.8395 - val_loss: 1130.9872 - val_acc: 0.5475\n",
            "Epoch 76/100\n",
            "835/835 [==============================] - 30s 36ms/step - loss: 100.2569 - acc: 0.8347 - val_loss: 1117.0874 - val_acc: 0.7123\n",
            "Epoch 77/100\n",
            "835/835 [==============================] - 30s 36ms/step - loss: 104.8444 - acc: 0.8311 - val_loss: 1030.0913 - val_acc: 0.6899\n",
            "Epoch 78/100\n",
            "835/835 [==============================] - 30s 36ms/step - loss: 188.9597 - acc: 0.8299 - val_loss: 1349.5420 - val_acc: 0.4972\n",
            "Epoch 79/100\n",
            "835/835 [==============================] - 30s 36ms/step - loss: 131.6802 - acc: 0.8156 - val_loss: 1004.5377 - val_acc: 0.6788\n",
            "Epoch 80/100\n",
            "835/835 [==============================] - 30s 36ms/step - loss: 125.2736 - acc: 0.8251 - val_loss: 1034.3023 - val_acc: 0.6927\n",
            "Epoch 81/100\n",
            "835/835 [==============================] - 30s 36ms/step - loss: 117.5817 - acc: 0.8323 - val_loss: 1059.0212 - val_acc: 0.6844\n",
            "Epoch 82/100\n",
            "835/835 [==============================] - 30s 36ms/step - loss: 107.2791 - acc: 0.8479 - val_loss: 1048.7390 - val_acc: 0.6620\n",
            "Epoch 83/100\n",
            "835/835 [==============================] - 30s 37ms/step - loss: 104.1347 - acc: 0.8527 - val_loss: 974.9926 - val_acc: 0.7039\n",
            "Epoch 84/100\n",
            "835/835 [==============================] - 30s 36ms/step - loss: 94.4201 - acc: 0.8563 - val_loss: 980.4266 - val_acc: 0.7039\n",
            "Epoch 85/100\n",
            "835/835 [==============================] - 30s 36ms/step - loss: 84.2623 - acc: 0.8431 - val_loss: 968.2921 - val_acc: 0.6983\n",
            "Epoch 86/100\n",
            "835/835 [==============================] - 30s 36ms/step - loss: 80.2517 - acc: 0.8695 - val_loss: 978.1477 - val_acc: 0.6899\n",
            "Epoch 87/100\n",
            "835/835 [==============================] - 30s 36ms/step - loss: 100.8386 - acc: 0.8467 - val_loss: 980.8667 - val_acc: 0.6872\n",
            "Epoch 88/100\n",
            "835/835 [==============================] - 30s 36ms/step - loss: 93.3399 - acc: 0.8743 - val_loss: 956.4349 - val_acc: 0.7039\n",
            "Epoch 89/100\n",
            "835/835 [==============================] - 31s 37ms/step - loss: 87.5280 - acc: 0.8766 - val_loss: 1060.3208 - val_acc: 0.6955\n",
            "Epoch 90/100\n",
            "835/835 [==============================] - 30s 36ms/step - loss: 64.0542 - acc: 0.8647 - val_loss: 1005.7174 - val_acc: 0.7011\n",
            "Epoch 91/100\n",
            "835/835 [==============================] - 30s 36ms/step - loss: 77.3485 - acc: 0.8659 - val_loss: 975.3463 - val_acc: 0.7039\n",
            "Epoch 92/100\n",
            "835/835 [==============================] - 31s 37ms/step - loss: 113.8588 - acc: 0.8790 - val_loss: 1077.5286 - val_acc: 0.7179\n",
            "Epoch 93/100\n",
            "835/835 [==============================] - 31s 37ms/step - loss: 91.0398 - acc: 0.8503 - val_loss: 1102.4588 - val_acc: 0.7123\n",
            "Epoch 94/100\n",
            "835/835 [==============================] - 31s 37ms/step - loss: 73.9779 - acc: 0.8599 - val_loss: 1171.3261 - val_acc: 0.6788\n",
            "Epoch 95/100\n",
            "835/835 [==============================] - 30s 36ms/step - loss: 81.1785 - acc: 0.8623 - val_loss: 1051.0635 - val_acc: 0.7123\n",
            "Epoch 96/100\n",
            "835/835 [==============================] - 30s 36ms/step - loss: 73.6630 - acc: 0.8611 - val_loss: 1003.9314 - val_acc: 0.7039\n",
            "Epoch 97/100\n",
            "835/835 [==============================] - 30s 36ms/step - loss: 79.3567 - acc: 0.8395 - val_loss: 969.7670 - val_acc: 0.6899\n",
            "Epoch 98/100\n",
            "835/835 [==============================] - 30s 36ms/step - loss: 91.5089 - acc: 0.8635 - val_loss: 984.2532 - val_acc: 0.6816\n",
            "Epoch 99/100\n",
            "835/835 [==============================] - 31s 37ms/step - loss: 79.5776 - acc: 0.8587 - val_loss: 1003.7018 - val_acc: 0.6369\n",
            "Epoch 100/100\n",
            "835/835 [==============================] - 30s 36ms/step - loss: 83.6165 - acc: 0.8443 - val_loss: 957.0126 - val_acc: 0.6983\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HFhnMesr6e_",
        "colab_type": "text"
      },
      "source": [
        "## Plot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDyIV-kzr7p9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "outputId": "6a92deb7-986a-474f-e0f2-06f7e28e229a"
      },
      "source": [
        "plot_history(True, regr_history, folder_name = 'models/Regression/', img_name = 'regression_15+100')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Image have been saved.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nB+y8bHmW3iQZAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Image have been saved.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZUAAAEWCAYAAACufwpNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSg5fAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbsAnIKBr2qq",
        "colab_type": "text"
      },
      "source": [
        "## Save"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xx2-tt0CXvKd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4367c98f-f80c-4afb-efaf-472889d55449"
      },
      "source": [
        "save_model(regr_model, model_name = 'regression_115.h5', folder_name = 'models/Regression/')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "moxFhmeMgkzw",
        "colab_type": "text"
      },
      "source": [
        "## Clear session"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9R70ADELyDr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "K.clear_session()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}