{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pprint\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "from optparse import OptionParser\n",
    "import pickle\n",
    "import math\n",
    "import cv2\n",
    "import copy\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_path = \"/content/drive/My Drive/Dog_Breed/7_breeds/\"\n",
    "labels_path = '/content/drive/My Drive/Dog_Breed/7_breeds/data/256_labels.csv'\n",
    "# !ls \"/content/drive/My Drive/Dog_Breed/7_breeds/\"\n",
    "\n",
    "num_rois = 4   #Number of RoIs to process at once.\n",
    "\n",
    "# Augmentation flags\n",
    "horizontal_flips = True # Augment with horizontal flips in training. \n",
    "vertical_flips = True   # Augment with vertical flips in training. \n",
    "rot_90 = True           # Augment with 90 degree rotations in training. \n",
    "\n",
    "base_weight_path = os.path.join(working_path, 'model/ResNet50_300/ResResNet50_300+100+400.h5')\n",
    "\n",
    "output_weight_path = os.path.join(working_path, 'model/FR-CNN/frcnn_resnet.hdf5')\n",
    "\n",
    "\n",
    "record_path = os.path.join(working_path, 'records/record.csv') # Record data (used to save the losses, classification accuracy and mean average precision)\n",
    "\n",
    "config_output_filename = os.path.join(working_path, 'model_resnet_config.pickle')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        # Print the process or not\n",
    "        self.verbose = True\n",
    "\n",
    "        # Name of base network\n",
    "        self.network = 'vgg'\n",
    "\n",
    "        # Setting for data augmentation\n",
    "        self.use_horizontal_flips = False\n",
    "        self.use_vertical_flips = False\n",
    "        self.rot_90 = False\n",
    "\n",
    "        # Anchor box scales\n",
    "    # Note that if im_size is smaller, anchor_box_scales should be scaled\n",
    "    # Original anchor_box_scales in the paper is [128, 256, 512]\n",
    "        self.anchor_box_scales = [64, 128, 256] \n",
    "\n",
    "        # Anchor box ratios\n",
    "        self.anchor_box_ratios = [[1, 1], [1./math.sqrt(2), 2./math.sqrt(2)], [2./math.sqrt(2), 1./math.sqrt(2)]]\n",
    "\n",
    "        # Size to resize the smallest side of the image\n",
    "        # Original setting in paper is 600. Set to 300 in here to save training time\n",
    "        self.im_size = 256\n",
    "\n",
    "        # image channel-wise mean to subtract\n",
    "        self.img_channel_mean = [103.939, 116.779, 123.68]\n",
    "        self.img_scaling_factor = 1.0\n",
    "\n",
    "        # number of ROIs at once\n",
    "        self.num_rois = 4\n",
    "\n",
    "        # stride at the RPN (this depends on the network configuration)\n",
    "        self.rpn_stride = 16\n",
    "\n",
    "        self.balanced_classes = False\n",
    "\n",
    "        # scaling the stdev\n",
    "        self.std_scaling = 4.0\n",
    "        self.classifier_regr_std = [8.0, 8.0, 4.0, 4.0]\n",
    "\n",
    "        # overlaps for RPN\n",
    "        self.rpn_min_overlap = 0.3\n",
    "        self.rpn_max_overlap = 0.7\n",
    "\n",
    "        # overlaps for classifier ROIs\n",
    "        self.classifier_min_overlap = 0.1\n",
    "        self.classifier_max_overlap = 0.5\n",
    "\n",
    "        # placeholder for the class mapping, automatically generated by the parser\n",
    "        self.class_mapping = None\n",
    "\n",
    "        self.model_path = None\n",
    "        \n",
    "#         self.record_path = None\n",
    "        \n",
    "#         self.base_net_weights = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the config\n",
    "C = Config()\n",
    "\n",
    "C.use_horizontal_flips = horizontal_flips\n",
    "C.use_vertical_flips = vertical_flips\n",
    "C.rot_90 = rot_90\n",
    "\n",
    "C.record_path = record_path\n",
    "C.model_path = output_weight_path\n",
    "C.num_rois = num_rois\n",
    "\n",
    "C.base_net_weights = base_weight_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(input_path):\n",
    "    \n",
    "    found_bg = False\n",
    "    all_imgs = {}\n",
    "\n",
    "    classes_count = {}\n",
    "\n",
    "    class_mapping = {}\n",
    "\n",
    "    visualise = True\n",
    "\n",
    "    i = 1\n",
    "\n",
    "    with open(input_path,'r') as f:\n",
    "\n",
    "        print('Parsing annotation files')\n",
    "\n",
    "        for line in f:\n",
    "\n",
    "            # Print process\n",
    "            sys.stdout.write('\\r'+'idx=' + str(i))\n",
    "            i += 1\n",
    "\n",
    "            line_split = line.strip().split(',')\n",
    "\n",
    "            (filename,_,_,class_name,_,_,_,_,_,_,_,_,_,_,x1,y1,x2,y2) = line_split\n",
    "\n",
    "            if class_name not in classes_count:\n",
    "                classes_count[class_name] = 1\n",
    "            else:\n",
    "                classes_count[class_name] += 1\n",
    "    \n",
    "            if class_name not in class_mapping:\n",
    "                if class_name == 'bg' and found_bg == False:\n",
    "                    print('Found class name with special name bg. Will be treated as a background region (this is usually for hard negative mining).')\n",
    "                    found_bg = True\n",
    "                class_mapping[class_name] = len(class_mapping)\n",
    "            \n",
    "            if filename not in all_imgs:\n",
    "                all_imgs[filename] = {}\n",
    "\n",
    "                img = cv2.imread(filename)\n",
    "                (rows,cols) = img.shape[:2]\n",
    "                all_imgs[filename]['filepath'] = filename\n",
    "                all_imgs[filename]['width'] = cols\n",
    "                all_imgs[filename]['height'] = rows\n",
    "                all_imgs[filename]['bboxes'] = []\n",
    "                if np.random.randint(0,6) > 0:\n",
    "                    all_imgs[filename]['imageset'] = 'trainval'\n",
    "                else:\n",
    "                    all_imgs[filename]['imageset'] = 'test'\n",
    "                    \n",
    "            all_imgs[filename]['bboxes'].append({'class': class_name, 'x1': int(x1), 'x2': int(x2), 'y1': int(y1), 'y2': int(y2)})\n",
    "            \n",
    "        all_data = []\n",
    "        for key in all_imgs:\n",
    "            all_data.append(all_imgs[key])\n",
    "        \n",
    "        # make sure the bg class is last in the list\n",
    "        if found_bg:\n",
    "            if class_mapping['bg'] != len(class_mapping) - 1:\n",
    "                key_to_switch = [key for key in class_mapping.keys() if class_mapping[key] == len(class_mapping)-1][0]\n",
    "                val_to_switch = class_mapping['bg']\n",
    "                class_mapping['bg'] = len(class_mapping) - 1\n",
    "                class_mapping[key_to_switch] = val_to_switch\n",
    "        \n",
    "        return all_data, classes_count, class_mapping        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = time.time()\n",
    "train_imgs, classes_count, class_mapping = get_data(train_path)\n",
    "print()\n",
    "print('Spend %0.2f mins to load the data' % ((time.time()-st)/60) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'bg' not in classes_count:\n",
    "    classes_count['bg'] = 0\n",
    "    class_mapping['bg'] = len(class_mapping)\n",
    "# e.g.\n",
    "#    classes_count: {'Car': 2383, 'Mobile phone': 1108, 'Person': 3745, 'bg': 0}\n",
    "#    class_mapping: {'Person': 0, 'Car': 1, 'Mobile phone': 2, 'bg': 3}\n",
    "C.class_mapping = class_mapping\n",
    "\n",
    "print('Training images per class:')\n",
    "pprint.pprint(classes_count)\n",
    "print('Num classes (including bg) = {}'.format(len(classes_count)))\n",
    "print(class_mapping)\n",
    "\n",
    "# Save the configuration\n",
    "with open(config_output_filename, 'wb') as config_f:\n",
    "    pickle.dump(C,config_f)\n",
    "    print('Config has been written to {}, and can be loaded when testing to ensure correct results'.format(config_output_filename))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
